var tipuesearch = {"pages":[{"title":"Time Series Analysis on Superstore Sales Date","text":"Time Series Analysis Project on Superstore Sales Data Time Series Analysis (TSA) is an important part in the field of data science. TSA uses methods for analyzing time series data in order to identify useful patterns and extract meaningful statistics of the data. There are two major goals of TSA: 1) identifing patterns or features represented by the data; and 2) forecasting (using a model to predict future values based on previous data). TSA is widely used for analysing non-stationary data, like weather data, stock price prediction, economic trend prediction and store sales prediction in this post. Different techniques in TSA will be demonstrated in this post. Let's get into it. The data we will use is the superstore sales data, and it can be download here . A Brief Exploratory Data Analysis (EDA) In [1]: # Import libraries import itertools import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . style . use ( 'fivethirtyeight' ) import statsmodels.api as sm import matplotlib matplotlib . rcParams [ 'axes.labelsize' ] = 14 matplotlib . rcParams [ 'xtick.labelsize' ] = 12 matplotlib . rcParams [ 'ytick.labelsize' ] = 12 matplotlib . rcParams [ 'text.color' ] = 'k' import warnings warnings . filterwarnings ( 'ignore' ) Read in the data. We do a brief EDA to see what's in the data. In [2]: # Import data df = pd . read_excel ( 'data\\Superstore.xls' ) df . head () Out[2]: Row ID Order ID Order Date Ship Date Ship Mode Customer ID Customer Name Segment Country City ... Postal Code Region Product ID Category Sub-Category Product Name Sales Quantity Discount Profit 0 1 CA-2016-152156 2016-11-08 2016-11-11 Second Class CG-12520 Claire Gute Consumer United States Henderson ... 42420 South FUR-BO-10001798 Furniture Bookcases Bush Somerset Collection Bookcase 261.9600 2 0.00 41.9136 1 2 CA-2016-152156 2016-11-08 2016-11-11 Second Class CG-12520 Claire Gute Consumer United States Henderson ... 42420 South FUR-CH-10000454 Furniture Chairs Hon Deluxe Fabric Upholstered Stacking Chairs,... 731.9400 3 0.00 219.5820 2 3 CA-2016-138688 2016-06-12 2016-06-16 Second Class DV-13045 Darrin Van Huff Corporate United States Los Angeles ... 90036 West OFF-LA-10000240 Office Supplies Labels Self-Adhesive Address Labels for Typewriters b... 14.6200 2 0.00 6.8714 3 4 US-2015-108966 2015-10-11 2015-10-18 Standard Class SO-20335 Sean O'Donnell Consumer United States Fort Lauderdale ... 33311 South FUR-TA-10000577 Furniture Tables Bretford CR4500 Series Slim Rectangular Table 957.5775 5 0.45 -383.0310 4 5 US-2015-108966 2015-10-11 2015-10-18 Standard Class SO-20335 Sean O'Donnell Consumer United States Fort Lauderdale ... 33311 South OFF-ST-10000760 Office Supplies Storage Eldon Fold 'N Roll Cart System 22.3680 2 0.20 2.5164 5 rows × 21 columns In [3]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 9994 entries, 0 to 9993 Data columns (total 21 columns): Row ID 9994 non-null int64 Order ID 9994 non-null object Order Date 9994 non-null datetime64[ns] Ship Date 9994 non-null datetime64[ns] Ship Mode 9994 non-null object Customer ID 9994 non-null object Customer Name 9994 non-null object Segment 9994 non-null object Country 9994 non-null object City 9994 non-null object State 9994 non-null object Postal Code 9994 non-null int64 Region 9994 non-null object Product ID 9994 non-null object Category 9994 non-null object Sub-Category 9994 non-null object Product Name 9994 non-null object Sales 9994 non-null float64 Quantity 9994 non-null int64 Discount 9994 non-null float64 Profit 9994 non-null float64 dtypes: datetime64[ns](2), float64(3), int64(3), object(13) memory usage: 1.6+ MB We have 21 columns in the dataset. Let's take a look what are they. In [4]: df . columns . values Out[4]: array(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit'], dtype=object) Check if there is any missing values in the data. It turns out there's no missing values in the data. In [5]: df . isnull () . any () Out[5]: Row ID False Order ID False Order Date False Ship Date False Ship Mode False Customer ID False Customer Name False Segment False Country False City False State False Postal Code False Region False Product ID False Category False Sub-Category False Product Name False Sales False Quantity False Discount False Profit False dtype: bool Let's take a look about the statistics of numeric values in the data. In [6]: df . describe () Out[6]: Row ID Postal Code Sales Quantity Discount Profit count 9994.000000 9994.000000 9994.000000 9994.000000 9994.000000 9994.000000 mean 4997.500000 55190.379428 229.858001 3.789574 0.156203 28.656896 std 2885.163629 32063.693350 623.245101 2.225110 0.206452 234.260108 min 1.000000 1040.000000 0.444000 1.000000 0.000000 -6599.978000 25% 2499.250000 23223.000000 17.280000 2.000000 0.000000 1.728750 50% 4997.500000 56430.500000 54.490000 3.000000 0.200000 8.666500 75% 7495.750000 90008.000000 209.940000 5.000000 0.200000 29.364000 max 9994.000000 99301.000000 22638.480000 14.000000 0.800000 8399.976000 We found that there is an interesting column Category which can divided the data into several subsets according to the product categories. Let's see how many categories of products we have in the data. In [7]: df . Category . unique () Out[7]: array(['Furniture', 'Office Supplies', 'Technology'], dtype=object) In [8]: df . Category . value_counts () Out[8]: Office Supplies 6026 Furniture 2121 Technology 1847 Name: Category, dtype: int64 Now we can analyse different categories of products accordingly. In this post, let's take the Office Supplies data as an example. We will start our time series analysis on this type of products. You can also explore other categories in a similar way. In [9]: # Extract the data of Office Supplies office_supplies = df . loc [ df [ 'Category' ] == 'Office Supplies' ] We have a good 4-year office supplies sales data. We will try to find some meaningful patterns in this data. In [10]: office_supplies [ 'Order Date' ] . min (), office_supplies [ 'Order Date' ] . max () Out[10]: (Timestamp('2014-01-03 00:00:00'), Timestamp('2017-12-30 00:00:00')) Data Processing In this post, we only care about the sales data of office supplies. We'll start by droping other columns. First, let us set the index of our data using the 'Order Data' column. Note that, the sales data on the same date should be integrated together. In [11]: office_supplies = office_supplies [[ 'Order Date' , 'Sales' ]] office_supplies . head () Out[11]: Order Date Sales 2 2016-06-12 14.620 4 2015-10-11 22.368 6 2014-06-09 7.280 8 2014-06-09 18.504 9 2014-06-09 114.900 In [12]: office_supplies = office_supplies . sort_values ( 'Order Date' ) office_supplies = office_supplies . groupby ( 'Order Date' )[ 'Sales' ] . sum () . reset_index () office_supplies . head () Out[12]: Order Date Sales 0 2014-01-03 16.448 1 2014-01-04 288.060 2 2014-01-05 19.536 3 2014-01-06 685.340 4 2014-01-07 10.430 Indexing with the time series data In [13]: office_supplies = office_supplies . set_index ( 'Order Date' ) office_supplies . head () Out[13]: Sales Order Date 2014-01-03 16.448 2014-01-04 288.060 2014-01-05 19.536 2014-01-06 685.340 2014-01-07 10.430 In [14]: office_supplies . index Out[14]: DatetimeIndex(['2014-01-03', '2014-01-04', '2014-01-05', '2014-01-06', '2014-01-07', '2014-01-09', '2014-01-10', '2014-01-13', '2014-01-16', '2014-01-18', ... '2017-12-21', '2017-12-22', '2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30'], dtype='datetime64[ns]', name='Order Date', length=1148, freq=None) As can be seen from the index, our current datetime data is not continuous and can be tricky to work with. Therefore, we will use the average daily sales value for that month instead, and we are using the start of each month as the timestamp. This requires us to resample the data. In [15]: monthly = office_supplies [ 'Sales' ] . resample ( 'MS' ) . mean () In [16]: monthly Out[16]: Order Date 2014-01-01 285.357647 2014-02-01 63.042588 2014-03-01 391.176318 2014-04-01 464.794750 2014-05-01 324.346545 2014-06-01 588.774409 2014-07-01 756.060400 2014-08-01 541.879143 2014-09-01 1015.677704 2014-10-01 267.078815 2014-11-01 959.372714 2014-12-01 692.556231 2015-01-01 129.198571 2015-02-01 335.504187 2015-03-01 690.545522 2015-04-01 502.342320 2015-05-01 364.549440 2015-06-01 560.407737 2015-07-01 205.214739 2015-08-01 558.814667 2015-09-01 772.230680 2015-10-01 361.392083 2015-11-01 757.790357 2015-12-01 540.055800 2016-01-01 331.230125 2016-02-01 357.597368 2016-03-01 693.877240 2016-04-01 462.932478 2016-05-01 449.489724 2016-06-01 436.072400 2016-07-01 587.474727 2016-08-01 344.605385 2016-09-01 830.847786 2016-10-01 678.408083 2016-11-01 787.972231 2016-12-01 1357.055929 2017-01-01 967.013136 2017-02-01 389.882737 2017-03-01 538.899481 2017-04-01 558.229296 2017-05-01 508.776444 2017-06-01 650.463038 2017-07-01 393.902615 2017-08-01 1156.148154 2017-09-01 1139.137250 2017-10-01 886.045846 2017-11-01 1124.012036 2017-12-01 1049.549724 Freq: MS, Name: Sales, dtype: float64 Visualizing the Sales Time Series Data In [17]: monthly . plot ( figsize = ( 16 , 7 )) plt . show () Some distinguishable patterns appear when we plot the data. The time-series has seasonality pattern, such as sales are always low at the beginning of the year and high at the end of the year. There is always an upward trend within any single year with a couple of low months in the mid of the year. We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise. In [18]: from pylab import rcParams rcParams [ 'figure.figsize' ] = 16 , 7 decomposition = sm . tsa . seasonal_decompose ( monthly , model = 'additive' ) fig = decomposition . plot () plt . show () The figure above clearly shows the seasonality in our data, and the trend is gradually increasing through the years. Time Series Forecasting with ARIMA Autoregressive Integrated Moving Average (ARIMA) is a commonly used method for time-series prediction. ARIMA models are denoted with the notation ARIMA(p, d, q) . These three parameters account for seasonality, trend, and noise in data: In [19]: p = d = q = range ( 0 , 2 ) pdq = list ( itertools . product ( p , d , q )) seasonal_pdq = [( x [ 0 ], x [ 1 ], x [ 2 ], 12 ) for x in list ( itertools . product ( p , d , q ))] print ( 'Examples of parameter combinations for Seasonal ARIMA...' ) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 1 ], seasonal_pdq [ 1 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 1 ], seasonal_pdq [ 2 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 2 ], seasonal_pdq [ 3 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 2 ], seasonal_pdq [ 4 ])) Examples of parameter combinations for Seasonal ARIMA... SARIMAX: (0, 0, 1) x (0, 0, 1, 12) SARIMAX: (0, 0, 1) x (0, 1, 0, 12) SARIMAX: (0, 1, 0) x (0, 1, 1, 12) SARIMAX: (0, 1, 0) x (1, 0, 0, 12) This step is parameter selection for our Office Supplies sales ARIMA Time Series Model. Our goal here is to use a \"grid search\" to find the optimal set of parameters that yields the best performance for our model. In [20]: for param in pdq : for param_seasonal in seasonal_pdq : try : mod = sm . tsa . statespace . SARIMAX ( monthly , order = param , seasonal_order = param_seasonal , enforce_stationarity = False , enforce_invertibility = False ) results = mod . fit () print ( 'ARIMA {} x {} 12 - AIC: {} ' . format ( param , param_seasonal , results . aic )) except : continue ARIMA(0, 0, 0)x(0, 0, 0, 12)12 - AIC:747.1990404227043 ARIMA(0, 0, 0)x(0, 0, 1, 12)12 - AIC:1464.4967660059162 ARIMA(0, 0, 0)x(0, 1, 0, 12)12 - AIC:500.7335398750499 ARIMA(0, 0, 0)x(1, 0, 0, 12)12 - AIC:516.0876543936834 ARIMA(0, 0, 0)x(1, 0, 1, 12)12 - AIC:1459.9493195280877 ARIMA(0, 0, 0)x(1, 1, 0, 12)12 - AIC:347.44488563489716 ARIMA(0, 0, 1)x(0, 0, 0, 12)12 - AIC:702.6891395292475 ARIMA(0, 0, 1)x(0, 0, 1, 12)12 - AIC:nan ARIMA(0, 0, 1)x(0, 1, 0, 12)12 - AIC:487.76014158147206 ARIMA(0, 0, 1)x(1, 0, 0, 12)12 - AIC:517.4952646585557 ARIMA(0, 0, 1)x(1, 0, 1, 12)12 - AIC:3022.312250877466 ARIMA(0, 0, 1)x(1, 1, 0, 12)12 - AIC:345.69220944601045 ARIMA(0, 1, 0)x(0, 0, 0, 12)12 - AIC:666.4572045007284 ARIMA(0, 1, 0)x(0, 0, 1, 12)12 - AIC:1360.057691075371 ARIMA(0, 1, 0)x(0, 1, 0, 12)12 - AIC:503.0840747609876 ARIMA(0, 1, 0)x(1, 0, 0, 12)12 - AIC:500.0109385290892 ARIMA(0, 1, 0)x(1, 0, 1, 12)12 - AIC:1483.149298292236 ARIMA(0, 1, 0)x(1, 1, 0, 12)12 - AIC:335.7148959418817 ARIMA(0, 1, 1)x(0, 0, 0, 12)12 - AIC:637.3530008828178 ARIMA(0, 1, 1)x(0, 0, 1, 12)12 - AIC:3075.7023343964047 ARIMA(0, 1, 1)x(0, 1, 0, 12)12 - AIC:473.71539674554265 ARIMA(0, 1, 1)x(1, 0, 0, 12)12 - AIC:489.9419970027669 ARIMA(0, 1, 1)x(1, 0, 1, 12)12 - AIC:2578.319436918695 ARIMA(0, 1, 1)x(1, 1, 0, 12)12 - AIC:330.7061513093243 ARIMA(1, 0, 0)x(0, 0, 0, 12)12 - AIC:680.4032716562347 ARIMA(1, 0, 0)x(0, 0, 1, 12)12 - AIC:1588.3750282900137 ARIMA(1, 0, 0)x(0, 1, 0, 12)12 - AIC:501.7031226672368 ARIMA(1, 0, 0)x(1, 0, 0, 12)12 - AIC:500.2190534421442 ARIMA(1, 0, 0)x(1, 0, 1, 12)12 - AIC:1455.6307367581146 ARIMA(1, 0, 0)x(1, 1, 0, 12)12 - AIC:331.0719973254865 ARIMA(1, 0, 1)x(0, 0, 0, 12)12 - AIC:651.1768264308408 ARIMA(1, 0, 1)x(0, 0, 1, 12)12 - AIC:28112.535708773736 ARIMA(1, 0, 1)x(0, 1, 0, 12)12 - AIC:488.4314196132838 ARIMA(1, 0, 1)x(1, 0, 0, 12)12 - AIC:489.254640043546 ARIMA(1, 0, 1)x(1, 0, 1, 12)12 - AIC:2661.0163451942744 ARIMA(1, 0, 1)x(1, 1, 0, 12)12 - AIC:331.7136802273554 ARIMA(1, 1, 0)x(0, 0, 0, 12)12 - AIC:658.0713305703066 ARIMA(1, 1, 0)x(0, 0, 1, 12)12 - AIC:1417.437544166757 ARIMA(1, 1, 0)x(0, 1, 0, 12)12 - AIC:494.907679737394 ARIMA(1, 1, 0)x(1, 0, 0, 12)12 - AIC:482.80837530249664 ARIMA(1, 1, 0)x(1, 0, 1, 12)12 - AIC:1329.3989860683482 ARIMA(1, 1, 0)x(1, 1, 0, 12)12 - AIC:321.2325469884074 ARIMA(1, 1, 1)x(0, 0, 0, 12)12 - AIC:638.7962401173565 ARIMA(1, 1, 1)x(0, 0, 1, 12)12 - AIC:41587.6558850662 ARIMA(1, 1, 1)x(0, 1, 0, 12)12 - AIC:475.71388519565295 ARIMA(1, 1, 1)x(1, 0, 0, 12)12 - AIC:478.6987301743289 ARIMA(1, 1, 1)x(1, 0, 1, 12)12 - AIC:2580.1698313906977 ARIMA(1, 1, 1)x(1, 1, 0, 12)12 - AIC:318.20664790842784 Fitting the ARIMA Model using the Optimal parameters In [21]: mod = sm . tsa . statespace . SARIMAX ( monthly , order = ( 1 , 1 , 1 ), seasonal_order = ( 1 , 1 , 0 , 12 ), enforce_stationarity = False , enforce_invertibility = False ) results = mod . fit () print ( results . summary () . tables [ 1 ]) ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.2479 0.328 0.755 0.450 -0.395 0.891 ma.L1 -0.9389 0.462 -2.030 0.042 -1.845 -0.032 ar.S.L12 -0.6135 0.348 -1.762 0.078 -1.296 0.069 sigma2 7.421e+04 2.68e+04 2.768 0.006 2.17e+04 1.27e+05 ============================================================================== Let's see the model diagnostics to investigate any unusual behavior. In [22]: results . plot_diagnostics ( figsize = ( 16 , 7 )) plt . show () It is not a perfect model. However, our model diagnostics suggests that the model residuals are nearly normally distributed. Validating Forecasts To help us understand the accuracy of our forecasts, we compare predicted sales to real sales of the time series, and we set forecasts to start at 2017–01–01 to the end of the data. In [23]: pred = results . get_prediction ( start = pd . to_datetime ( '2017-01-01' ), dynamic = False ) pred_ci = pred . conf_int () ax = monthly [ '2014' :] . plot ( label = 'observed' ) pred . predicted_mean . plot ( ax = ax , label = 'One-step ahead Forecast' , alpha = . 7 , figsize = ( 14 , 6 )) ax . fill_between ( pred_ci . index , pred_ci . iloc [:, 0 ], pred_ci . iloc [:, 1 ], color = 'k' , alpha = . 2 ) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Office Supplies Sales' ) plt . legend () plt . show () The line plot is showing the observed values compared to the rolling forecast predictions. Overall, our forecasts align with the true values very well, showing an upward trend starts from the beginning of the year and captured the seasonality toward the end of the year. The grey area shows the confidence interval. In [24]: monthly_forecasted = pred . predicted_mean monthly_truth = monthly [ '2017-01-01' :] mse = (( monthly_forecasted - monthly_truth ) ** 2 ) . mean () print ( 'The Mean Squared Error of our forecasts is {} ' . format ( round ( mse , 2 ))) The Mean Squared Error of our forecasts is 65844.6 In [25]: print ( 'The Root Mean Squared Error of our forecasts is {} ' . format ( round ( np . sqrt ( mse ), 2 ))) The Root Mean Squared Error of our forecasts is 256.6 The Root Mean Squared Error (RMSE) shows that our model was able to predict the average daily Office Supplies Sales in the test set within 256.6 of the real sales. Our Office Supplies daily sales range from around 50 to 1350. So, our model works pretty well so far. Producting and Visualizing Forecasts In [26]: pred_uc = results . get_forecast ( steps = 100 ) pred_ci = pred_uc . conf_int () ax = monthly . plot ( label = 'observed' , figsize = ( 14 , 7 )) pred_uc . predicted_mean . plot ( ax = ax , label = 'Forecast' ) ax . fill_between ( pred_ci . index , pred_ci . iloc [:, 0 ], pred_ci . iloc [:, 1 ], color = 'k' , alpha = . 2 ) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Office Supplies Sales' ) plt . legend () plt . show () Our ARIMA prediction model clearly captured the office supplies sales seasonality. As we forecast further out into the future, it is natural for us to become less confident in our predicted values (the expansion of the grey area). The above time series analysis for Office Supplies makes me curious about other categories, and how do they compare with each other over time. Therefore, we are going to compare time series of office supplies and furniture. Time Series of Office Supplies vs. Furniture According to our data, the size of Office Supplies is much larger than the size of Furniture. In [27]: furniture = df . loc [ df [ 'Category' ] == 'Furniture' ] furniture = furniture [[ 'Order Date' , 'Sales' ]] furniture = furniture . sort_values ( 'Order Date' ) furniture = furniture . groupby ( 'Order Date' )[ 'Sales' ] . sum () . reset_index () furniture = furniture . set_index ( 'Order Date' ) furniture . head () Out[27]: Sales Order Date 2014-01-06 2573.820 2014-01-07 76.728 2014-01-10 51.940 2014-01-11 9.940 2014-01-13 879.939 In [28]: monthly_office = monthly monthly_furniture = furniture [ 'Sales' ] . resample ( 'MS' ) . mean () In [29]: furniture = pd . DataFrame ({ 'Order Date' : monthly_furniture . index , 'Sales' : monthly_furniture . values }) office = pd . DataFrame ({ 'Order Date' : monthly_office . index , 'Sales' : monthly_office . values }) store = furniture . merge ( office , how = 'inner' , on = 'Order Date' ) store . rename ( columns = { 'Sales_x' : 'furniture_sales' , 'Sales_y' : 'office_sales' }, inplace = True ) store . head () Out[29]: Order Date furniture_sales office_sales 0 2014-01-01 480.194231 285.357647 1 2014-02-01 367.931600 63.042588 2 2014-03-01 857.291529 391.176318 3 2014-04-01 567.488357 464.794750 4 2014-05-01 432.049188 324.346545 Visualizing the Sales Data In [30]: plt . figure ( figsize = ( 16 , 7 )) plt . plot ( store [ 'Order Date' ], store [ 'furniture_sales' ], 'b-' , label = 'Furniture' ) plt . plot ( store [ 'Order Date' ], store [ 'office_sales' ], 'r-' , label = 'Office Supplies' ) plt . xlabel ( 'Date' ) plt . ylabel ( 'Sales' ) plt . title ( 'Sales Furniture vs. Office Supplies' ) plt . legend () plt . show () We observe that sales of furniture and office supplies shared a similar seasonal pattern. Early of the year is the off season for both of the two categories. It seems summer time is quiet for office supplies too. in addition, average daily sales for furniture are higher than those of office supplies in most of the months. It is understandable, as the value of furniture should be much higher than those of office supplies. Occasionally, office supplies passed furniture on average daily sales. Let's find out when was the first time office supplies' sales surpassed those of furniture's. In [31]: first_date = store . ix [ np . min ( list ( np . where ( store [ 'office_sales' ] > store [ 'furniture_sales' ])[ 0 ])), 'Order Date' ] print ( \"Office supplies first time produced higher sales than furniture is {} .\" . format ( first_date . date ())) Office supplies first time produced higher sales than furniture is 2014-07-01. Time Series Modeling with Prophet Prophet is designed for analyzing time-series that display patterns on different time scales such as yearly, weekly and daily. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints. Therefore, we are using Prophet to get a model up and running. Let's take look at the Prophet forcasting models for Furniture and Office Supplies Sales. In [32]: from fbprophet import Prophet furniture = furniture . rename ( columns = { 'Order Date' : 'ds' , 'Sales' : 'y' }) furniture_model = Prophet ( interval_width = 0.95 ) furniture_model . fit ( furniture ) office = office . rename ( columns = { 'Order Date' : 'ds' , 'Sales' : 'y' }) office_model = Prophet ( interval_width = 0.95 ) office_model . fit ( office ) furniture_forecast = furniture_model . make_future_dataframe ( periods = 36 , freq = 'MS' ) furniture_forecast = furniture_model . predict ( furniture_forecast ) office_forecast = office_model . make_future_dataframe ( periods = 36 , freq = 'MS' ) office_forecast = office_model . predict ( office_forecast ) plt . figure ( figsize = ( 16 , 7 )) furniture_model . plot ( furniture_forecast , xlabel = 'Date' , ylabel = 'Sales' ) plt . title ( 'Furniture Sales' ) plt . show () INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. <Figure size 1152x504 with 0 Axes> In [33]: plt . figure ( figsize = ( 16 , 7 )) office_model . plot ( office_forecast , xlabel = 'Date' , ylabel = 'Sales' ) plt . title ( 'Office Supplies Sales' ) plt . show () <Figure size 1152x504 with 0 Axes> Compare the Two Forecasts We already have the forecasts for three years for these two categories into the future. We will now join them together to compare their future forecasts. In [34]: furniture_names = [ 'furniture_ %s ' % column for column in furniture_forecast . columns ] office_names = [ 'office_ %s ' % column for column in office_forecast . columns ] merge_furniture_forecast = furniture_forecast . copy () merge_office_forecast = office_forecast . copy () merge_furniture_forecast . columns = furniture_names merge_office_forecast . columns = office_names forecast = pd . merge ( merge_furniture_forecast , merge_office_forecast , how = 'inner' , left_on = 'furniture_ds' , right_on = 'office_ds' ) forecast = forecast . rename ( columns = { 'furniture_ds' : 'Date' }) . drop ( 'office_ds' , axis = 1 ) forecast . head () Out[34]: Date furniture_trend furniture_yhat_lower furniture_yhat_upper furniture_trend_lower furniture_trend_upper furniture_additive_terms furniture_additive_terms_lower furniture_additive_terms_upper furniture_yearly ... office_additive_terms office_additive_terms_lower office_additive_terms_upper office_yearly office_yearly_lower office_yearly_upper office_multiplicative_terms office_multiplicative_terms_lower office_multiplicative_terms_upper office_yhat 0 2014-01-01 731.079361 266.970810 823.529872 731.079361 731.079361 -178.836100 -178.836100 -178.836100 -178.836100 ... -132.483942 -132.483942 -132.483942 -132.483942 -132.483942 -132.483942 0.0 0.0 0.0 297.865749 1 2014-02-01 733.206972 133.362486 678.590785 733.206972 733.206972 -324.048145 -324.048145 -324.048145 -324.048145 ... -288.226070 -288.226070 -288.226070 -288.226070 -288.226070 -288.226070 0.0 0.0 0.0 149.595672 2 2014-03-01 735.128684 395.356700 921.716107 735.128684 735.128684 -69.406915 -69.406915 -69.406915 -69.406915 ... 0.829065 0.829065 0.829065 0.829065 0.829065 0.829065 0.0 0.0 0.0 445.399757 3 2014-04-01 737.256294 330.392503 863.616976 737.256294 737.256294 -140.477169 -140.477169 -140.477169 -140.477169 ... -89.156127 -89.156127 -89.156127 -89.156127 -89.156127 -89.156127 0.0 0.0 0.0 362.886617 4 2014-05-01 739.315271 273.750063 839.110706 739.315271 739.315271 -172.355011 -172.355011 -172.355011 -172.355011 ... -183.195734 -183.195734 -183.195734 -183.195734 -183.195734 -183.195734 0.0 0.0 0.0 276.078026 5 rows × 31 columns Trend and Forecast Visualization In [35]: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( forecast [ 'Date' ], forecast [ 'furniture_trend' ], 'b-' ) plt . plot ( forecast [ 'Date' ], forecast [ 'office_trend' ], 'r-' ) plt . legend (); plt . xlabel ( 'Date' ); plt . ylabel ( 'Sales' ) plt . title ( 'Furniture vs. Office Supplies Sales Trend' ); plt . show () In [36]: plt . figure ( figsize = ( 10 , 7 )) plt . plot ( forecast [ 'Date' ], forecast [ 'furniture_yhat' ], 'b-' ) plt . plot ( forecast [ 'Date' ], forecast [ 'office_yhat' ], 'r-' ) plt . legend (); plt . xlabel ( 'Date' ); plt . ylabel ( 'Sales' ) plt . title ( 'Furniture vs. Office Supplies Estimate' ); Trends and Patterns Now, we can use the Prophet Models to inspect different trends of these two categories in the data. In [37]: furniture_model . plot_components ( furniture_forecast ) plt . show () In [38]: office_model . plot_components ( office_forecast ) plt . show () Good to see that the sales for both furniture and office supplies have been linearly increasing over time and will be keep growing, although office supplies' growth seems slightly stronger.","tags":"Blog","url":"https://ericchen23.github.io/2019/02/Time Series Analysis.html","loc":"https://ericchen23.github.io/2019/02/Time Series Analysis.html"},{"title":"Car Crash Prediction in NZ - Machine Learning Pipeline","text":"Car Crash Prediction in NZ - Machine Learning Pipeline 1. Introduction The dataset we use for this post is New Zealand Crash Analysis Dataset which is updated on a quarterly basis by the Transport Agency. The dataset was last updated on October 2018 (from January 2000). It contains all traffic crashes as reported to the Transport Agency by the NZ police. However, not all crashes are reported NZ police. A big portion of minor car crashes are settled on site by the parties without reporting to the police. The level of reporting increases with the severity of the crash. Due to the nature of non-fatal crashes it is believed that these are under-reported. The dataset is available in many different formats and APIs. It is easy to grab them through API interfaces, instead of downloading to your local machine. In this way, we can have the newest version of the dataset every time we run it. If you want to download the dataset to your local machine, it can be found here . The crashes shown in the dataset are strongly related to geography, so we will use the geojson format. So, we can also perform geographic data analysis without creating geometries from latitude and longtitude and deal with coordinate reference systems and projections. And we will also use different machine learning models to predict future car crashes based on the available data. Now, let's get into it. This post is a self-learning project, and the original content can be found here 2. Exploratory Data Analysis (EDA) Import Libraries In [1]: # Import libraries import pandas as pd import numpy as np import geopandas as gpd import matplotlib.pyplot as plt import folium import requests import seaborn as sns Import Data Import the dataset through API. This process can take a while. In [2]: url = 'https://opendata.arcgis.com/datasets/a163c5addf2c4b7f9079f08751bd2e1a_0.geojson' geojson = requests . get ( url ) . json () crs = { 'init' : 'epsg:3851' } #Coordinate reference system for New Zealand gdf = gpd . GeoDataFrame . from_features ( geojson [ 'features' ], crs = crs ) print ( gdf . shape ) gdf . head () (665847, 88) Out[2]: OBJECTID Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 3001 1 0 0 514101 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 3002 1 0 0 514102 0 0 0 0 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 3003 1 0 0 521802 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 3004 0 0 0 508801 0 0 0 0 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 3005 1 0 0 508411 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 88 columns In [3]: gdf . columns Out[3]: Index(['OBJECTID', 'Pedestrian', 'advisorySpeed', 'animals', 'areaUnitID', 'bicycle', 'bridge', 'bus', 'carStationWagon', 'cliffBank', 'cornerRoadSideRoad', 'crashDirectionDescription', 'crashDistance', 'crashFinancialYear', 'crashLocation1', 'crashLocation2', 'crashRPDirectionDescription', 'crashRPDisplacement', 'crashRPNewsDescription', 'crashRPRS', 'crashRPSH', 'crashSHDescription', 'crashSeverity', 'crashYear', 'darkLight', 'debris', 'directionRoleDescription', 'ditch', 'easting', 'fatalCount', 'fence', 'flatHill', 'geometry', 'guardRail', 'holiday', 'houseBuilding', 'intersection', 'intersectionMidblock', 'junctionType', 'kerb', 'light', 'meshblockID', 'minorInjuryCount', 'moped', 'motorcycle', 'multiVehicle', 'northing', 'numberOfLanes', 'objectThrownOrDropped', 'other', 'otherVehicleType', 'outdatedLocationDescription', 'overBank', 'parkedVehicle', 'phoneBoxEtc', 'postOrPole', 'regionDesc', 'roadCharacter', 'roadCurvature', 'roadLane', 'roadMarkings', 'roadSurface', 'roadWet', 'roadworks', 'schoolBus', 'seriousInjuryCount', 'slipFlood', 'speedLimit', 'strayAnimal', 'streetLight', 'suv', 'taxi', 'temporarySpeedLimit', 'tlaID', 'tlaName', 'trafficControl', 'trafficIsland', 'trafficSign', 'train', 'tree', 'truck', 'unknownVehicleType', 'urban', 'vanOrUtility', 'vehicle', 'waterRiver', 'weatherA', 'weatherB'], dtype='object') Important Crash Information Crash information on death and injuries are very important to us. We will first explore the dataset on this part of information. crashSeverity : the severity of a car crash. Possible values are 'F'-fatal, 'S'-serious, 'M'-minor, 'N'-non-injury. This is determined by the worst injury sustained in the crash at the time of entry. seriousInjuryCount : the number of serious injuries associated with a crash. minorInjuryCount : the number of minor injuries associated with a crash. fatalCount : the number of fatal casualties associated with a crash. In [4]: start_year = gdf . crashYear . min () end_year = gdf . crashYear . max () fatal = gdf [ 'fatalCount' ] . sum () serious = gdf [ 'seriousInjuryCount' ] . sum () minor = gdf [ 'minorInjuryCount' ] . sum () print ( \"The total death in car crash accident in New Zealand from {} to {} is {} .\" . format ( start_year , end_year , fatal )) print ( \"While the total number of serious injuries and minor injuries in car crashes is {} , and {} , respectively.\" . format ( serious , minor )) The total death in car crash accident in New Zealand from 2000 to 2018 is 6922. While the total number of serious injuries and minor injuries in car crashes is 45044, and 205895, respectively. In [5]: import warnings warnings . simplefilter ( 'ignore' ) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 25 , 5 )) sns . barplot ( x = 'crashYear' , y = 'fatalCount' , data = gdf , ax = ax [ 0 ], color = 'k' ) ax [ 0 ] . set_title ( \"The number of total death from year 2000 to 2018\" , fontsize = 14 ) sns . barplot ( x = 'crashYear' , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], color = 'r' ) ax [ 1 ] . set_title ( \"The number of total serious injuries from year 2000 to 2018\" , fontsize = 14 ) sns . barplot ( x = 'crashYear' , y = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ], color = 'g' ) ax [ 2 ] . set_title ( \"The number of total minor injuries from year 2000 to 2018\" , fontsize = 14 ) [ ax [ i ] . set_xlabel ( 'Crash Year' ) for i in range ( 3 )] plt . tight_layout () plt . show () In [6]: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) sns . countplot ( x = 'fatalCount' , data = gdf , ax = ax [ 0 ], color = 'k' ) sns . countplot ( x = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], color = 'r' ) sns . countplot ( x = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ], color = 'g' ) plt . tight_layout () plt . show () In [7]: fig , ax = plt . subplots ( figsize = ( 10 , 4 )) sns . countplot ( y = \"crashSeverity\" , data = gdf , order = 'NMSF' , ax = ax ) # Non-injury, Minor, Seriours, Fatal plt . show () In [8]: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 25 , 4 )) sns . lineplot ( x = 'crashYear' , y = 'fatalCount' , data = gdf , ax = ax [ 0 ]) ax [ 0 ] . set_title ( \"The number of total death from year 2000 to 2018\" , fontsize = 14 ) sns . lineplot ( x = 'crashYear' , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ]) ax [ 1 ] . set_title ( \"The number of total serious injuries from year 2000 to 2018\" , fontsize = 14 ) sns . lineplot ( x = 'crashYear' , y = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ]) ax [ 2 ] . set_title ( \"The number of total minor injuries from year 2000 to 2018\" , fontsize = 14 ) [ ax [ i ] . set_xlabel ( 'Crash Year' ) for i in range ( 3 )] plt . tight_layout () plt . show () Road Information These variables, including roadworks, junctionType, roadCharacter, roadCurvature, roadLane, roadMarkings, roadSurface, roadWet, numberOfLanes, intersection, intersectionMidblock, flatHill, darkLight , provide information about the road condition when an accident occurs. Let's plot some of them out to see their patterns. In [9]: fig , ax = plt . subplots ( 3 , 3 , figsize = ( 20 , 10 )) sns . countplot ( x = 'roadCurvature' , data = gdf , ax = ax [ 0 , 0 ]) sns . countplot ( x = \"junctionType\" , data = gdf , ax = ax [ 0 , 1 ]) sns . countplot ( x = 'roadLane' , data = gdf , ax = ax [ 0 , 2 ]) sns . countplot ( x = 'numberOfLanes' , data = gdf , ax = ax [ 1 , 0 ]) sns . countplot ( x = 'roadWet' , data = gdf , ax = ax [ 1 , 1 ]) sns . countplot ( x = 'roadMarkings' , data = gdf , ax = ax [ 1 , 2 ]) sns . countplot ( x = 'roadSurface' , data = gdf , ax = ax [ 2 , 0 ]) sns . countplot ( x = 'darkLight' , data = gdf , ax = ax [ 2 , 1 ]) sns . countplot ( x = 'intersection' , data = gdf , ax = ax [ 2 , 2 ]) #intersectionMidblock plt . tight_layout () NumberOfLanes influence on fatal and serious injuries. In [10]: plt . figure ( figsize = ( 12 , 5 )) sns . barplot ( x = 'numberOfLanes' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' ) plt . figure ( figsize = ( 12 , 5 )) sns . barplot ( x = 'numberOfLanes' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' ) plt . show () JunctionType influence on fatal and serious injuries. In [11]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) sns . barplot ( x = 'junctionType' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'junctionType' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () RoadWet influence on fatal and serious injuries. In [12]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'roadWet' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'roadWet' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () DarkLight influence. Dark Light condition also has a significant influence on fatal and serious injury accident, observed from the figure shown below. In [13]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'darkLight' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'darkLight' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () RoadSurface influence. From the figure shown below, it can be easily observed that unsealed road surface has a much higher fatal or serious injury rate than sealed roads. In [14]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'roadSurface' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'roadSurface' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () Traffic Conditions SpeedLimit with fatal and serious injuries. In [15]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 4 )) sns . barplot ( x = 'speedLimit' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'speedLimit' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () StreetLight with fatal and serious injuries. In [16]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 4 )) sns . barplot ( x = 'streetLight' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'streetLight' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () AdvisorySpeed with fatal and serious injuries. In [17]: sns . catplot ( x = \"advisorySpeed\" , y = \"crashSeverity\" , data = gdf , kind = 'bar' ); plt . show () Weather Conditions Total Weather Counts , weatherA and weatherB . And their influence on fatal and serious injuries. In [18]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . countplot ( x = \"weatherA\" , data = gdf , ax = ax [ 0 ], palette = \"Greens_d\" ); sns . countplot ( x = \"weatherB\" , data = gdf , ax = ax [ 1 ], palette = \"Greens_d\" ); plt . tight_layout () plt . show () In [19]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . barplot ( x = \"weatherA\" , y = 'fatalCount' , data = gdf , ax = ax [ 0 ], palette = \"Reds_d\" ); sns . barplot ( x = \"weatherB\" , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], palette = \"Blues_d\" ); plt . tight_layout () plt . show () Geographic Data Exploration In [20]: gdf . plot ( markersize = 0.01 , edgecolor = 'r' , figsize = ( 12 , 12 )) plt . axis ( 'off' ) Out[20]: (166.67652523873306, 179.10957030210642, -47.52866483091503, -33.806398085455236) In [21]: from folium.plugins import MarkerCluster gdf_sample = gdf . sample ( 5000 ) lons = gdf_sample . geometry . x lats = gdf_sample . geometry . y m = folium . Map ( location = [ np . mean ( lats ), np . mean ( lons )], zoom_start = 5.7 ) #FastMarkerCluster(data=list(zip(lats, lons))).add_to(m) MarkerCluster ( list ( zip ( lats , lons ))) . add_to ( m ) folium . LayerControl () . add_to ( m ) m Out[21]: In [22]: gdf_sample = gdf . sample ( 5000 ) lons = gdf_sample . geometry . x lats = gdf_sample . geometry . y heat_cols = list ( zip ( lats , lons )) from folium.plugins import HeatMap m = folium . Map ([ np . mean ( lats ), np . mean ( lons )], tiles = 'CartoDB dark_matter' , zoom_start = 7 ) HeatMap ( heat_cols ) . add_to ( m ) m Out[22]: Data Modelling We can approach the machine learning part in different ways. We can consider it as a regression problem and predict the number of death or serious injuries. We can also take it as a classification problem and classify the severity of a certain car crash. In this post, we will first approach it as a regression problem. We will try to predict the the fatalCount based on all other variables. Random Forest will be used the the modelling algorithm. It needs only minimal data preprocessing and does well in many cases. In [23]: # import libraries from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # first tranfer our data from geoDataFrame to pandas DataFrame, so that we can use sklearn to work on it. df = pd . DataFrame ( gdf . drop ([ 'geometry' , 'OBJECTID' ], axis = 1 )) df . head () Out[23]: Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank cornerRoadSideRoad ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 1 0 0 514101 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 1 0 0 514102 0 0 0 0 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 1 0 0 521802 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 0 0 0 508801 0 0 0 0 1 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 1 0 0 508411 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 86 columns In [24]: gdf . head () Out[24]: OBJECTID Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 3001 1 0 0 514101 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 3002 1 0 0 514102 0 0 0 0 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 3003 1 0 0 521802 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 3004 0 0 0 508801 0 0 0 0 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 3005 1 0 0 508411 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 88 columns Regression Modeling with Random forest We first need to convert categorical variables into numerical variables, using LabelEncoder and OneHotEncoder. In [25]: from sklearn.preprocessing import LabelEncoder LE = LabelEncoder () for i in df : if df [ i ] . dtype == 'object' : LE . fit ( df [ i ]) df [ i ] = LE . transform ( df [ i ]) df = pd . get_dummies ( df ) df . head () Out[25]: Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank cornerRoadSideRoad ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 1 0 0 514101 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 1 1 0 0 514102 0 0 0 0 0 1 ... 0 0 0 0 2 0 0 0 0 2 2 1 0 0 521802 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 3 0 0 0 508801 0 0 0 0 1 1 ... 0 0 0 0 2 1 0 0 0 2 4 1 0 0 508411 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 5 rows × 86 columns In [26]: # split data into training and validation sets X_train , X_test , y_train , y_test = train_test_split ( df . drop ( 'fatalCount' , axis = 1 ), df . fatalCount , test_size = 0.2 , random_state = 42 ) print (( X_train . shape , X_test . shape , y_train . shape , y_test . shape )) ((532677, 85), (133170, 85), (532677,), (133170,)) In [36]: def rmse ( x , y ): return np . sqrt ((( x - y ) ** 2 ) . mean ()) def print_score ( m ): res = [ rmse ( m . predict ( X_train ), y_train ), rmse ( m . predict ( X_test ), y_test ), m . score ( X_train , y_train ), m . score ( X_test , y_test )] if hasattr ( m , 'oob_score_' ): res . append ( m . oob_score_ ) print ( res ) In [28]: # Initialize the regressor m = RandomForestRegressor ( n_estimators = 50 ) m . fit ( X_train , Y_train ) print_score ( m ) [0.017907117808386587, 0.04273625304671058, 0.9766110961503995, 0.8620394418555027] Feature Importance In [29]: f_imp = pd . DataFrame ( data = { 'importance' : m . feature_importances_ , 'features' : X_train . columns }) . set_index ( 'features' ) f_imp = f_imp . sort_values ( 'importance' , ascending = False ) f_imp . head ( 10 ) Out[29]: importance features crashSeverity 0.855630 crashLocation2 0.010599 easting 0.009086 crashDistance 0.007509 crashRPDisplacement 0.007109 crashRPRS 0.006428 crashLocation1 0.006199 seriousInjuryCount 0.005648 northing 0.005389 meshblockID 0.004877 Clearly, crashSeverity is the most important in our model, because it is derived from the number of deaths. In [30]: f_imp [: 20 ] . plot ( kind = 'bar' , figsize = ( 12 , 10 )) plt . show () Select the variables with importance. In [31]: f_imp_sel = f_imp [ f_imp [ 'importance' ] > 0.0001 ] In [32]: len ( f_imp_sel . index ), len ( f_imp . index ) Out[32]: (73, 85) We have 73 out of 85 variables with importance value over 0.0001. In [33]: y = df . fatalCount df_imp = df [ f_imp_sel . index ] df_imp . shape Out[33]: (665847, 73) Retrain the RandomForestRegressor with only the variables with significance. In [34]: # Let us split our data into training and validation sets X_train , X_test , y_train , y_test = train_test_split ( df_imp , y , test_size = 0.33 , random_state = 42 ) m = RandomForestRegressor ( n_estimators = 50 , oob_score = True ) m . fit ( X_train , y_train ) Out[34]: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None, oob_score=True, random_state=None, verbose=0, warm_start=False) In [37]: print_score ( m ) [0.017239725389319133, 0.04320178481356111, 0.978469926234861, 0.8589594053214539, 0.8404163772494878] Adjusting the parameters of the RF regressor to see if we can get better results. Of course, you can also use a Grid Search Cross-validation to find more suitable parameters. Here, to make it easy, I will not go through this procedure. Because it may take a long time. In [38]: m = RandomForestRegressor ( n_estimators = 100 , min_samples_leaf = 10 , oob_score = True ) m . fit ( X_train , y_train ) print_score ( m ) [0.03777296689755251, 0.04108996727501145, 0.8966413028101987, 0.8724112618047403, 0.8542367058273785]","tags":"Blog","url":"https://ericchen23.github.io/2019/01/Prediction.html","loc":"https://ericchen23.github.io/2019/01/Prediction.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part III","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II Introduction This is the third post of the valuable Matplotlib & Seaborn Visualization Handbook. This post will focus on the last three categories discussed in the last post, as follows: Composition Waffle chart Pie chart Treemap Bar chart Change Time series plot Time series with peaks and troughs annotated Autocorrelation plot Cross correlation plot Time series decomposition plot Multiple time series Plotting with different scales using secondary Y axis Time series with error bands Stacked area chart Unstacked area chart Calender heat map Seasonal plot Groups Dendrogram Cluster plot Andrews curve Parallel coordinates First, import neccessary libraries. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 5. Composition Waffle chart Waffle charts can be used to show the compositional information of groups in a larger population. It can be done using the pywaffle packege. In [2]: from pywaffle import Waffle import warnings warnings . simplefilter ( 'ignore' ) # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data # By Class Data df_class = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts_class' ) n_categories = df_class . shape [ 0 ] colors_class = [ plt . cm . Set3 ( i / float ( n_categories )) for i in range ( n_categories )] # By Cylinders Data df_cyl = df_raw . groupby ( 'cyl' ) . size () . reset_index ( name = 'counts_cyl' ) n_categories = df_cyl . shape [ 0 ] colors_cyl = [ plt . cm . Spectral ( i / float ( n_categories )) for i in range ( n_categories )] # By Make Data df_make = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts_make' ) n_categories = df_make . shape [ 0 ] colors_make = [ plt . cm . tab20b ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '311' : { 'values' : df_class [ 'counts_class' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_class [[ 'class' , 'counts_class' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Class' }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_class }, '312' : { 'values' : df_cyl [ 'counts_cyl' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_cyl [[ 'cyl' , 'counts_cyl' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Cyl' }, 'title' : { 'label' : '# Vehicles by Cyl' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_cyl }, '313' : { 'values' : df_make [ 'counts_make' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_make [[ 'manufacturer' , 'counts_make' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Manufacturer' }, 'title' : { 'label' : '# Vehicles by Make' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_make } }, rows = 9 , figsize = ( 16 , 14 ) ) Pie chart Pie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading, especially for 3D pie chart. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie. Here is a bad example without showing the percentage information of each part. In [3]: # Import data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () # Make the plot with pandas df . plot ( kind = 'pie' , subplots = True , figsize = ( 8 , 8 )) plt . title ( \"Pie Chart of Vehicle Class - Bad\" ) plt . ylabel ( \"\" ) plt . show () Now let's see a good example of using pie chart. It shows the exact compositions of each part. In [4]: # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) # Draw Plot fig , ax = plt . subplots ( figsize = ( 12 , 7 ), subplot_kw = dict ( aspect = \"equal\" ), dpi = 80 ) data = df [ 'counts' ] categories = df [ 'class' ] explode = [ 0 , 0.1 , 0 , 0 , 0 , 0 , 0 ] def func ( pct , allvals ): absolute = int ( pct / 100. * np . sum ( allvals )) return \" {:.1f} % ( {:d} )\" . format ( pct , absolute ) wedges , texts , autotexts = ax . pie ( data , autopct = lambda pct : func ( pct , data ), textprops = dict ( color = \"w\" ), colors = plt . cm . Dark2 . colors , labels = categories . values . tolist (), shadow = False , startangle = 140 , explode = explode ) # Decoration ax . legend ( wedges , categories , title = \"Vehicle Class\" , loc = \"center left\" , bbox_to_anchor = ( 1 , 0 , 0.5 , 1 )) plt . setp ( autotexts , size = 10 , weight = 700 ) ax . set_title ( \"Class of Vehicles: Pie Chart\" ) plt . show () Treemap Treemap is another compistional chart. In my opinion, it is less used than pie chart. In [5]: import squarify # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) labels = df . apply ( lambda x : str ( x [ 0 ]) + \" \\n (\" + str ( x [ 1 ]) + \")\" , axis = 1 ) sizes = df [ 'counts' ] . values . tolist () colors = [ plt . cm . Spectral ( i / float ( len ( labels ))) for i in range ( len ( labels ))] # Draw Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) squarify . plot ( sizes = sizes , label = labels , color = colors , alpha =. 8 ) # Decorate plt . title ( 'Treemap of Vechile Class' ) plt . axis ( 'off' ) plt . show () Bar chart Bar chart, both verticle and horizontal, can give a good presentation of the compositional information. In [6]: import random # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts' ) n = df [ 'manufacturer' ] . unique () . __len__ () + 1 all_colors = list ( plt . cm . colors . cnames . keys ()) random . seed ( 100 ) c = random . choices ( all_colors , k = n ) # Plot Bars plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . bar ( df [ 'manufacturer' ], df [ 'counts' ], color = c , width =. 5 ) for i , val in enumerate ( df [ 'counts' ] . values ): plt . text ( i , val , float ( val ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontdict = { 'fontweight' : 500 , 'size' : 12 }) # Decoration plt . gca () . set_xticklabels ( df [ 'manufacturer' ], rotation = 60 , horizontalalignment = 'right' ) plt . title ( \"Number of Vehicles by Manaufacturers\" , fontsize = 22 ) plt . ylabel ( '# Vehicles' ) plt . ylim ( 0 , 45 ) plt . show () 6. Change Time series plot In [7]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:red' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 12 ] xtick_labels = [ x [ - 4 :] for x in df . date . tolist ()[:: 12 ]] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 0 , fontsize = 12 , horizontalalignment = 'center' , alpha =. 7 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'both' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.3 ) plt . show () Time series with peaks and troughs annotated In [8]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Get the Peaks and Troughs data = df [ 'traffic' ] . values doublediff = np . diff ( np . sign ( np . diff ( data ))) peak_locations = np . where ( doublediff == - 2 )[ 0 ] + 1 doublediff2 = np . diff ( np . sign ( np . diff ( - 1 * data ))) trough_locations = np . where ( doublediff2 == - 2 )[ 0 ] + 1 # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:blue' , label = 'Air Traffic' ) plt . scatter ( df . date [ peak_locations ], df . traffic [ peak_locations ], marker = mpl . markers . CARETUPBASE , color = 'tab:green' , s = 100 , label = 'Peaks' ) plt . scatter ( df . date [ trough_locations ], df . traffic [ trough_locations ], marker = mpl . markers . CARETDOWNBASE , color = 'tab:red' , s = 100 , label = 'Troughs' ) # Annotate for t , p in zip ( trough_locations [ 1 :: 5 ], peak_locations [:: 3 ]): plt . text ( df . date [ p ], df . traffic [ p ] + 15 , df . date [ p ], horizontalalignment = 'center' , color = 'darkgreen' ) plt . text ( df . date [ t ], df . traffic [ t ] - 35 , df . date [ t ], horizontalalignment = 'center' , color = 'darkred' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 6 ] xtick_labels = df . date . tolist ()[:: 6 ] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 45 , fontsize = 12 , alpha =. 7 ) plt . title ( \"Peak and Troughs of Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . legend ( loc = 'upper left' ) plt . grid ( axis = 'y' , alpha =. 3 ) plt . show () Autocorrelation and partial autocorrelation plot The Autocorrelation plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags. Partial autocorrelation plot on the other hand, shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed. In [9]: from statsmodels.graphics.tsaplots import plot_acf , plot_pacf # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 ), dpi = 80 ) plot_acf ( df . traffic . tolist (), ax = ax1 , lags = 50 ) plot_pacf ( df . traffic . tolist (), ax = ax2 , lags = 20 ) # Decorate # lighten the borders ax1 . spines [ \"top\" ] . set_alpha ( . 3 ); ax2 . spines [ \"top\" ] . set_alpha ( . 3 ) ax1 . spines [ \"bottom\" ] . set_alpha ( . 3 ); ax2 . spines [ \"bottom\" ] . set_alpha ( . 3 ) ax1 . spines [ \"right\" ] . set_alpha ( . 3 ); ax2 . spines [ \"right\" ] . set_alpha ( . 3 ) ax1 . spines [ \"left\" ] . set_alpha ( . 3 ); ax2 . spines [ \"left\" ] . set_alpha ( . 3 ) # font size of tick labels ax1 . tick_params ( axis = 'both' , labelsize = 12 ) ax2 . tick_params ( axis = 'both' , labelsize = 12 ) plt . show () Cross correlation plot Cross correlation plot shows the lags of two time series with each other. In [10]: import statsmodels.tsa.stattools as stattools # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) x = df [ 'mdeaths' ] y = df [ 'fdeaths' ] # Compute Cross Correlations ccs = stattools . ccf ( x , y )[: 100 ] nlags = len ( ccs ) # Compute the Significance level # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-significance-in-r/3128#3128 conf_level = 2 / np . sqrt ( nlags ) # Draw Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . hlines ( 0 , xmin = 0 , xmax = 100 , color = 'gray' ) # 0 axis plt . hlines ( conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . hlines ( - conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . bar ( x = np . arange ( len ( ccs )), height = ccs , width =. 3 ) # Decoration plt . title ( '$Cross\\; Correlation\\; Plot:\\; mdeaths\\; vs\\; fdeaths$' , fontsize = 22 ) plt . xlim ( 0 , len ( ccs )) plt . show () Time series decomposition plot Time series decomposition plot shows the break down of the time series into trend, seasonal and residual components. In [11]: from statsmodels.tsa.seasonal import seasonal_decompose from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) dates = pd . DatetimeIndex ([ parse ( d ) . strftime ( '%Y-%m-01' ) for d in df [ 'date' ]]) df . set_index ( dates , inplace = True ) # Decompose result = seasonal_decompose ( df [ 'traffic' ], model = 'multiplicative' ) # Plot plt . rcParams . update ({ 'figure.figsize' : ( 10 , 10 )}) result . plot () . suptitle ( 'Time Series Decomposition of Air Passengers' ) plt . show () Multiple time series In [12]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) # Define the upper limit, lower limit, interval of Y axis and colors y_LL = 100 y_UL = int ( df . iloc [:, 1 :] . max () . max () * 1.1 ) y_interval = 400 mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] for i , column in enumerate ( columns ): plt . plot ( df . date . values , df [ column ], lw = 1.5 , color = mycolors [ i ]) plt . text ( df . shape [ 0 ] + 1 , df [ column ] . values [ - 1 ], column , fontsize = 14 , color = mycolors [ i ]) # Draw Tick lines for y in range ( y_LL , y_UL , y_interval ): plt . hlines ( y , xmin = 0 , xmax = 71 , colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Decorations plt . tick_params ( axis = \"both\" , which = \"both\" , bottom = False , top = False , labelbottom = True , left = False , right = False , labelleft = True ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Number of Deaths from Lung Diseases in the UK (1974-1979)' , fontsize = 22 ) plt . yticks ( range ( y_LL , y_UL , y_interval ), [ str ( y ) for y in range ( y_LL , y_UL , y_interval )], fontsize = 12 ) plt . xticks ( range ( 0 , df . shape [ 0 ], 12 ), df . date . values [:: 12 ], horizontalalignment = 'left' , fontsize = 12 ) plt . ylim ( y_LL , y_UL ) plt . xlim ( - 2 , 80 ) plt . show () Plotting with different scales using secondary Y axis In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) x = df [ 'date' ] y1 = df [ 'psavert' ] y2 = df [ 'unemploy' ] # Plot Line1 (Left Y Axis) fig , ax1 = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax1 . plot ( x , y1 , color = 'tab:blue' ) # Plot Line2 (Right Y Axis) ax2 = ax1 . twinx () # instantiate a second axes that shares the same x-axis ax2 . plot ( x , y2 , color = 'tab:red' ) # Decorations # ax1 (left Y axis) ax1 . set_xlabel ( 'Year' , fontsize = 20 ) ax1 . tick_params ( axis = 'x' , rotation = 0 , labelsize = 12 ) ax1 . set_ylabel ( 'Personal Savings Rate' , color = 'tab:blue' , fontsize = 20 ) ax1 . tick_params ( axis = 'y' , rotation = 0 , labelcolor = 'tab:blue' ) ax1 . grid ( alpha =. 4 ) # ax2 (right Y axis) ax2 . set_ylabel ( \"# Unemployed (1000's)\" , color = 'tab:red' , fontsize = 20 ) ax2 . tick_params ( axis = 'y' , labelcolor = 'tab:red' ) ax2 . set_xticks ( np . arange ( 0 , len ( x ), 60 )) ax2 . set_xticklabels ( x [:: 60 ], rotation = 90 , fontdict = { 'fontsize' : 10 }) ax2 . set_title ( \"Personal Savings Rate vs Unemployed: Plotting in Secondary Y Axis\" , fontsize = 22 ) fig . tight_layout () plt . show () Time series with error bands In [14]: from scipy.stats import sem # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\" ) df_mean = df . groupby ( 'order_hour_of_day' ) . quantity . mean () df_se = df . groupby ( 'order_hour_of_day' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . ylabel ( \"# Orders\" , fontsize = 16 ) x = df_mean . index plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 2 ], [ str ( d ) for d in x [:: 2 ]] , fontsize = 12 ) plt . title ( \"User Orders by Hour of Day (95 % c onfidence)\" , fontsize = 22 ) plt . xlabel ( \"Hour of Day\" ) s , e = plt . gca () . get_xlim () plt . xlim ( s , e ) # Draw Horizontal Tick lines for y in range ( 8 , 20 , 2 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show () Stacked area chart Stacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other. In [15]: # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/nightvisitors.csv' ) # Decide Colors mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] labs = columns . values . tolist () # Prepare data x = df [ 'yearmon' ] . values . tolist () y0 = df [ columns [ 0 ]] . values . tolist () y1 = df [ columns [ 1 ]] . values . tolist () y2 = df [ columns [ 2 ]] . values . tolist () y3 = df [ columns [ 3 ]] . values . tolist () y4 = df [ columns [ 4 ]] . values . tolist () y5 = df [ columns [ 5 ]] . values . tolist () y6 = df [ columns [ 6 ]] . values . tolist () y7 = df [ columns [ 7 ]] . values . tolist () y = np . vstack ([ y0 , y2 , y4 , y6 , y7 , y5 , y1 , y3 ]) # Plot for each column labs = columns . values . tolist () ax = plt . gca () ax . stackplot ( x , y , labels = labs , colors = mycolors , alpha = 0.8 ) # Decorations ax . set_title ( 'Night Visitors in Australian Regions' , fontsize = 18 ) ax . set ( ylim = [ 0 , 100000 ]) ax . legend ( fontsize = 10 , ncol = 4 ) plt . xticks ( x [:: 5 ], fontsize = 10 , rotation = 45 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 10000 , 100000 , 20000 ), fontsize = 10 ) plt . xlim ( x [ 0 ], x [ - 1 ]) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Area chart unstacked An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In [16]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) # Prepare Data x = df [ 'date' ] . values . tolist () y1 = df [ 'psavert' ] . values . tolist () y2 = df [ 'uempmed' ] . values . tolist () mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] columns = [ 'psavert' , 'uempmed' ] # Draw Plot fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 7 ), dpi = 80 ) ax . fill_between ( x , y1 = y1 , y2 = 0 , label = columns [ 1 ], alpha = 0.5 , color = mycolors [ 1 ], linewidth = 2 ) ax . fill_between ( x , y1 = y2 , y2 = 0 , label = columns [ 0 ], alpha = 0.5 , color = mycolors [ 0 ], linewidth = 2 ) # Decorations ax . set_title ( 'Personal Savings Rate vs Median Duration of Unemployment' , fontsize = 18 ) ax . set ( ylim = [ 0 , 30 ]) ax . legend ( loc = 'best' , fontsize = 12 ) plt . xticks ( x [:: 50 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 2.5 , 30.0 , 2.5 ), fontsize = 10 ) plt . xlim ( - 10 , x [ - 1 ]) # Draw Tick lines for y in np . arange ( 2.5 , 30.0 , 2.5 ): plt . hlines ( y , xmin = 0 , xmax = len ( x ), colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Calender heat map Calendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely. In [17]: import matplotlib as mpl import calmap # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\" , parse_dates = [ 'date' ]) df . set_index ( 'date' , inplace = True ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) calmap . calendarplot ( df [ '2014' ][ 'VIX.Close' ], fig_kws = { 'figsize' : ( 16 , 10 )}, yearlabel_kws = { 'color' : 'black' , 'fontsize' : 14 }, subplot_kws = { 'title' : 'Yahoo Stock Prices' }) plt . show () <Figure size 1280x800 with 0 Axes> Seasonal plot The seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc). In [18]: from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Prepare data df [ 'year' ] = [ parse ( d ) . year for d in df . date ] df [ 'month' ] = [ parse ( d ) . strftime ( '%b' ) for d in df . date ] years = df [ 'year' ] . unique () # Draw Plot mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' , 'deeppink' , 'steelblue' , 'firebrick' , 'mediumseagreen' ] plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) for i , y in enumerate ( years ): plt . plot ( 'month' , 'traffic' , data = df . loc [ df . year == y , :], color = mycolors [ i ], label = y ) plt . text ( df . loc [ df . year == y , :] . shape [ 0 ] -. 9 , df . loc [ df . year == y , 'traffic' ][ - 1 :] . values [ 0 ], y , fontsize = 12 , color = mycolors [ i ]) # Decoration plt . ylim ( 50 , 750 ) plt . xlim ( - 0.3 , 11 ) plt . ylabel ( '$Air Traffic$' ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'y' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.5 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.5 ) # plt.legend(loc='upper right', ncol=2, fontsize=12) plt . show () 7. Groups Dendrogram A Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point's similarity. In [19]: import scipy.cluster.hierarchy as shc # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) plt . title ( \"USArrests Dendograms\" , fontsize = 22 ) dend = shc . dendrogram ( shc . linkage ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]], method = 'ward' ), labels = df . State . values , color_threshold = 100 ) plt . xticks ( fontsize = 12 ) plt . show () Cluster plot In [20]: from sklearn.cluster import AgglomerativeClustering from scipy.spatial import ConvexHull # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Agglomerative Clustering cluster = AgglomerativeClustering ( n_clusters = 5 , affinity = 'euclidean' , linkage = 'ward' ) cluster . fit_predict ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]]) # Plot plt . figure ( figsize = ( 13 , 8 ), dpi = 80 ) plt . scatter ( df . iloc [:, 0 ], df . iloc [:, 1 ], c = cluster . labels_ , cmap = 'tab10' ) # Encircle def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Draw polygon surrounding vertices encircle ( df . loc [ cluster . labels_ == 0 , 'Murder' ], df . loc [ cluster . labels_ == 0 , 'Assault' ], ec = \"k\" , fc = \"gold\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 1 , 'Murder' ], df . loc [ cluster . labels_ == 1 , 'Assault' ], ec = \"k\" , fc = \"tab:blue\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 2 , 'Murder' ], df . loc [ cluster . labels_ == 2 , 'Assault' ], ec = \"k\" , fc = \"tab:red\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 3 , 'Murder' ], df . loc [ cluster . labels_ == 3 , 'Assault' ], ec = \"k\" , fc = \"tab:green\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 4 , 'Murder' ], df . loc [ cluster . labels_ == 4 , 'Assault' ], ec = \"k\" , fc = \"tab:orange\" , alpha = 0.2 , linewidth = 0 ) # Decorations plt . xlabel ( 'Murder' ); plt . xticks ( fontsize = 12 ) plt . ylabel ( 'Assault' ); plt . yticks ( fontsize = 12 ) plt . title ( 'Agglomerative Clustering of USArrests (5 Groups)' , fontsize = 22 ) plt . show () Andrews curve Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn't help discriminate the group (cyl), then the lines will not be well segregated as you see below. In [21]: from pandas.plotting import andrews_curves # Import df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) df . drop ([ 'cars' , 'carname' ], axis = 1 , inplace = True ) # Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) andrews_curves ( df , 'cyl' , colormap = 'Set1' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Andrews Curves of mtcars' , fontsize = 22 ) plt . xlim ( - 3 , 3 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Parallel coordinates Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group. In [22]: from pandas.plotting import parallel_coordinates # Import Data df_final = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\" ) # Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) parallel_coordinates ( df_final , 'cut' , colormap = 'Dark2' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Parallel Coordinated of Diamonds' , fontsize = 18 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","tags":"Visualization","url":"https://ericchen23.github.io/2018/12/50-visualization-part3.html","loc":"https://ericchen23.github.io/2018/12/50-visualization-part3.html"},{"title":"Twitter Sentiment Modeling on Detecting Racist or Sexist tweets","text":"Twitter Sentiment Modeling on Detecting Racist or Sexist tweets 1. Introduction Twitter is one of the most popular social media nowadays. It provides online news and social networking service on which users post and interact with messages known as \"tweets\". Registered users can post, like, and retweet tweets, but unregistered users can only read them. Sentiment Analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. (Source: Wikipedia) Sentiment analysis on tweets can be extremely useful. By analyzing the tweets, we can find the sentiment of people on certain affair, understand people's opinion. Also it can help us make right strategies/reactions. In this post, our main goal is to build a model to identify tweets with racist or sexist sentiment. 2. The data we use We have a training data of 31,962 tweets with labeled sentiments, the dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet. Label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist. We also have a test data of 17,197 tweets. The test data file contains only tweet ids and the tweet text with each tweet in a new line. The dataset we use can be download from here . 3. Tweets cleaning and preprocessing Load tweets In [1]: # importing libraries import pandas as pd import numpy as np import re import string import nltk import warnings warnings . filterwarnings ( \"ignore\" ) #import plotting libraries import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline # loading training and test tweets train = pd . read_csv ( 'data \\\\ train_E6oV3lV.csv' ) test = pd . read_csv ( 'data \\\\ test_tweets_anuFYb8.csv' ) print ( \"The size of the training tweets is {} \" . format ( train . shape )) print ( \"The size of the test set is {} \" . format ( test . shape )) The size of the training tweets is (31962, 3) The size of the test set is (17197, 2) Let's take a look at the first 5 rows of the training data. In [2]: train . head () Out[2]: id label tweet 0 1 0 @user when a father is dysfunctional and is s... 1 2 0 @user @user thanks for #lyft credit i can't us... 2 3 0 bihday your majesty 3 4 0 #model i love u take with u all the time in ... 4 5 0 factsguide: society now #motivation In [3]: train [ 'label' ] . value_counts () Out[3]: 0 29720 1 2242 Name: label, dtype: int64 We caclulated the number of tweets of each label in our training data. As can be seen, it is quite uneven. We have to take this into account when building our prediction model. The data has 3 columns id , label , and tweet . id is a unique id of each tweet. label is the binary target variable, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist. tweet contains the tweets that we will clean and preprocess. Tweets cleaning After taking a look at the first 5 records, we can have some initial thoughts regards to cleaning our training data: The Twitter handles are already masked as @user due to privacy concerns. So, these Twitter handles are hardly giving any information about the nature of the tweet and they need to be removed. The punctuations, numbers and special characters also need to be removed. Short words, like \"is\", \"are\", \"for\", \"and\" and so on, which do not have much value to the tweets also need to be cleaned. Tokenization. Tokenization is an essential step in any NLP task. Tokenization splits every tweets into individual tokens/words. Stemming/lemmatizing. Reduce similar words (different variations of words of the same root). For example, 'love', 'loving', 'loves' etc. are from the same root 'love'. We can reduce the total number of unique words in our data without losing a significant amount of information. 1. Removing handles (@user) For our convience, let's combine train and test set together to perform cleaning. In [4]: all_tweets = train . append ( test , ignore_index = True ) We define a function _remove pattern to remove unwanted patterns in our tweets data. In our case, the unwanted pattern is '@user'. In [5]: # function to remove unwanted patterns def remove_pattern ( input_txt , pattern ): matches = re . findall ( pattern , input_txt ) for match in matches : input_txt = re . sub ( match , '' , input_txt ) return input_txt Now let's create a new column tidy_tweet , it will contain the cleaned and processed tweets. Note that we have passed \"@[\\w]*\" as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with '@'. In [6]: # remove twitter handles (@username) all_tweets [ 'tidy_tweet' ] = np . vectorize ( remove_pattern )( all_tweets [ 'tweet' ], \"@[\\w]*\" ) 2. Romving punctuations, numbers, and special characters We also use regular expressions to remove punctuations, numbers, and special characters in this step. In [7]: # remove punctuations, numbers, and special characters all_tweets [ 'tidy_tweet' ] = all_tweets [ 'tidy_tweet' ] . str . replace ( \"[&#94;a-zA-Z#]\" , \" \" ) 3. Removing short words Some short words are of little useful information. So we need to remove them from our tweets data. Let's have a look at the data after the three cleaning steps. In [8]: all_tweets [ 'tidy_tweet' ] = all_tweets [ 'tidy_tweet' ] . apply ( lambda x : ' ' . join ([ w for w in x . split () if len ( w ) > 3 ])) all_tweets . head () Out[8]: id label tweet tidy_tweet 0 1 0.0 @user when a father is dysfunctional and is s... when father dysfunctional selfish drags kids i... 1 2 0.0 @user @user thanks for #lyft credit i can't us... thanks #lyft credit cause they offer wheelchai... 2 3 0.0 bihday your majesty bihday your majesty 3 4 0.0 #model i love u take with u all the time in ... #model love take with time 4 5 0.0 factsguide: society now #motivation factsguide society #motivation As can be clearly seen, the tweets in _tidy tweet column are much shorter than the original ones. They only contain important words and the noise(numbers, punctuations, and special characters) has been removed effectively. 4. Tokenization In [9]: # tokenization tokenized_tweet = all_tweets [ 'tidy_tweet' ] . apply ( lambda x : x . split ()) tokenized_tweet . head () Out[9]: 0 [when, father, dysfunctional, selfish, drags, ... 1 [thanks, #lyft, credit, cause, they, offer, wh... 2 [bihday, your, majesty] 3 [#model, love, take, with, time] 4 [factsguide, society, #motivation] Name: tidy_tweet, dtype: object 5. Stemming/Lemmatizing Stemming is a rule-based process of stripping the suffixes (\"ing\", \"ly\", \"es\", \"s\" etc) from a word. In [10]: from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer () tokenized_tweet = tokenized_tweet . apply ( lambda x : [ stemmer . stem ( i ) for i in x ]) tokenized_tweet . head () Out[10]: 0 [when, father, dysfunct, selfish, drag, kid, i... 1 [thank, #lyft, credit, caus, they, offer, whee... 2 [bihday, your, majesti] 3 [#model, love, take, with, time] 4 [factsguid, societi, #motiv] Name: tidy_tweet, dtype: object Now let's stitch these tokens back together. In [11]: for i in range ( len ( tokenized_tweet )): tokenized_tweet [ i ] = ' ' . join ( tokenized_tweet [ i ]) all_tweets [ 'tidy_tweet' ] = tokenized_tweet all_tweets . head () Out[11]: id label tweet tidy_tweet 0 1 0.0 @user when a father is dysfunctional and is s... when father dysfunct selfish drag kid into dys... 1 2 0.0 @user @user thanks for #lyft credit i can't us... thank #lyft credit caus they offer wheelchair ... 2 3 0.0 bihday your majesty bihday your majesti 3 4 0.0 #model i love u take with u all the time in ... #model love take with time 4 5 0.0 factsguide: society now #motivation factsguid societi #motiv 4. Insights and visualization from cleaned tweets A few questions we are gonna ask are: What are the most common words in the entire dataset? What are the most common words in the dataset for negative and positive tweets, respectively? How many hashtags are there in a tweet? Which trends are associated with my dataset? Which trends are associated with either of the sentiments? Are they compatible with the sentiments? 1. Most common words: WordCloud Wordcloud is an useful tool in finding the most common words appreared in the dataset. The most frequent words apprear in larger sizes and the less common words appear in smaller sizes. In [12]: all_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ]]) from wordcloud import WordCloud wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( all_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () We can see most of the words are positive or neutral. With happy , thank , today and love being the most frequent ones. t doesn't give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes(racist/sexist or not) in our train data. 2. Most common words in normal/positive tweets In [13]: normal_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ][ all_tweets [ 'label' ] == 0 ]]) wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( normal_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( 'off' ) plt . show () As expected, the most common words in normal/positive tweets are either positive or neutral. 3. Racist/sexist tweets In [14]: negative_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ][ all_tweets [ 'label' ] == 1 ]]) wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( negative_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data. 4. Exploring hashtags on tweets sentiment Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. The hashtags may have values in helping us finding negative tweets. We will first store all the hashtags into two separate lists - one for normal/positive tweets and the other for tweets contain racist/sexist. In [15]: # function to collect hashtags def hashtag_extract ( x ): hashtags = [] # loop over the words in the tweet for i in x : ht = re . findall ( r \"#(\\w+)\" , i ) hashtags . append ( ht ) return hashtags In [16]: #extracting hashtags from non racist/sexist tweets HT_regular = hashtag_extract ( all_tweets [ 'tidy_tweet' ] [ all_tweets [ 'label' ] == 0 ]) # extracting hashtags from racist/sexist tweets HT_negative = hashtag_extract ( all_tweets [ 'tidy_tweet' ] [ all_tweets [ 'label' ] == 1 ]) # unnesting list HT_regular = sum ( HT_regular , []) HT_negative = sum ( HT_negative , []) Top hashtags in non-racist/sexist tweets In [17]: a = nltk . FreqDist ( HT_regular ) d = pd . DataFrame ({ 'Hashtag' : list ( a . keys ()), 'Count' : list ( a . values ())}) # selecting top 10 most frequent hashtags d = d . nlargest ( columns = 'Count' , n = 10 ) plt . figure ( figsize = ( 16 , 5 )) ax = sns . barplot ( data = d , x = \"Hashtag\" , y = \"Count\" ) ax . set ( ylabel = 'Count' ) plt . show () Top hastags in racist/sexist tweets In [18]: b = nltk . FreqDist ( HT_negative ) e = pd . DataFrame ({ 'Hashtag' : list ( b . keys ()), 'Count' : list ( b . values ())}) # selecting top 10 most frequent hashtags e = e . nlargest ( columns = \"Count\" , n = 10 ) plt . figure ( figsize = ( 16 , 5 )) ax = sns . barplot ( data = e , x = \"Hashtag\" , y = \"Count\" ) ax . set ( ylabel = 'Count' ) plt . show () As can be seen, the positive/regular hashtags are positive or neutral, and the negative hashtags are really negative. 5. Feature extraction from processed tweets Depending upon the usage, text features can be constructed using assorted techniques – Bag-of-Words, term frequency–inverse document frequency (TF-IDF), and Word Embeddings. In this post, we will be covering only Bag-of-Words and TF-IDF. Bag-of-words features Bag-of-Words is a method to convert text into numerical features. Bag-of-Words features can be easily created using sklearn's CountVectorizer function. We will set the parameter max_features = 1000 to select only top 1000 terms ordered by term frequency across the corpus. In [19]: from sklearn.feature_extraction.text import CountVectorizer bow_vectorizer = CountVectorizer ( max_df = 0.9 , min_df = 2 , max_features = 1000 , stop_words = 'english' ) # bag-of-words feature matrix bow = bow_vectorizer . fit_transform ( all_tweets [ 'tidy_tweet' ]) TF-IDF features Term frequency–inverse document frequency (TF-IDF), takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus. TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents. TF = (number of appearance of a term t)/(number of total terms in the document) IDF = log(N/n), where N is the number of documents and n is the number of documents a term t has appeared in. TF-IDF = TF * IDF In [20]: from sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer = TfidfVectorizer ( max_df = 0.9 , min_df = 2 , max_features = 1000 , stop_words = 'english' ) # TF-IDF feature matrix tfidf = tfidf_vectorizer . fit_transform ( all_tweets [ 'tidy_tweet' ]) 6. Sentiment modeling Since this is an binary prediction, we will use Logistic Regression to build our prediction model. It predicts the probability of occurrence of fitting data into logit function. As mentioned before, the distribution in our training data is quite uneven. So simply using prediction accuracy/precision is not able to correctly evaluate our model's performance. Here, we use F1_score which can be more useful in this situation. This metric can be understood as: Precision = TP/TP+FP Recall = TP/TP+FN F1 Score = 2 (Recall Precision) / (Recall + Precision) Where TP - true positive, FP - false positive, TN - true negative, FN - false negative. Building a model using bag-of-words features In [21]: from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score train_bow = bow [: 31962 , :] test_bow = bow [ 31962 :, :] # splitting the train_bow into training and validation set xtrain_bow , xvalid_bow , ytrain , yvalid = train_test_split ( train_bow , train [ 'label' ], random_state = 42 , test_size = 0.3 ) lreg = LogisticRegression () lreg . fit ( xtrain_bow , ytrain ) # make prediction on validation set prediction = lreg . predict_proba ( xvalid_bow ) prediction_int = prediction [:, 1 ] >= 0.3 #if prediction is greater than or equal to 0.3 than 1 else 0 prediction_int = prediction_int . astype ( np . int ) f1 = f1_score ( yvalid , prediction_int ) #calculate f1_score print ( \"The f1-score for using only the bag-of-words features is : {} \" . format ( f1 )) The f1-score for using only the bag-of-words features is : 0.5307820299500832 In [22]: test_pred = lreg . predict_proba ( test_bow ) test_pred_int = test_pred >= 0.3 test_pred_int = test_pred_int . astype ( np . int ) # test['label'] = test_pred_int # submission = test[['id','label']] # submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file Building a model using TF-IDF features In [23]: train_tfidf = tfidf [: 31962 , :] test_tfidf = tfidf [ 31962 :, :] xtrain_tfidf = train_tfidf [ ytrain . index ] xvalid_tfidf = train_tfidf [ yvalid . index ] lreg . fit ( xtrain_tfidf , ytrain ) prediction = lreg . predict_proba ( xvalid_tfidf ) prediction_int = prediction [:, 1 ] >= 0.3 prediction_int = prediction_int . astype ( np . int ) f1 = f1_score ( yvalid , prediction_int ) print ( \"The f1-score for using only the TF-IDF features is : {} \" . format ( f1 )) The f1-score for using only the TF-IDF features is : 0.5446507515473032 The output is about 0.545 by using the TF-IDF features which is pretty good. The public leader board F1 score is 0.564.","tags":"Blog","url":"https://ericchen23.github.io/2018/12/NLP-twitter-sentiment.html","loc":"https://ericchen23.github.io/2018/12/NLP-twitter-sentiment.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II Introduction This is the second post of the valuable Matplotlib & Seaborn Visualization Handbook. This post will focus on the third and forth categories discussed in the last post, as follows: Ranking Ordered bar chart Lollipop chart Dot plot Slope chart Dumbbell plot Distribution Histogram for continuous variables Histogram for categorical variables Density plot Density curves with histogram Joy plot Distributed dot plot Box plot Dot+box plot Violin plot Population pyramid Categorical plots Before getting into it, let's first import neccessary libraries and plot settings. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 3. Ranking Ordered bar chart Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself. In [2]: import matplotlib.patches as patches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) fig , ax = plt . subplots ( figsize = ( 16 , 10 ), facecolor = 'white' , dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 20 ) # Annotate Text for i , cty in enumerate ( df . cty ): ax . text ( i , cty + 0.5 , round ( cty , 1 ), horizontalalignment = 'center' ) # Title, Label, Ticks and Ylim ax . set_title ( 'Bar Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set ( ylabel = 'Miles Per Gallon' , ylim = ( 0 , 30 )) plt . xticks ( df . index , df . manufacturer . str . upper (), rotation = 60 , horizontalalignment = 'right' , fontsize = 12 ) # Add patches to color the X axis labels fig . patches . extend ([ plt . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure )]) fig . patches . extend ([ plt . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure )]) plt . show () Lollipop chart Lollipop chart serves a similar purpose as a ordered bar chart. In [3]: # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 14 , 10 ), dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 2 ) ax . scatter ( x = df . index , y = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Lollipop Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_ylabel ( 'Miles Per Gallon' ) ax . set_xticks ( df . index ) ax . set_xticklabels ( df . manufacturer . str . upper (), rotation = 60 , fontdict = { 'horizontalalignment' : 'right' , 'size' : 12 }) ax . set_ylim ( 0 , 30 ) # Annotate for row in df . itertuples (): ax . text ( row . Index , row . cty +. 5 , s = round ( row . cty , 2 ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontsize = 14 ) fig . patches . extend ([ plt . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure )]) fig . patches . extend ([ plt . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure )]) plt . show () Dot Plot The dot plot is another way to present rankings. An example is presented below. In [4]: # Import data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Plot fig , ax = plt . subplots ( figsize = ( 14 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 11 , xmax = 26 , color = 'gray' , alpha = 0.7 , linewidth = 1 , linestyle = 'dashdot' ) ax . scatter ( y = df . index , x = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Decorations ax . set_title ( 'Dot plot for Highway Mileage' , fontdict = { 'size' : 18 }) ax . set_xlabel ( 'Miles Per Gallon' ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }) ax . set_xlim ( 10 , 27 ) plt . show () Slope chart Slope chart is most suitable for comparing the ‘Before' and ‘After' positions of a given person/item. In [5]: import matplotlib.lines as mlines # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\" ) left_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1952' ])] right_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1957' ])] klass = [ 'red' if ( y1 - y2 ) < 0 else 'green' for y1 , y2 in zip ( df [ '1952' ], df [ '1957' ])] # draw line # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941 def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'red' if p1 [ 1 ] - p2 [ 1 ] > 0 else 'green' , marker = 'o' , markersize = 6 ) ax . add_line ( l ) return l fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), dpi = 80 ) # Vertical Lines ax . vlines ( x = 1 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x = 3 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ '1952' ], x = np . repeat ( 1 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) ax . scatter ( y = df [ '1957' ], x = np . repeat ( 3 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) # Line Segmentsand Annotation for p1 , p2 , c in zip ( df [ '1952' ], df [ '1957' ], df [ 'continent' ]): newline ([ 1 , p1 ], [ 3 , p2 ]) ax . text ( 1 - 0.05 , p1 , c + ', ' + str ( round ( p1 )), horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) ax . text ( 3 + 0.05 , p2 , c + ', ' + str ( round ( p2 )), horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) # 'Before' and 'After' Annotations ax . text ( 1 - 0.05 , 13000 , 'BEFORE' , horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) ax . text ( 3 + 0.05 , 13000 , 'AFTER' , horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) # Decoration ax . set_title ( \"Slopechart: Comparing GDP Per Capita between 1952 vs 1957\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , 4 ), ylim = ( 0 , 14000 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ 1 , 3 ]) ax . set_xticklabels ([ \"1952\" , \"1957\" ]) plt . yticks ( np . arange ( 500 , 13000 , 2000 ), fontsize = 12 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 0 ) plt . show () Dumbell plot Dumbell plot conveys the ‘before' and ‘after' positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects. In [6]: import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/health.csv\" ) df . sort_values ( 'pct_2014' , inplace = True ) df . reset_index ( inplace = True ) # Func to draw line segment def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'skyblue' ) ax . add_line ( l ) return l # Figure and Axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), facecolor = '#f7f7f7' , dpi = 80 ) # Vertical Lines ax . vlines ( x =. 05 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 10 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 15 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 20 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2013' ], s = 50 , color = '#0e668b' , alpha = 0.7 ) ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2014' ], s = 50 , color = '#a3c4dc' , alpha = 0.7 ) # Line Segments for i , p1 , p2 in zip ( df [ 'index' ], df [ 'pct_2013' ], df [ 'pct_2014' ]): newline ([ p1 , i ], [ p2 , i ]) # Decoration ax . set_facecolor ( '#f7f7f7' ) ax . set_title ( \"Dumbell Chart: Pct Change - 2013 vs 2014\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , . 25 ), ylim = ( - 1 , 27 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ . 05 , . 1 , . 15 , . 20 ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) plt . show () 4. Distribution Histogram for continuous variable Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem. In [7]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'displ' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , 30 , stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 25 ) plt . xticks ( bins [:: 3 ], [ round ( b , 1 ) for b in bins [:: 3 ]]) plt . show () Histogram for categorical variable The histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors. In [8]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'manufacturer' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , df [ x_var ] . unique () . __len__ (), stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 40 ) plt . xticks ( bins , np . unique ( df [ x_var ]) . tolist (), rotation = 90 , horizontalalignment = 'left' ) plt . show () Density plot Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the ‘response' variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders. In [9]: import warnings warnings . filterwarnings ( \"ignore\" ) # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 4 , \"cty\" ], shade = True , color = \"g\" , label = \"Cyl=4\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 5 , \"cty\" ], shade = True , color = \"deeppink\" , label = \"Cyl=5\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 6 , \"cty\" ], shade = True , color = \"dodgerblue\" , label = \"Cyl=6\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 8 , \"cty\" ], shade = True , color = \"orange\" , label = \"Cyl=8\" , alpha =. 7 ) # Decoration plt . title ( 'Density Plot of City Mileage by n_Cylinders' , fontsize = 18 ) plt . xlabel ( 'City Mileage' , fontsize = 18 ) plt . ylabel ( 'Density' , fontsize = 18 ) plt . legend () plt . show () Density curves with histogram Density curve with histogram brings together the collective information conveyed by the two plots so you can have and compare them both in a single figure instead of two. In [10]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 8 ), dpi = 80 ) sns . distplot ( df . loc [ df [ 'class' ] == 'compact' , \"cty\" ], color = \"dodgerblue\" , label = \"Compact\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'suv' , \"cty\" ], color = \"orange\" , label = \"SUV\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'minivan' , \"cty\" ], color = \"g\" , label = \"minivan\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) plt . ylim ( 0 , 0.35 ) # Decoration plt . title ( 'Density Plot of City Mileage by Vehicle Type' , fontsize = 18 ) plt . xlabel ( 'City Mileage' , fontsize = 18 ) plt . ylabel ( 'Density' , fontsize = 18 ) plt . legend () plt . show () Joy plot Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib In [11]: import joypy # Import Data mpg = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 14 , 9 ), dpi = 80 ) fig , axes = joypy . joyplot ( mpg , column = [ 'hwy' , 'cty' ], by = \"class\" , ylim = 'own' , fade = True , figsize = ( 14 , 10 )) # Decoration plt . title ( 'Joy Plot of City and Highway Mileage by Class' , fontsize = 18 ) plt . legend ([ 'hwy' , 'cty' ]) plt . show () <Figure size 1120x720 with 0 Axes> Distriubted dot plot Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly. In [12]: import matplotlib.patches as mpatches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) cyl_colors = { 4 : 'tab:red' , 5 : 'tab:green' , 6 : 'tab:blue' , 8 : 'tab:orange' } df_raw [ 'cyl_color' ] = df_raw . cyl . map ( cyl_colors ) # Mean and Median city mileage by make df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , ascending = False , inplace = True ) df . reset_index ( inplace = True ) df_median = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . median ()) # Draw horizontal lines fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 0 , xmax = 40 , color = 'gray' , alpha = 0.5 , linewidth =. 5 , linestyles = 'dashdot' ) # Draw the Dots for i , make in enumerate ( df . manufacturer ): df_make = df_raw . loc [ df_raw . manufacturer == make , :] ax . scatter ( y = np . repeat ( i , df_make . shape [ 0 ]), x = 'cty' , data = df_make , s = 75 , edgecolors = 'gray' , c = 'w' , alpha = 0.5 ) ax . scatter ( y = i , x = 'cty' , data = df_median . loc [ df_median . index == make , :], s = 75 , c = 'firebrick' ) # Annotate ax . text ( 33 , 13 , \"$red \\; dots \\; are \\; the \\: median$\" , fontdict = { 'size' : 12 }, color = 'firebrick' ) # Decorations red_patch = plt . plot ([],[], marker = \"o\" , ms = 10 , ls = \"\" , mec = None , color = 'firebrick' , label = \"Median\" ) plt . legend ( handles = red_patch ) ax . set_title ( 'Distribution of City Mileage by Make' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon (City)' , alpha = 0.7 ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }, alpha = 0.7 ) ax . set_xlim ( 1 , 40 ) plt . xticks ( alpha = 0.7 ) plt . gca () . spines [ \"top\" ] . set_visible ( False ) plt . gca () . spines [ \"bottom\" ] . set_visible ( False ) plt . gca () . spines [ \"right\" ] . set_visible ( False ) plt . gca () . spines [ \"left\" ] . set_visible ( False ) plt . grid ( axis = 'both' , alpha =. 4 , linewidth =. 1 ) plt . show () Box plot Box plots are a great way to visualize the distribution, keeping the median, 25th, and 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback. We can use sns.boxplot to draw the box plot. For example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary. In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , notch = False ) # Add N Obs inside boxplot (optional) def add_n_obs ( df , group_col , y ): medians_dict = { grp [ 0 ]: grp [ 1 ][ y ] . median () for grp in df . groupby ( group_col )} xticklabels = [ x . get_text () for x in plt . gca () . get_xticklabels ()] n_obs = df . groupby ( group_col )[ y ] . size () . values for ( x , xticklabel ), n_ob in zip ( enumerate ( xticklabels ), n_obs ): plt . text ( x , medians_dict [ xticklabel ] * 1.01 , \"#obs : \" + str ( n_ob ), horizontalalignment = 'center' , fontdict = { 'size' : 14 }, color = 'white' ) add_n_obs ( df , group_col = 'class' , y = 'hwy' ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . ylim ( 10 , 40 ) plt . show () Population pyramid Population pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel. In [14]: # Read data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) group_col = 'Gender' order_of_bars = df . Stage . unique ()[:: - 1 ] colors = [ plt . cm . Spectral ( i / float ( len ( df [ group_col ] . unique ()) - 1 )) for i in range ( len ( df [ group_col ] . unique ()))] for c , group in zip ( colors , df [ group_col ] . unique ()): sns . barplot ( x = 'Users' , y = 'Stage' , data = df . loc [ df [ group_col ] == group , :], order = order_of_bars , color = c , label = group ) # Decorations plt . xlabel ( \"$Users$\" ) plt . ylabel ( \"Stage of Purchase\" ) plt . yticks ( fontsize = 10 ) plt . title ( \"Population Pyramid of the Marketing Funnel\" , fontsize = 22 ) plt . legend () plt . show () Categorical plots Categorical plots can be used to visualize the counts distribution of 2 or more categorical variables. It can be done using sns.catplot(). In [15]: # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot g = sns . catplot ( \"alive\" , col = \"deck\" , col_wrap = 4 , data = titanic [ titanic . deck . notnull ()], kind = \"count\" , height = 3.5 , aspect =. 8 , palette = 'tab20' ) fig . suptitle ( 'sf' ) plt . show () In [16]: # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot sns . catplot ( x = \"age\" , y = \"embark_town\" , hue = \"sex\" , col = \"class\" , data = titanic [ titanic . embark_town . notnull ()], orient = \"h\" , height = 5 , aspect = 1 , palette = \"tab10\" , kind = \"violin\" , dodge = True , cut = 0 , bw =. 2 ) plt . show ()","tags":"Visualization","url":"https://ericchen23.github.io/2018/11/50-visualization-part2.html","loc":"https://ericchen23.github.io/2018/11/50-visualization-part2.html"},{"title":"Modeling on Amazon Phone and Accessories Reviews using NLP","text":"Topic Modeling on Amazon Phone and Accessories Reviews using NLP Introduction Online shopping now makes our life much easier than it used to be. Without the need of going out and visting a shopping mall or a grocery store, we can buy anything we want through e-shopping. But online shopping comes with its own caveats. One of the biggest challenges, and I guess almost every would face, is verifying the authenticity of a product. Is it as good as advertised on the e-commerce site? Will the product has a good quality? Are the reviews given by other customers really true or are they false advertising, or are they fake or bots' reviews? These are important questions customers need to ask before splurging their money. Importance of online reviews I believe there are a bunch of people like me, who believe in customers' reviews much much more than the fancy product description of an item. In making our decision in buying a product, if most customers' reviews talking about how good it is, then it is probably a really good product. If every customer talks bad about a product, I am afraid I will not buy it no matter how good it shows me in the product description. And in some funny cases, which is quite often from my perspective, some users like to leave a high rating or score no matter how bad they think the product is. Oh god, this is tricky! Identifying the problem Here, an interesting problem comes up. What if the number of reviews is in the hundreds or thousands? It's just not feasible to go through all those reviews, right? And this is where natural language processing (NLP) comes up trumps. But how? Online reviews can be really overwhelming in terms of numbers and information, an intelligent system, capable of finding key insights (topics) from these reviews, will be of great help for the consumers to get an real impression on products. To solve this task, we will use the concept of Topic Modeling (LDA) on Amazon Phone and Accessories Review data. The dataset can be downloaded here . The details of the dataset will be given in the following section during exploratory analysis. Topic modeling Topic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including: Document clustering Organizing large blocks of textual data Text feature selection information retrieval from unstructured text In the case of online product reviews, we intend to extract a certain number of groups of important words from the reviews. These groups of words are basically the topics which would help in ascertaining what the consumers are actually talking about in the reviews. Analyzing the review dataset First, let's import the necessary libraries. In [1]: # import libraries import nltk from nltk import FreqDist import numpy as np import pandas as pd import re import spacy import gensim from gensim import corpora import warnings warnings . filterwarnings ( \"ignore\" ) #libraries for visualization import matplotlib.pyplot as plt # plt.style.use('classic') import seaborn as sns import pyLDAvis import pyLDAvis.gensim % matplotlib inline C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\") Loading data The online review data is stored in a json file. Let's read in the file, and take a look at what's in the dataset. In [2]: # load the dataset df = pd . read_json ( 'data \\\\ Cell_Phones_and_Accessories_5.json' , lines = True ) print ( df . shape ) df . head () (194439, 9) Out[2]: asin helpful overall reviewText reviewTime reviewerID reviewerName summary unixReviewTime 0 120401325X [0, 0] 4 They look good and stick good! I just don't li... 05 21, 2014 A30TL5EWN6DFXT christina Looks Good 1400630400 1 120401325X [0, 0] 5 These stickers work like the review says they ... 01 14, 2014 ASY55RVNIL0UD emily l. Really great product. 1389657600 2 120401325X [0, 0] 5 These are awesome and make my phone look so st... 06 26, 2014 A2TMXE2AFO7ONB Erica LOVE LOVE LOVE 1403740800 3 120401325X [4, 4] 4 Item arrived in great time and was in perfect ... 10 21, 2013 AWJ0WZQYMYFQ4 JM Cute! 1382313600 4 120401325X [2, 3] 5 awesome! stays on, and looks great. can be use... 02 3, 2013 ATX7CZYFXI1KW patrice m rogoza leopard home button sticker for iphone 4s 1359849600 In the Phone and Accessories review dataset, we have nearly 200,000 total reviews and 9 columns. The columns contain the following information: asin - product ID helpful - helpfulness rating of the review, e.g. [19, 19] means 19 out of 19 people think this review is helpful overall - rating of the product (1 - 5 stars) reviewText - the text content of the review reviewTime - tiem of the review (raw) reviewerID - reviewer ID reviewerName - reviewer name summary - summary of the review unixReviewTime - time of the review (unix time) For the scope of our analysis and making it simple, we will be using only the reviewText column. Data cleansing Data cleansing is a important step before NLP analysis. The original text in the reviews data is very complicated. It contains many punctuations, stopwords and useful information etc. In this cleaning and preprocessing step, we should remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, let's define a function that would plot a bar graph of n most frequent words in the data. In [3]: # function to plot most frequent words def freq_words ( x , words = 30 ): all_words = ' ' . join ([ text for text in x ]) all_words = all_words . split () #frequency distribution fdist = FreqDist ( all_words ) words_df = pd . DataFrame ({ 'word' : list ( fdist . keys ()), 'count' : list ( fdist . values ())}) # select the top 30 (default) most frequent words d = words_df . nlargest ( columns = 'count' , n = words ) plt . figure ( figsize = ( 20 , 5 )) ax = sns . barplot ( data = d , x = \"word\" , y = \"count\" ) ax . set ( ylabel = 'Count' ) plt . show () Let's find out the most frequent words in the reviewText column without any preprocessing. In [4]: freq_words ( df [ 'reviewText' ]) As can be easily find, the most common words without any preprocessing are 'the', 'and', 'to', 'a' etc. These words contain little information and are not so important for us to find the key topics. We have to remove these types of words. Before that we also need to remove the punctuations and numbers from the data. In [5]: # remove unwanted characters, numbers and symbols df [ 'reviewText' ] = df [ 'reviewText' ] . str . replace ( \"[&#94;a-zA-Z#]\" , \" \" ) In [6]: # import stopwords library from nltk.corpus import stopwords stop_words = stopwords . words ( 'english' ) # function toremove stopwords and very short words (< 2 letters) # from the review text def remove_stopwords ( rev ): rev_new = \" \" . join ([ i for i in rev if i not in stop_words ]) return rev_new # remove short words df [ 'reviewText' ] = df [ 'reviewText' ] . apply ( lambda x : ' ' . join ([ w for w in x . split () if len ( w ) > 2 ])) # remove stopwords reviews = [ remove_stopwords ( r . split ()) for r in df [ 'reviewText' ]] # lowercase the reviews reviews = [ r . lower () for r in reviews ] After removing the numbers, special characters and the stopwords, let's now look again at the 30 most frequent words. In [7]: freq_words ( reviews ) We observe some improvement in the result. We can find some more useful words in the chart, like \"battery', 'price', 'good', 'product', 'oil' and so on. However, some neutral terms such 'the', 'this', 'they' etc. still stay at the top positions. To further remove noise from the text we can use lemmatization from the spaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word. In [8]: nlp = spacy . load ( 'en' , disable = [ 'parser' , 'ner' ]) # function to filter noun and adjective def lemmatization ( texts , tags = [ 'NOUN' , 'ADJ' ]): output = [] for sent in texts : doc = nlp ( \" \" . join ( sent )) output . append ([ token . lemma_ for token in doc if token . pos_ in tags ]) return output Tokenize and lemmatize the reviews In [9]: # tokenization tokenized_reviews = pd . Series ( reviews ) . apply ( lambda x : x . split ()) # an example after tokenization print ( tokenized_reviews [ 2 ]) ['these', 'awesome', 'make', 'phone', 'look', 'stylish', 'used', 'one', 'far', 'almost', 'year', 'can', 'you', 'believe', 'that', 'one', 'year', 'great', 'quality'] In [10]: # lemmatization reviews_2 = lemmatization ( tokenized_reviews ) #the above result followed by lemmatization print ( reviews_2 [ 2 ]) ['awesome', 'phone', 'stylish', 'year', 'year', 'great', 'quality'] As we can see, we have not only lemmatized the words in the reivews but also filtered out nouns and adjectives. Let's de-tokenize the lemmatized reviews and plot the most common words. In [11]: reviews_3 = [] for i in range ( len ( reviews_2 )): reviews_3 . append ( ' ' . join ( reviews_2 [ i ])) df [ 'reviews' ] = reviews_3 freq_words ( df [ 'reviews' ]) From the above chart of the top 30 words, we can see they all contain relevant information we are interested in. Now, let's move on and start building our topic model. Let's then visualize the frequent words using WordCloud. A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes. In [12]: all_words = ' ' . join ([ text for text in df [ 'reviews' ]]) from wordcloud import WordCloud wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( all_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () Create the model Building the term dictionary of our corpus Then we will convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above. In [13]: dictionary = corpora . Dictionary ( reviews_2 ) doc_term_matrix = [ dictionary . doc2bow ( rev ) for rev in reviews_2 ] Creating a Latent Dirichlet Allocation (LDA) model We specify the model to find the num_topics = 5 topics. In [14]: # Creating the LDA model using gensim LDA = gensim . models . ldamodel . LdaModel lda_model = LDA ( corpus = doc_term_matrix , id2word = dictionary , num_topics = 5 , random_state = 100 , chunksize = 1000 , passes = 50 ) Let's print out our 5 topics that the LDA model has learned. In [15]: lda_model . print_topics () Out[15]: [(0, '0.056*\"phone\" + 0.014*\"good\" + 0.011*\"app\" + 0.010*\"day\" + 0.010*\"time\" + 0.009*\"screen\" + 0.008*\"camera\" + 0.008*\"battery\" + 0.007*\"new\" + 0.007*\"great\"'), (1, '0.095*\"case\" + 0.046*\"phone\" + 0.027*\"screen\" + 0.019*\"good\" + 0.018*\"protector\" + 0.016*\"great\" + 0.012*\"protection\" + 0.011*\"color\" + 0.010*\"nice\" + 0.010*\"iphone\"'), (2, '0.022*\"phone\" + 0.013*\"use\" + 0.010*\"place\" + 0.009*\"car\" + 0.009*\"mount\" + 0.009*\"pad\" + 0.009*\"note\" + 0.008*\"tip\" + 0.008*\"small\" + 0.008*\"holder\"'), (3, '0.037*\"charger\" + 0.036*\"battery\" + 0.036*\"charge\" + 0.030*\"device\" + 0.027*\"cable\" + 0.025*\"usb\" + 0.022*\"power\" + 0.021*\"port\" + 0.013*\"phone\" + 0.012*\"time\"'), (4, '0.039*\"speaker\" + 0.026*\"bluetooth\" + 0.024*\"sound\" + 0.023*\"good\" + 0.016*\"quality\" + 0.015*\"headphone\" + 0.015*\"music\" + 0.014*\"button\" + 0.013*\"volume\" + 0.012*\"ear\"')] From the first topic, topic index number 0, we can see terms like \"phone\", \"good\", \"app\", \"screen\", \"camera\" and so on, indciating that the topic is very much related to hardware quality of cellphones. From the second topic, which is Topic 1 , it might be raleted to accessories description of cellphones. Topics visulaization To visualize our topics in a 2-dimensional space we will use the pyLDAvis library. This visualization is interactive in nature and displays topics along with the most relevant words. In [16]: # Visualize the topics pyLDAvis . enable_notebook () vis = pyLDAvis . gensim . prepare ( lda_model , doc_term_matrix , dictionary ) vis Out[16]: Summary From the above visualization, we can clearly see the most relevant of words in each topic learned by our model. By assigning a lager number to the _num topics in our model training, we can find much more specific topics on the online reviews. Topic modeling help us finding the most important topics in a large text review dataset. We can easily get an overview of all the reviews and find useful information for us to make purchasing desicions.","tags":"Blog","url":"https://ericchen23.github.io/2018/11/NLP-topic-modeling.html","loc":"https://ericchen23.github.io/2018/11/NLP-topic-modeling.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part I","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part I 1. Introduction This post summarizes the top 50 most valuable Matplotlib & Seaborn data visualizations in data science. It can be taken as a data visualization handbook for you to look up for useful visulaization. The 50 visualizations are categorized into 7 different application scenarios, and this post would mainly focuses on the first two categories, shown as follows: Correlation Scatter plot Bubble plot with Encircling Scatter plot with line of the best fit Jittering with stripplot Counts plot Marginal histogram Marginal boxplot Correlogram Pairwise plot Deviation Diverging bars Diverging texts Diverging dot plot Diverging Lollipop chart with markers Area chart Ranking Ordered bar chart Lollipop chart Dot plot Slope chart Dumbbell plot Distribution Histogram for continuous variables Histogram for categorical variables Density plot Density curves with histogram Joy plot Distributed dot plot Box plot Dot+box plot Violin plot Population pyramid Categorical plots Composition Waffle chart Pie chart Treemap Bar chart Change Time series plot Time series with peaks and troughs annotated Autocorrelation plot Cross correlation plot Time series decomposition plot Multiple time series Plotting with different scales using secondary Y axis Time series with error bands Stacked area chart Unstacked area chart Calender heat map Seasonal plot Groups Dendrogram Cluster plot Andrews curve Parallel coordinates Before heading into the 50 visualizations, let's first import neccesary libraries and plot settings. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 2. Correlation The plot of correlation can used to visualize the correlation between two or more variables. Scatter plot Scatter plot is a classical plot to visualize the correlation between variables. It can be done using plt.scatterplot(). In [2]: #import dataset midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) #prepare data #create as many colors as there are unique midewest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] #draw plot for each category plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 20 , cmap = colors [ i ], label = str ( category )) #decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Bubble plot with Encircling This type of plot shows the significance within a circular range. Although we use the data same as we did for the scatter plot above, the data importing procedure is done once again to keep the visualization as a whole process. In [3]: from matplotlib import patches from scipy.spatial import ConvexHull #import data midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # As many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] #Step 2: draw scatterplot with unique color for each category fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 'dot_size' , cmap = colors [ i ], label = str ( category ), edgecolors = 'black' , linewidths = . 5 ) #Step 3: Encircling def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices , :], ** kw ) ax . add_patch ( poly ) #Select the data to be encircled midwest_encircle_data = midwest . loc [ midwest . state == 'IN' , :] #Draw polygon surrounding vertices encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = 'k' , fc = 'gold' , alpha = 0.1 ) encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = 'firebrick' , fc = 'none' , linewidth = 1.5 ) #decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Scatter plot with line of the best fit We use linear regression to find the best fit line. It can be used to visualize how two variables interact with each other. If you only want to fit one single line for the entire dataset, delete the parameter hue = 'cyl' in sns.lmplot(). In [4]: import warnings warnings . simplefilter ( 'ignore' ) #Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = 'displ' , y = 'hwy' , hue = 'cyl' , data = df_select , size = 7 , aspect = 1.6 , robust = True , palette = 'tab10' , scatter_kws = dict ( s = 60 , linewidths = . 7 , edgecolors = 'black' )) #Decorations gridobj . set ( xlim = [ 0.5 , 7.5 ], ylim = [ 0 , 50 ]) plt . title ( \"Scatterplot with line of best fit grouped by number of cylinders\" , fontsize = 20 ) plt . show () Or we can plot the best fit line for each column. This can be done by setting col = groupingcolumn in sns.lmplot(). In [5]: # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Each line in its own column sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , data = df_select , size = 7 , robust = True , palette = 'Set1' , col = \"cyl\" , scatter_kws = dict ( s = 60 , linewidths = . 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . show () Jittering with stripplot In order to avoid hidden data points in the plot, usually due to the same or similar value, we use jittering plot to see all the overlapping data points. This can be done using stripplot() from seaborn. In [6]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df . cty , df . hwy , jitter = 0.25 , size = 8 , ax = ax , linewidth = 0.5 ) # Decorations plt . title ( \"Use jittered plots to avoid overlapping of points\" , fontsize = 20 ) plt . show () Counts plot This is another approach to visualize overlapping points. The bigger the ploted point is, the more data points are located nearby. In [7]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_counts = df . groupby ([ 'hwy' , 'cty' ]) . size () . reset_index ( name = 'counts' ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df_counts . cty , df_counts . hwy , size = df_counts . counts * 2 , ax = ax ) # Decorations plt . title ( \"Counts plot - Size of circle is bigger as more points overlap\" , fontsize = 20 ) plt . show () Marginal histogram A plot of marginal histogram has two histograms along X and Y axes. This plot can not only visualize the relationship between X and Y, but also their separate distribution. This plot is often used in exploratory data analysis (EDA). In [8]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) #Create figure and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) #Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) #Scatterplot on the main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 4 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha = 0.9 , data = df , cmap = \"tab10\" , edgecolors = 'gray' , linewidths = 0.5 ) #histogram in the bottom ax_bottom . hist ( df . displ , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'b' ) ax_bottom . invert_yaxis () #histogram on the right ax_right . hist ( df . hwy , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'b' ) #Decorations ax_main . set ( title = \"Scatterplot with Histograms \\n displ vs hwy\" , xlabel = 'displ' , ylabel = 'hwy' ) ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) xlabels = ax_main . get_xticks () . tolist () ax_main . set_xticklabels ( xlabels ) plt . show () Mariginal boxplot Marginal boxplot allows to see the median and percentiles of the two variables, X and Y. In [9]: # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 5 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"Set1\" , edgecolors = 'black' , linewidths =. 5 ) # Add a boxplot in each part sns . boxplot ( df . hwy , ax = ax_right , orient = 'v' ) sns . boxplot ( df . displ , ax = ax_bottom , orient = 'h' ) # Decorations ------------------ # Remove x axis name for the boxplot ax_bottom . set ( xlabel = '' ) ax_right . set ( ylabel = '' ) # Main Title, Xlabel and YLabel ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) # Set font size of different components ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) plt . show () Correlogram Correlogram is used to show the correlation matrix between all possible pairs of numeric variables in a given dataframe. We can use the sns.heatmap() to draw the plot. In [10]: # Import Dataset df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) # Plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) sns . heatmap ( df . corr (), xticklabels = df . corr () . columns , yticklabels = df . corr () . columns , cmap = 'RdYlGn' , center = 0 , annot = True ) # Decorations plt . title ( \"Correlogram of mtcars\" , fontsize = 20 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Pairwise plot Pairwise plot is another way to visualize all possible relationships between each pair of numeric variables. We can use sns.pairplot() to generate the plot. It's easy and nice. First, let's use the scatter plot in the pairplot. In [11]: # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"scatter\" , hue = \"species\" , plot_kws = dict ( s = 80 , edgecolor = \"white\" , linewidth = 2.5 )) plt . show () <Figure size 800x640 with 0 Axes> Then, let's see the pairplot with regression lines. In [12]: # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"reg\" , hue = \"species\" ) plt . show () <Figure size 800x640 with 0 Axes> 2. Deviation Diverging bars The diverging bars plot is a useful tool if you want to visualize how your observations behave based on a single variable. In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] # Calculate the z score of mpg df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 5 ) # Decorations plt . gca () . set ( ylabel = '$Model$' , xlabel = '$Mileage$' ) plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Diverging texts Diverging texts is similar to diverging bars and it is preferred if you want to show the exact value of eahc observationwithin the chart in a nice and presentable way. In [14]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 2 ), horizontalalignment = 'right' if x < 0 else 'left' , verticalalignment = 'center' , fontdict = { 'color' : 'red' if x < 0 else 'green' , 'size' : 14 }) # Decorations plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Text Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging dot plot The dot plot changes the lines into dots. The absence of bars reduces the amount of contrast and disparity between observations. In my opinion, it is not as good as the above two plots, but it is still an option. In [15]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'darkgreen' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . scatter ( df . mpg_z , df . index , s = 450 , alpha =. 6 , color = df . colors ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 1 ), horizontalalignment = 'center' , verticalalignment = 'center' , fontdict = { 'color' : 'white' }) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . yticks ( df . index , df . cars , fontsize = 10 ) plt . title ( 'Diverging Dotplot of Car Mileage' , fontdict = { 'size' : 20 }) plt . xlabel ( '$Mileage$' ) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging Lollipop chart with markers Lollipop with markers provides a flexible way fo visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately. In [16]: # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = 'black' # Color Fiat differently than other cars df . loc [ df . cars == 'Fiat X1-9' , 'colors' ] = 'darkorange' df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 1 ) plt . scatter ( df . mpg_z , df . index , color = df . colors , s = [ 600 if x == 'Fiat X1-9' else 300 for x in df . cars ], alpha = 0.6 ) plt . yticks ( df . index , df . cars ) plt . xticks ( fontsize = 12 ) # Annotate plt . annotate ( 'Mercedes Models' , xy = ( 0.0 , 11.0 ), xytext = ( 1.0 , 11 ), xycoords = 'data' , fontsize = 15 , ha = 'center' , va = 'center' , bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( arrowstyle = '-[, widthB=2.0, lengthB=1.5' , lw = 2.0 , color = 'steelblue' ), color = 'white' ) # Add Patches p1 = patches . Rectangle (( - 2.0 , - 1 ), width =. 3 , height = 3 , alpha =. 2 , facecolor = 'red' ) p2 = patches . Rectangle (( 1.5 , 27 ), width =. 8 , height = 5 , alpha =. 2 , facecolor = 'green' ) plt . gca () . add_patch ( p1 ) plt . gca () . add_patch ( p2 ) # Decorate plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Area chart By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs, but also the duration of hte highs and lows. The longer the duration of the highs, the larger is the area under the line. In [17]: # Import data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" , parse_dates = [ 'date' ]) . head ( 100 ) x = np . arange ( df . shape [ 0 ]) y_returns = ( df . psavert . diff () . fillna ( 0 ) / df . psavert . shift ( 1 )) . fillna ( 0 ) * 100 # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] >= 0 , facecolor = 'green' , interpolate = True , alpha = 0.7 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] <= 0 , facecolor = 'red' , interpolate = True , alpha = 0.7 ) plt . annotate ( 'Peak \\n 1975' , xy = ( 94.0 , 21.0 ), xytext = ( 88.0 , 28 ), bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( facecolor = 'steelblue' , shrink = 0.05 ), fontsize = 15 , color = 'white' ) # Decorations xtickvals = [ str ( m )[: 3 ] . upper () + \"-\" + str ( y ) for y , m in zip ( df . date . dt . year , df . date . dt . month_name ())] plt . gca () . set_xticks ( x [:: 6 ]) plt . gca () . set_xticklabels ( xtickvals [:: 6 ], rotation = 45 , fontdict = { 'horizontalalignment' : 'center' , 'verticalalignment' : 'top' , 'fontsize' : 12 }) plt . ylim ( - 35 , 35 ) plt . xlim ( 1 , 100 ) plt . title ( \"Month Economics Return %\" , fontsize = 22 ) plt . ylabel ( 'Monthly returns %' ) plt . grid ( alpha = 0.5 ) plt . show () The rest of the top visualizations will be presented in the future posts. Peace~","tags":"Visualization","url":"https://ericchen23.github.io/2018/10/50-visualization-part1.html","loc":"https://ericchen23.github.io/2018/10/50-visualization-part1.html"},{"title":"New Airbnb User Booking Prediction","text":"Airbnb New User Booking Predictions Introduction Predict new users' first bookings for their stay in a specific country. Details can be found in a Kaggle competion here . Data Overview There are 6 files provided. Two of these files provide background information (countries.csv and age_gender_bkts.csv), while sample_submission_NDF.csv provides an example of how the submission file containing our final predictions should be formatted. The three remaining files are the key ones: 1. train_users_2.csv – This dataset contains data on Airbnb users, including the destination countries. 2. test_users.csv – This dataset also contains data on Airbnb users, in the same format as train_users_2.csv, except without the destination country. These are the users for which we will have to make our final predictions. 3. sessions.csv – This data is supplementary data that can be used to train the model and make the final predictions. It contains information about the actions (e.g. clicked on a listing, updated a wish list, ran a search etc.) taken by the users in both the testing and training datasets above. A glimpse on the data In [1]: import pandas as pd #import data tr_datapath = \"data/train_users_2.csv\" te_datapath = \"data/test_users.csv\" df_train = pd . read_csv ( tr_datapath , header = 0 , index_col = None ) df_test = pd . read_csv ( te_datapath , header = 0 , index_col = None ) In [2]: # size of training data print ( df_train . shape ) df_train . head () (213451, 16) Out[2]: id date_account_created timestamp_first_active date_first_booking gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser country_destination 0 gxn3p5htnn 2010-06-28 20090319043255 NaN -unknown- NaN facebook 0 en direct direct untracked Web Mac Desktop Chrome NDF 1 820tgsjxq7 2011-05-25 20090523174809 NaN MALE 38.0 facebook 0 en seo google untracked Web Mac Desktop Chrome NDF 2 4ft3gnwmtx 2010-09-28 20090609231247 2010-08-02 FEMALE 56.0 basic 3 en direct direct untracked Web Windows Desktop IE US 3 bjjt8pjhuk 2011-12-05 20091031060129 2012-09-08 FEMALE 42.0 facebook 0 en direct direct untracked Web Mac Desktop Firefox other 4 87mebub9p4 2010-09-14 20091208061105 2010-02-18 -unknown- 41.0 basic 0 en direct direct untracked Web Mac Desktop Chrome US In [3]: # size of test data, short of the country_destination column which need to be predicted by our model print ( df_test . shape ) df_test . head () (62096, 15) Out[3]: id date_account_created timestamp_first_active date_first_booking gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser 0 5uwns89zht 2014-07-01 20140701000006 NaN FEMALE 35.0 facebook 0 en direct direct untracked Moweb iPhone Mobile Safari 1 jtl0dijy2j 2014-07-01 20140701000051 NaN -unknown- NaN basic 0 en direct direct untracked Moweb iPhone Mobile Safari 2 xx0ulgorjt 2014-07-01 20140701000148 NaN -unknown- NaN basic 0 en direct direct linked Web Windows Desktop Chrome 3 6c6puo6ix0 2014-07-01 20140701000215 NaN -unknown- NaN basic 0 en direct direct linked Web Windows Desktop IE 4 czqhjk3yfe 2014-07-01 20140701000305 NaN -unknown- NaN basic 0 en direct direct untracked Web Mac Desktop Safari Data cleansing From the above snapshot of the data in training and test files, a few key pieces of information about the integrity of this dataset can be identified. Firstly, is that at least two columns have missing values – the age column and date_first_booking column. Secondly, most of the columns provided contain categorical data. In fact 11 of the 16 columns provided appear to be categorical. Thirdly, the timestamp_first_active column looks to be a full timestamp, but in the format of a number. For example 20090609231247 looks like it should be 2009-06-09 23:12:47. Fourthly, erroneous values. For some columns, there are values that can be identified as obviously incorrect. This may be a gender column where someone has entered a number, or an age column where someone has entered a value well over 100. These values either need to be corrected (if the correct value can be determined) or assumed to be missing. Lastly, some columns need to be standardized. For example, when collecting data on country of birth, if users are not provided with a standardized list of countries, the data will inevitably contain multiple spellings of the same country (e.g. USA, United States, U.S. and so on). One of the main cleaning tasks often involves standardizing these values to ensure that there is only one version of each value. First, let's combine the training data and test data into one DataFrame so that we can do data cleansing at the same time. In [4]: # combine df_train and df_test into one DataFrame df_all = pd . concat (( df_train , df_test ), axis = 0 , ignore_index = True , sort = False ) Fix the format of the dates Because we will use the date information, cleaning the date timestamps is necessary. If we want to do anything with those dates (e.g. subtract one date from another, extract the month of the year from each date etc.), it will be far easier if Python recognizes the values as dates. In [5]: # fixing the date_account_created column df_all [ 'date_account_created' ] = pd . to_datetime ( df_all [ 'date_account_created' ], format = '%Y-%m- %d ' ) # fixing the timestamp_first_active column df_all [ 'timestamp_first_active' ] = pd . to_datetime ( df_all [ 'timestamp_first_active' ], format = '%Y%m %d %H%M%S' ) # use the timestamp_first_active column to fill the missing values in data_account_created column df_all [ 'date_account_created' ] . fillna ( df_all . timestamp_first_active , inplace = True ) Drop inconsistant columns There are three date fields, but we have only covered two above. The remaining date field, date_first_booking, we are going to drop (remove) from the training data altogether. The reason is that this field is only populated for users who have made a booking. For the data in training_users_2.csv, all the users that have a first booking country have a value in the date_first_booking column and for those that have not made a booking (country_destination = NDF) the value is missing. However, for the data in test_users.csv, the date_first_booking column is empty for all the records. This means that this column is not going to be useful for predicting which country a booking will be made. What is more, if we leave it in the training dataset when building the model, it will likely increase the chances that the model predicts NDF as those are the records without dates in the training dataset. In [6]: # Drop the date_first_booking column df_all . drop ( 'date_first_booking' , axis = 1 , inplace = True ) Correct the age column As noticed earlier, there are several age values that are clearly incorrect (unreasonably high or too low). In this step, we replace these incorrect values with ‘NaN'. To do this, we create a simple function that intakes a dataframe (table), a column name, a maximum acceptable value (90) and a minimum acceptable value (15). This function will then replace the values in the specified column that are outside the acceptable range with NaN. Besides, the significant portion of users who did not provide a age value should also be noticed. After we have converted the incorrect age values to NaN, we then change all the NaN values to -1. After testing with other methods of filling the NaN values, including average, median, and most frequent value, using the value -1 yields the best prediction model. In [7]: import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) #avoid comparison with NaN values df_all [ 'age' ] . fillna ( - 1 , inplace = True ) # function to clean incorrect value def remove_outliers ( df , column , min_val , max_val ): col_values = df [ column ] . values df [ column ] = np . where ( np . logical_or ( col_values < min_val , col_values > max_val ), np . NaN , col_values ) return df # Fixing age column df_all = remove_outliers ( df = df_all , column = 'age' , min_val = 15 , max_val = 90 ) df_all [ 'age' ] . fillna ( - 1 , inplace = True ) Fill the missing values in column first_affiliate_tracked And then view the DataFrame. In [8]: # Fill missing values in first_affliate_tracked df_all [ 'first_affiliate_tracked' ] . fillna ( - 1 , inplace = True ) df_all . tail () Out[8]: id date_account_created timestamp_first_active gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser country_destination 275542 cv0na2lf5a 2014-09-30 2014-09-30 23:52:32 -unknown- 31.0 basic 0 en direct direct untracked Web Windows Desktop IE NaN 275543 zp8xfonng8 2014-09-30 2014-09-30 23:53:06 -unknown- -1.0 basic 23 ko direct direct untracked Android Android Phone -unknown- NaN 275544 fa6260ziny 2014-09-30 2014-09-30 23:54:08 -unknown- -1.0 basic 0 de direct direct linked Web Windows Desktop Firefox NaN 275545 87k0fy4ugm 2014-09-30 2014-09-30 23:54:30 -unknown- -1.0 basic 0 en sem-brand google omg Web Mac Desktop Safari NaN 275546 9uqfg8txu3 2014-09-30 2014-09-30 23:59:01 FEMALE 49.0 basic 0 en other other tracked-other Web Windows Desktop Chrome NaN Data transformation and feature extraction We use data transformation is undertaken with the intention to enhance the ability of the classification algorithm to extract information from the data. We then use feature extraction to create new features which will help improve the prediction accuracy of our model. We will first focus on data transformation. Transforming categorical data - one hot encoding The first step we are going to undertake is some One Hot Encoding – replacing the categorical fields in the dataset with multiple columns representing one value from each column. In [9]: # one hot encoding function def convert_to_onehot ( df , column_to_convert ): categories = list ( df [ column_to_convert ] . drop_duplicates ()) for category in categories : cat_name = str ( category ) . replace ( \" \" , \"_\" ) . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"-\" , \"\" ) . lower () col_name = column_to_convert [: 5 ] + '_' + cat_name [: 10 ] df [ col_name ] = 0 df . loc [( df [ column_to_convert ] == category ), col_name ] = 1 return df #One hot encoding, and drop the original column from df_all columns_to_convert = [ 'gender' , 'signup_method' , 'signup_flow' , 'language' , 'affiliate_channel' , 'affiliate_provider' , 'first_affiliate_tracked' , 'signup_app' , 'first_device_type' , 'first_browser' ] for column in columns_to_convert : df_all = convert_to_onehot ( df_all , column ) df_all . drop ( column , axis = 1 , inplace = True ) df_all . head () Out[9]: id date_account_created timestamp_first_active age country_destination gende_unknown gende_male gende_female gende_other signu_facebook ... first_theworld_b first_slimbrowse first_epic first_stainless first_googlebot first_outlook_20 first_icedragon first_ibrowse first_nintendo_b first_uc_browser 0 gxn3p5htnn 2010-06-28 2009-03-19 04:32:55 -1.0 NDF 1 0 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 1 820tgsjxq7 2011-05-25 2009-05-23 17:48:09 38.0 NDF 0 1 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 2 4ft3gnwmtx 2010-09-28 2009-06-09 23:12:47 56.0 US 0 0 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 bjjt8pjhuk 2011-12-05 2009-10-31 06:01:29 42.0 other 0 0 1 0 1 ... 0 0 0 0 0 0 0 0 0 0 4 87mebub9p4 2010-09-14 2009-12-08 06:11:05 41.0 US 1 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows × 157 columns Creating new featrues Two fields that can be used to create some new features are the two date fields – date_account_created and timestamp_first_active. We want to extract all the information we can out of these two date fields that could potentially differentiate which country someone will make their first booking in. In [10]: # Add new datetime related fields df_all [ 'day_account_created' ] = df_all [ 'date_account_created' ] . dt . weekday df_all [ 'month_account_created' ] = df_all [ 'date_account_created' ] . dt . month df_all [ 'quarter_account_created' ] = df_all [ 'date_account_created' ] . dt . quarter df_all [ 'year_account_created' ] = df_all [ 'date_account_created' ] . dt . year df_all [ 'hour_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . hour df_all [ 'day_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . weekday df_all [ 'month_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . month df_all [ 'quarter_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . quarter df_all [ 'year_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . year df_all [ 'created_less_active' ] = ( df_all [ 'date_account_created' ] - df_all [ 'timestamp_first_active' ]) . dt . days # Drop unnecessary columns columns_to_drop = [ 'date_account_created' , 'timestamp_first_active' , 'date_first_booking' , 'country_destination' ] for column in columns_to_drop : if column in df_all . columns : df_all . drop ( column , axis = 1 , inplace = True ) print ( df_all . shape ) (275547, 164) Adding new data We will see what new data we can add from the sessios.csv file. The dataset contains records of user actions, with each row representing one action a user took. Every time a user reviewed search results, updated a wish list or updated their account information, a new row was created in this dataset. Although this data is likely to be very useful for our goal of predicting which country a user will make their first booking in, it also complicates the process of combining this data with the data from training.csv, as it will have to be aggregated so that there is one row per user. Aside from details of the actions taken, there are a couple of interesting fields in this data. The first is device_type – this field contains the type of device used for the specified action. The second interesting field is the secs_elapsed field. This shows us how long (in seconds) was spent on a particular action. Import sessions data In [11]: # read sessions.csv session_path = 'data/sessions.csv' sessions = pd . read_csv ( session_path , header = 0 , index_col = False ) sessions . head () Out[11]: user_id action action_type action_detail device_type secs_elapsed 0 d1mm9tcy42 lookup NaN NaN Windows Desktop 319.0 1 d1mm9tcy42 search_results click view_search_results Windows Desktop 67753.0 2 d1mm9tcy42 lookup NaN NaN Windows Desktop 301.0 3 d1mm9tcy42 search_results click view_search_results Windows Desktop 22141.0 4 d1mm9tcy42 lookup NaN NaN Windows Desktop 435.0 Extract the primary and secondary devides for each user How do we determine what a user's primary and secondary devices are? We look at how much time they spent on each device. One thing to note as we make these transformations is that by aggregating the data this way, we are also implicitly removing the missing values. In [12]: # Determine primary device sessions_device = sessions . loc [:, [ 'user_id' , 'device_type' , 'secs_elapsed' ]] aggregated_lvl1 = sessions_device . groupby ([ 'user_id' , 'device_type' ], as_index = False , sort = False ) . aggregate ( np . sum ) index = aggregated_lvl1 . groupby ([ 'user_id' ], sort = False )[ 'secs_elapsed' ] . transform ( max ) == aggregated_lvl1 [ 'secs_elapsed' ] df_primary = pd . DataFrame ( aggregated_lvl1 . loc [ index , [ 'user_id' , 'device_type' , 'secs_elapsed' ]]) df_primary . rename ( columns = { 'device_type' : 'primary_device' , 'secs_elapsed' : 'primary_secs' }, inplace = True ) df_primary = convert_to_onehot ( df_primary , column_to_convert = 'primary_device' ) df_primary . drop ( 'primary_device' , axis = 1 , inplace = True ) # Determine secondary device remaining = aggregated_lvl1 . drop ( aggregated_lvl1 . index [ index ]) index = remaining . groupby ( [ 'user_id' ], sort = False )[ 'secs_elapsed' ] . transform ( max ) == remaining [ 'secs_elapsed' ] df_secondary = pd . DataFrame ( remaining . loc [ index , [ 'user_id' , 'device_type' , 'secs_elapsed' ]]) df_secondary . rename ( columns = { 'device_type' : 'secondary_device' , 'secs_elapsed' : 'secondary secs' }, inplace = True ) df_secondary = convert_to_onehot ( df_secondary , 'secondary_device' ) df_secondary . drop ( 'secondary_device' , axis = 1 , inplace = True ) Determine action counts Determine action counts for the three columns action, action_type, action_detail , to generate 3 sepparate tables. Then we join the three tables together on the basis of the user_id. In [13]: # function to count occurrences of value in a column def convert_to_counts ( df , id_col , column_to_convert ): id_list = df [ id_col ] . drop_duplicates () df_counts = df . loc [:,[ id_col , column_to_convert ]] df_counts [ 'count' ] = 1 df_counts = df_counts . groupby ( by = [ id_col , column_to_convert ], as_index = False , sort = False ) . sum () new_df = df_counts . pivot ( index = id_col , columns = column_to_convert , values = 'count' ) new_df = new_df . fillna ( 0 ) #rename columns categories = list ( df [ column_to_convert ] . drop_duplicates ()) for category in categories : cat_name = str ( category ) . replace ( \" \" , \"_\" ) . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"-\" , \"\" ) . lower () col_name = column_to_convert + '_' + cat_name new_df . rename ( columns = { category : col_name }, inplace = True ) return new_df # Aggregate and combine actions taken columns session_actions = sessions . loc [:, [ 'user_id' , 'action' , 'action_type' , 'action_detail' ]] columns_to_convert = [ 'action' , 'action_type' , 'action_detail' ] session_actions = session_actions . fillna ( 'not provided' ) # flag indicating the first loop first = True for column in columns_to_convert : print ( \"Converting \" + column + \" column...\" ) current_data = convert_to_counts ( df = session_actions , id_col = 'user_id' , column_to_convert = column ) if first : first = False actions_data = current_data else : actions_data = pd . concat ([ actions_data , current_data ], axis = 1 , join = 'inner' ) Converting action column... Converting action_type column... Converting action_detail column... Combine data sets The last steps are to combine the various datasets into one large dataset. First we combine the two device dataframes (df_primary and df_secondary) to create a device dataframe. Then we combine the device dataframe with the actions dataframe to create a sessions dataframe with all the features we extracted from sessions.csv. Finally, we combine the sessions dataframe with the user data dataframe. The first two joins need outer join because not all users have a secondary deivce. The second merge could use an outer join or an inner join, as both the device and actions datasets should contain all users. In this case we use an outer join just to ensure that if a user is missing from one of the datasets (for whatever reason), we will still capture them. For the third step we use an inner join for a key reason – we want our final training dataset to only include users that also have sessions data. Using an inner join here is an easy way to join the datasets and filter for the users with sessions data in one step. In [14]: # Combine device datasets df_primary . set_index ( 'user_id' , inplace = True ) df_secondary . set_index ( 'user_id' , inplace = True ) device_data = pd . concat ([ df_primary , df_secondary ], axis = 1 , join = 'outer' , sort = False ) #Combine device and actions datasets combined_results = pd . concat ([ device_data , actions_data ], axis = 1 , join = 'outer' , sort = False ) df_sessions = combined_results . fillna ( 0 ) #Combine user and sessions datasets df_all . set_index ( 'id' , inplace = True ) df_all = pd . concat ([ df_all , df_sessions ], axis = 1 , join = 'inner' , sort = False ) df_all . head () Out[14]: age gende_unknown gende_male gende_female gende_other signu_facebook signu_basic signu_google signu_weibo signu_0 ... action_detail_view_resolutions action_detail_view_search_results action_detail_view_security_checks action_detail_view_user_real_names action_detail_wishlist action_detail_wishlist_content_update action_detail_wishlist_note action_detail_your_listings action_detail_your_reservations action_detail_your_trips d1mm9tcy42 62.0 0 1 0 0 0 1 0 0 1 ... 0.0 23.0 0.0 0.0 0.0 25.0 0.0 0.0 0.0 0.0 yo8nz8bqcq -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 4grx6yxeby -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 ncf87guaf0 -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 32.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 0.0 4rvqpxoh3h -1.0 1 0 0 0 0 1 0 0 0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows × 720 columns Create a Model So far, the df_all dataset is ready to be used to train and test a model to predict the first booking destination country for each new user. The traing algorithm we will use is the popular XGBoost. From my perspective, this method is superior to random forest. It builds a first tree, typically a shallower tree than if you use one single decision tree, and makes predictions using that tree. Then the algorithm finds the records that are misclassified by that tree, and assigns a higher weight of importance to those records than the records that were correctly classified. The algorithm then builds a new tree with these new weightings. This whole process is repeated as many times as specified by the user. Once the specified number of trees have been built, all the trees built during this process are used to classify the records, with a majority rules approach used to determine the final prediction. Cross validation To avoid overfitting in our model, I will use 10-fold cross validation. K-fold cross validation involves splitting the training data into k subsets (where k is greater than or equal to 2), training the model using k – 1 of those subsets, then running the model on the subset that was not used in the training process. Because all of the data used in the cross validation process is training data, the correct classification for each record is known and so the predicted category can be compared to the actual category. Once all folds have been completed, the average score across all folds is taken as an estimate of how the model will perform on other data. In [15]: # Import libraries import xgboost as xgb from sklearn import decomposition from sklearn.model_selection import GridSearchCV , cross_validate from sklearn.preprocessing import LabelEncoder Prepare training data We previously combined the training and test data to simplify the cleaning and transforming process. To feed these into the model, we also need to split the training data into the three main components – the user IDs (we don't want to use these for training as they are randomly generated), the features to use for training (X), and the categories we are trying to predict (y). In [16]: # Prepare training data for model training df_train . set_index ( 'id' , inplace = True ) df_train = pd . concat ([ df_train [ 'country_destination' ], df_all ], axis = 1 , join = 'inner' , sort = False ) index_train = df_train . index . values labels = df_train [ 'country_destination' ] le = LabelEncoder () y = le . fit_transform ( labels ) # training labels x = df_train . drop ( 'country_destination' , axis = 1 , inplace = False ) # training data Now that we have our training data ready, we can use GridSearchCV to run the algorithm with a range of parameters, then select the model that has the highest cross validated score based on the chosen measure of a performance (in this case accuracy, but there are a range of metrics we could use based on our needs). In [17]: # Grid Search - used to find the best combination of parameters XGB_model = xgb . XGBClassifier ( objective = 'multi:softprob' , subsample = 0.8 , colsample_bytree = 0.8 , seed = 0 ) param_grid = { 'max_depth' : [ 3 , 4 , 5 ], 'learning_rate' : [ 0.1 , 0.3 ], 'n_estimators' : [ 25 , 50 ]} model = GridSearchCV ( estimator = XGB_model , param_grid = param_grid , scoring = 'accuracy' , verbose = 10 , n_jobs = 1 , iid = True , refit = True , cv = 3 ) # Model training model . fit ( x , y ) print ( \"Best score: %0.3f \" % model . best_score_ ) print ( \"Best parameters set:\" ) best_parameters = model . best_estimator_ . get_params () for param_name in sorted ( param_grid . keys ()): print ( \" \\t %s : %r \" % ( param_name , best_parameters [ param_name ])) Fitting 3 folds for each of 12 candidates, totalling 36 fits [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 2.7min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.7030189752549673, total= 2.7min C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 5.1min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.6923795976427556, total= 2.4min [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 3 out of 3 | elapsed: 7.5min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.6961665108337737, total= 2.3min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 4 out of 4 | elapsed: 12.1min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7052943805615375, total= 4.5min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 16.7min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.6867709815078236, total= 4.6min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 6 out of 6 | elapsed: 21.4min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7000691085003455, total= 4.6min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 7 out of 7 | elapsed: 24.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.7052943805615375, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 8 out of 8 | elapsed: 27.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.6942491363543996, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 30.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.7007195414447742, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.7052943805615375, total= 5.8min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.6862426336110546, total= 5.6min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.7025082320419529, total= 5.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.7049286904229816, total= 3.6min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.6935175777281041, total= 3.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.703849749989837, total= 3.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.704847425947747, total= 7.3min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.6824629140418614, total= 7.3min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.7048660514655067, total= 7.2min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.7021250660273861, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.6848201585043691, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.7008008455628277, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.701353053512657, total= 4.5min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.6821784190205242, total= 4.6min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.7010447579169885, total= 4.5min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.7028970785421154, total= 3.0min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.6659215606584028, total= 2.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.7034838814585959, total= 2.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.7018000081264475, total= 5.8min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.6597845966267019, total= 5.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.7045001829342656, total= 5.8min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.7033034009182886, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.6759195285511075, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.7001097605593724, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.700743569948397, total= 7.5min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.6653932127616338, total= 7.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 36 out of 36 | elapsed: 161.6min finished [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.6992560673198097, total= 7.5min Best score: 0.701 Best parameters set: learning_rate: 0.1 max_depth: 5 n_estimators: 25 Make Predictions on test data Now we can use our trained model (with best training parameters) to make predictions on our test data. In [18]: # Prepare test data for prediction df_test . set_index ( 'id' , inplace = True ) df_test = pd . merge ( df_test . loc [:, [ 'date_first_booking' ]], df_all , how = 'left' , left_index = True , right_index = True , sort = False ) x_test = df_test . drop ( 'date_first_booking' , axis = 1 , inplace = False ) x_test = x_test . fillna ( - 1 ) id_test = df_test . index . values # Make predictions y_pred = model . predict_proba ( x_test )","tags":"Blog","url":"https://ericchen23.github.io/2018/10/XGBOOST-predict.html","loc":"https://ericchen23.github.io/2018/10/XGBOOST-predict.html"},{"title":"Indian diabetes database modeling using Naive Bayes","text":"Indian diabetes database modeling using Naive Bayes Coding from scratch. To better understand the process of Naive Bayes model training. And in the end, we make another Naive Bayes Classifier through normal workflow using sklearn. Pros and cons of Naive Bayes Classifiers Pros: Computationally fast Simple to implement Works well with small datasets Works well with high dimensions Perform well even if the Naive Assumption is not perfectly met. In many cases, the approximation is enough to build a good classifier Cons: Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance. If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as \"Zero Frequency\". To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier. In [1]: #import libraries and basic settings import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np import pandas as pd import random import math from IPython.display import display pi = math . pi Exploratory analysis Check out the data to get a brief understanding of the dataset, including data integrity, size of the dataset, any missing values. In [2]: full_catalog = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) print ( full_catalog . columns ) print ( 'Size of the catalogue: {} ' . format ( len ( full_catalog ))) print ( 'Is there any missing value?: {} ' . format ( full_catalog . isnull () . any () . any ())) full_catalog . head () Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'], dtype='object') Size of the catalogue: 768 Is there any missing value?: False Out[2]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 No missing values are found in our dataset, which is good. Because the dataset is pretty clean we don't need to do too much data cleansing. The Outcome column is our target column. Value 1 stands positive diagnosis of diabetes and value 0 represents negative. Let's see how many people are positive and/or negative in our database. In [3]: # diabetes positive positive = full_catalog [ full_catalog [ 'Outcome' ] == 1 ] print ( 'The number of patients with diabetes: ' , len ( positive )) # diabetes negative negative = full_catalog [ full_catalog [ 'Outcome' ] == 0 ] print ( 'The number of healthy patients: ' , len ( negative )) The number of patients with diabetes: 268 The number of healthy patients: 500 Deeper into the data : let's see which features (attributes) can better explain our data according to the Outcome . Scatter matrix of all data First let's look at the scatter matrix of attributes one against another. Blue represents negative patients (healthy), and red represent positive patients (with diabetes). In [4]: # According to the color map, blue = 0, red = 1 df = pd . DataFrame ( full_catalog , columns = full_catalog . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = full_catalog [ 'Outcome' ] . values , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'green' }, s = 10 , alpha = 0.2 , cmap = plt . get_cmap ( 'bwr' )) plt . show () Scatter matrix of diabetes patients (positive) In [5]: # Scatter of patients with diabetes df = pd . DataFrame ( positive , columns = positive . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = 'red' , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'red' }, s = 10 , alpha = 0.2 ) plt . show () Scatter matrix of heathy patients (negative outcomes) In [6]: # Scatter of patients with diabetes df = pd . DataFrame ( negative , columns = negative . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = 'blue' , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'blue' }, s = 10 , alpha = 0.2 ) plt . show () Training set and test set preparation Of course we can use train_test_split from sklearn. But here, we use a scratch function to realize it. In [7]: ''' Function create_training_test divide the whole dataset into training set and test set. Parameters: dataset: the original dataset to be split fraction_training: fraction of training set, number in [0, 1] msg: debug flag. If True, display message of current process Output: training_set test_set ''' def create_training_test ( dataset , fraction_training , msg ): # define size of training and test sets size_dataset = len ( dataset ) size_training = round ( size_dataset * fraction_training ) size_test = size_dataset - size_training # initializing both the training and test set using the whole dataset training_set = dataset . copy () test_set = dataset . copy () #index of the dataset dataframe total_idx_list = list ( dataset . index . values ) #index of the test set. Randomly selected from total_idx_list test_idx_list = random . sample ( list ( dataset . index . values ), size_test ) test_idx_list . sort () #index of the training set training_idx_list = list ( set ( total_idx_list ) - set ( test_idx_list )) #drop the corresponding rows from the training and test dataframe training_set . drop ( training_set . index [ test_idx_list ], inplace = True ) test_set . drop ( test_set . index [ training_idx_list ], inplace = True ) if msg == True : training_positive = training_set [ training_set [ 'Outcome' ] == 1 ] training_negative = training_set [ training_set [ 'Outcome' ] == 0 ] print ( \"Size of the dataset : {} \" . format ( size_dataset )) print ( \"Size of the training set : {} samples ( {} of the whole dataset)\" . format ( len ( training_set ), fraction_training )) print ( \" \\t Positive cases in the training set: {} \" . format ( len ( training_positive ))) print ( \" \\t Negative cases in the training set: {} \" . format ( len ( training_negative ))) print ( \"Size of the test set : {} \" . format ( len ( test_set ))) return training_set , test_set In [8]: ''' Function get_parameters create a dictionary that contain the mean and standard deviation of each column in dataset Input: dataset: input dataset frame msg: debug flag Output: dict_parameters: a dictionary that contain the mean and standard deviation of each attribute in dataset ''' def get_parameters ( dataset , msg ): features = dataset . columns . values nbins = 10 dict_parameters = {} # exclude the 'Outcome' column from the loop for i in range ( 0 , len ( features ) - 1 ): #we single out the column 'feature[i]' from dataset aux_df = pd . DataFrame ( dataset [ features [ i ]]) #Here we make partition into nbins. #aux_df has an extra column indicating to which bin each instance belongs to aux_df [ 'bin' ] = pd . cut ( aux_df [ features [ i ]], nbins ) # 'counts' is a searies whose index is the bin interval and the values are the #number of counts in each bin counts = pd . value_counts ( aux_df [ 'bin' ]) points_X = np . zeros ( nbins ) points_Y = np . zeros ( nbins ) for j in range ( 0 , nbins ): points_X [ j ] = counts . index [ j ] . mid # the mid point of each bin points_Y [ j ] = counts . iloc [ j ] # the number of counts total_Y = np . sum ( points_Y ) # we compute the mean and std and store them in dict_parameters # whose keys are column names and values are (mu, sigma) tuple mu = np . sum ( points_X * points_Y ) / total_Y sigma2 = np . sum (( points_X - mu ) ** 2 * points_Y ) / ( total_Y - 1 ) sigma = math . sqrt ( sigma2 ) dict_parameters [ features [ i ]] = ( mu , sigma ) if msg == True : print ( ' \\t\\t feature: {} , mean: {} , standard deviation: {} ' . format ( features [ i ], mu , sigma )) return dict_parameters In [9]: ''' Function likelihood calculates the probability density of each column Input: instance: series, can be considered as each row in our dataset, the index of the series is the columns of our dataset dictionary Output: dict_likelihood: dictionary contains probability density ''' def likelihood ( instance , dictionary ): instance = instance [ instance . index != 'Outcome' ] dict_likelihood = {} for feature in instance . index : mu = dictionary [ feature ][ 0 ] sigma = dictionary [ feature ][ 1 ] measurement = instance [ feature ] if feature in [ 'Pregnancies' , 'Insulin' , 'DiabetesPedigreeFunction' , 'Age' ]: # We use exponential distribution for # columns ['Pregnancies', 'Insulin', 'DiabetesPedigreeFunction', 'Age'] dict_likelihood [ feature ] = 1. / mu * math . exp ( - measurement / mu ) elif feature in [ 'Glucose' , 'BloodPressure' , 'SkinThickness' , 'BMI' ]: # We use Gaussian distribution for columns['Glucose', 'BloodPressure', 'SkinThickness', 'BMI'] dict_likelihood [ feature ] = 1. / ( math . sqrt ( 2 * pi ) * sigma ) * math . exp ( - ( measurement - mu ) ** 2 / ( 2. * sigma ** 2 )) return dict_likelihood In [10]: ''' Function bayes classify the input instances according to Bayes Theory Input: lkh_positive: for each feature, P(features|outcome == 1) lkh_negative: for each feature, P(features|outcome == 0) prob_positive: the probability of patients with diabetes Output: predictions, 1-positive, 0-negative ''' def bayes ( lkh_positive , lkh_negative , prob_positive ): logPositive = 0 logNegative = 0 for feature in lkh_positive : logPositive += math . log ( lkh_positive [ feature ]) logNegative += math . log ( lkh_negative [ feature ]) logPositive = logPositive + math . log ( prob_positive ) logNegative = logNegative + math . log ( 1. - prob_positive ) if logPositive > logNegative : return 1 else : return 0 In [11]: # funciton to run the test def pima_diabetes_NBClassifier ( training_fraction , msg ): dataset = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) training , test = create_training_test ( dataset , training_fraction , msg ) #split the training set into training_positive and training_negative according to Outcome training_positive = training [ training [ 'Outcome' ] == 1 ] training_negative = training [ training [ 'Outcome' ] == 0 ] prob_positive = len ( training_positive ) / ( len ( training )) if msg == True : print ( 'Getting the parameters for the training set...' ) print ( ' \\t Positive cases subsample' ) param_positive = get_parameters ( training_positive , msg ) if msg == True : print ( ' \\t Negative cases subsample' ) param_negative = get_parameters ( training_negative , msg ) if msg == True : print ( ' \\t Probability of finding a positive case: {} ' . format ( prob_positive )) print ( 'Analyzing the test set...' ) # Here we compute the accuracy of the classifier by looping over the instances of the test set error_count = 0 for idx in test . index . values : instance = test . loc [ idx ] likelihood_positive = likelihood ( instance , param_positive ) likelihood_negative = likelihood ( instance , param_negative ) prediction = bayes ( likelihood_positive , likelihood_negative , prob_positive ) answer = int ( instance [ 'Outcome' ]) if prediction != answer : error_count += 1 error_rate = float ( error_count ) / len ( test ) if msg == True : print ( 'Results for this implementation:' ) print ( ' \\t Error rate: : {} ' . format ( error_rate )) print ( ' \\t Successful classification rate : {} ' . format ( 1. - error_rate )) return error_rate Single implementation In [12]: training_fraction = 0.75 msg = True pima_diabetes_NBClassifier ( training_fraction , msg ) Size of the dataset : 768 Size of the training set : 576 samples (0.75 of the whole dataset) Positive cases in the training set: 203 Negative cases in the training set: 373 Size of the test set : 192 Getting the parameters for the training set... Positive cases subsample feature: Pregnancies, mean: 5.06026354679803, standard deviation: 3.659147466140387 feature: Glucose, mean: 139.54409359605913, standard deviation: 31.20971649275315 feature: BloodPressure, mean: 70.25017241379311, standard deviation: 20.63956588198645 feature: SkinThickness, mean: 23.708549261083746, standard deviation: 16.27891796774379 feature: Insulin, mean: 109.93596059113301, standard deviation: 116.28083680196309 feature: BMI, mean: 35.053634975369455, standard deviation: 7.323798557556501 feature: DiabetesPedigreeFunction, mean: 0.5482635467980296, standard deviation: 0.35793174891647633 feature: Age, mean: 36.80145320197044, standard deviation: 10.80830792627758 Negative cases subsample feature: Pregnancies, mean: 3.3208900804289545, standard deviation: 2.885346134494473 feature: Glucose, mean: 110.5148873994638, standard deviation: 25.689628327169185 feature: BloodPressure, mean: 68.60210455764074, standard deviation: 18.27917311567462 feature: SkinThickness, mean: 20.48474798927614, standard deviation: 14.190959528879588 feature: Insulin, mean: 86.43265415549598, standard deviation: 88.69429208023446 feature: BMI, mean: 30.5620226541555, standard deviation: 7.792696402992349 feature: DiabetesPedigreeFunction, mean: 0.43494249329758705, standard deviation: 0.3122588634553088 feature: Age, mean: 31.4157908847185, standard deviation: 11.052104394041816 Probability of finding a positive case: 0.3524305555555556 Analyzing the test set... Results for this implementation: Error rate: : 0.19791666666666666 Successful classification rate : 0.8020833333333334 Out[12]: 0.19791666666666666 Multiple implementations Let's run the NBClassifier multiple times to get an mean and std values of the error rate and successful rate. In [13]: training_fraction = 0.75 nrealizations = 500 msg = False error_rate = np . zeros ( nrealizations ) sucess_rate = np . zeros ( nrealizations ) for i in range ( 0 , nrealizations ): aux = pima_diabetes_NBClassifier ( training_fraction , msg ) error_rate [ i ] = aux success_rate = 1. - aux print ( 'Results after {} realizations and training the classifier wiht {} of the wholesamples...' . format ( nrealizations , training_fraction )) print ( 'error rate mean: {} , std: {} ' . format ( np . mean ( error_rate ), np . std ( error_rate ))) print ( 'success rate mean: {} , std: {} ' . format ( np . mean ( success_rate ), np . std ( success_rate ))) Results after 500 realizations and training the classifier wiht 0.75 of the wholesamples... error rate mean: 0.232875, std: 0.02677764991036783 success rate mean: 0.734375, std: 0.0 Naive Bayes using sklearn: a comparison Now let's look at the normal workflow using the library of sklearn. In [14]: from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB #importing dataset data = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) data . dropna ( axis = 0 , how = 'any' , inplace = True ) X = data [[ 'Pregnancies' , 'Glucose' , 'BloodPressure' , 'SkinThickness' , 'Insulin' , 'BMI' , 'DiabetesPedigreeFunction' , 'Age' ]] y = data [ 'Outcome' ] #split dataset into training and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) In [15]: #initailizing the classifier, we use the GaussianNB gnb = GaussianNB () # train the classifier using training set gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) # print results print ( \"Number of misclassified samples out of a total {} samples: {} , performance {:05.2f} %\" . format ( X_test . shape [ 0 ], ( y_test != y_pred ) . sum (), 100 * ( 1 - ( y_test != y_pred ) . sum () / X_test . shape [ 0 ]))) Number of misclassified samples out of a total 192 samples: 51, performance 73.44%","tags":"Blog","url":"https://ericchen23.github.io/2018/09/Naive-Bayes.html","loc":"https://ericchen23.github.io/2018/09/Naive-Bayes.html"},{"title":"Churn Rate Prediction Using Neural Networks","text":"Churn Rate Prediction for a bank The basic aim of this notebook is to predict customer churn for a certain bank i.e. which customer is going to leave this bank service. Neural network will be used as the modelling method for this notebook. The dataset used in this notebook is introduced by Pushkar Mandot on his blog post. The dataset can be down load here . The dataset contains 10000 rows with 14 columns. I am not explaining data in detail as dataset is self explanatory. Importing data In [1]: # Importing the libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd # Importing the dataset dataset = pd . read_csv ( 'data\\Churn_Modelling.csv' ) print ( dataset . shape ) dataset . head () (10000, 14) Out[1]: RowNumber CustomerId Surname CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 0 1 15634602 Hargrave 619 France Female 42 2 0.00 1 1 1 101348.88 1 1 2 15647311 Hill 608 Spain Female 41 1 83807.86 1 0 1 112542.58 0 2 3 15619304 Onio 502 France Female 42 8 159660.80 3 1 0 113931.57 1 3 4 15701354 Boni 699 France Female 39 1 0.00 2 0 0 93826.63 0 4 5 15737888 Mitchell 850 Spain Female 43 2 125510.82 1 1 1 79084.10 0 Creating training and test set Create matrix of features and matrix of target variable. In this case we are excluding column 1 & 2 as those are ‘row_number' and ‘customerid' which are not useful in our analysis. Column 14, ‘Exited' is our Target Variable In [2]: X = dataset . iloc [:, 3 : 13 ] . values y = dataset . iloc [:, 13 ] . values Let us take a glimpse on the predictors in X. As can be seen below, the dataset is pretty clean. Only two columns of string variables need to be transferred to categorical variables or one hot values in order to be fed into a classifier. In [3]: print ( X ) [[619 'France' 'Female' ... 1 1 101348.88] [608 'Spain' 'Female' ... 0 1 112542.58] [502 'France' 'Female' ... 1 0 113931.57] ... [709 'France' 'Female' ... 0 1 42085.58] [772 'Germany' 'Male' ... 1 0 92888.52] [792 'France' 'Female' ... 1 0 38190.78]] The target y only contains 0s and 1s as 0 stands for customers still with us, and 1 represents customers left us. In [4]: print ( y ) [1 0 1 ... 1 1 0] Encoding categorical variables : we need to use the LabelEncoder and OneHotEncoder from the sklearn to transform string variables in X. Use LabelEncoder first to encode different labels in a certain column to numbers between 0 to n_class-1. Then, use OneHotEncoder to tranform the numbers into one hot manner. In [5]: from sklearn.preprocessing import LabelEncoder , OneHotEncoder labelencoder_X_1 = LabelEncoder () X [:, 1 ] = labelencoder_X_1 . fit_transform ( X [:, 1 ]) labelencoder_X_2 = LabelEncoder () X [:, 2 ] = labelencoder_X_2 . fit_transform ( X [:, 2 ]) X Out[5]: array([[619, 0, 0, ..., 1, 1, 101348.88], [608, 2, 0, ..., 0, 1, 112542.58], [502, 0, 0, ..., 1, 0, 113931.57], ..., [709, 0, 0, ..., 0, 1, 42085.58], [772, 1, 1, ..., 1, 0, 92888.52], [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object) Now you can see that Country names are replaced by 0,1 and 2 while male and female are replaced by 0 and 1. Label encoding has introduced new problem in our data. LabelEncoder has replaced France with 0, Germany 1 and Spain 2 but Germany is not higher than France and France is not smaller than Spain so we need to create a dummy variable for Country. We don't need to do same for Gender Variable as it is binary. Here, we use the OneHotEncoder to do the job. In [6]: onehotencoder = OneHotEncoder ( categorical_features = [ 1 ]) X = onehotencoder . fit_transform ( X ) . toarray () X = X [:, 1 :] X Out[6]: array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, ..., 1.0000000e+00, 1.0000000e+00, 1.0134888e+05], [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, ..., 0.0000000e+00, 1.0000000e+00, 1.1254258e+05], [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 1.1393157e+05], ..., [0.0000000e+00, 0.0000000e+00, 7.0900000e+02, ..., 0.0000000e+00, 1.0000000e+00, 4.2085580e+04], [1.0000000e+00, 0.0000000e+00, 7.7200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 9.2888520e+04], [0.0000000e+00, 0.0000000e+00, 7.9200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 3.8190780e+04]]) Assigning training set and test set In [7]: # Splitting the dataset into the Training set and Test set from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) Preprocessing We are going to fitting and transforming StandardScaler method on both the training. In order to make our model work on test data, we have to standardize our scaling so we will use the same fitted method to transform/scale test data. Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False. In [8]: # Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train = sc . fit_transform ( X_train ) X_test = sc . transform ( X_test ) Now, the preprocessing on our data is done. We will start building our neural network model. The library we use to build our NN model is Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. We need Sequential module for initializing NN and Dense module to add Hidden Layers. In [9]: # Importing the Keras libraries and packages import keras from keras.models import Sequential from keras.layers import Dense D:\\ANOCONDA\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. In [10]: #Initializing Neural Network classifier = Sequential () Adding layers to the neural network. Which activation function should be used is critical task. Here we are using rectifier(relu) function in our hidden layer and Sigmoid function in our output layer as we want binary result from output layer but if the number of categories in output layer is more than 2 then use SoftMax function. In [11]: # Adding the input layer and the first hidden layer classifier . add ( Dense ( activation = \"relu\" , input_dim = 11 , units = 6 , kernel_initializer = \"uniform\" )) # Adding the second hidden layer classifier . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the output layer classifier . add ( Dense ( activation = \"sigmoid\" , units = 1 , kernel_initializer = \"uniform\" )) Till now we have added multiple layers to out classifier now let's compile them which can be done using compile method. Arguments added in final compilation will control whole neural network so be careful on this step. In [12]: # Compiling Neural Network classifier . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) We will now train our model on training data but still one thing is remaining. We use fit method to the fit our model In previous some steps I said that we will be optimizing our weights to improve model efficiency so when are we updating out weights? Batch size is used to specify the number of observation after which you want to update weight. Epoch is nothing but the total number of iterations. In [13]: # Fitting our model classifier . fit ( X_train , y_train , batch_size = 10 , epochs = 100 ) Epoch 1/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.4914 - acc: 0.8006 Epoch 2/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.4090 - acc: 0.8232 Epoch 3/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3953 - acc: 0.8291 Epoch 4/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3854 - acc: 0.8310 Epoch 5/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3782 - acc: 0.8322 Epoch 6/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3724 - acc: 0.8431 Epoch 7/100 8000/8000 [==============================] - 1s 142us/step - loss: 0.3688 - acc: 0.8449 Epoch 8/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3655 - acc: 0.8459 Epoch 9/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3628 - acc: 0.8506 Epoch 10/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3599 - acc: 0.8527 Epoch 11/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3571 - acc: 0.8526 Epoch 12/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3556 - acc: 0.8530 Epoch 13/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3544 - acc: 0.8531 Epoch 14/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3532 - acc: 0.8550 Epoch 15/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3509 - acc: 0.8560 Epoch 16/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3500 - acc: 0.8589 Epoch 17/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3500 - acc: 0.8576 Epoch 18/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3496 - acc: 0.8596 Epoch 19/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3485 - acc: 0.8572 Epoch 20/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3482 - acc: 0.8571 Epoch 21/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3476 - acc: 0.8587 Epoch 22/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3466 - acc: 0.8547 Epoch 23/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3470 - acc: 0.8585 Epoch 24/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3458 - acc: 0.8584 Epoch 25/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3458 - acc: 0.8590 Epoch 26/100 8000/8000 [==============================] - 1s 164us/step - loss: 0.3466 - acc: 0.8602 Epoch 27/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3447 - acc: 0.8591 Epoch 28/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3446 - acc: 0.8581 Epoch 29/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3439 - acc: 0.8590 Epoch 30/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3450 - acc: 0.8581 Epoch 31/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3445 - acc: 0.8592 Epoch 32/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3435 - acc: 0.8592 Epoch 33/100 8000/8000 [==============================] - 1s 158us/step - loss: 0.3424 - acc: 0.8607 Epoch 34/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3432 - acc: 0.8580 Epoch 35/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3433 - acc: 0.8597 Epoch 36/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3431 - acc: 0.8592 Epoch 37/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3430 - acc: 0.8609 Epoch 38/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3428 - acc: 0.8579 Epoch 39/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3424 - acc: 0.8595 Epoch 40/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3421 - acc: 0.8585 Epoch 41/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3419 - acc: 0.8592 Epoch 42/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3421 - acc: 0.8597 Epoch 43/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3419 - acc: 0.8582 Epoch 44/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3403 - acc: 0.8609 Epoch 45/100 8000/8000 [==============================] - 1s 184us/step - loss: 0.3414 - acc: 0.8569 Epoch 46/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3413 - acc: 0.8585 Epoch 47/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3414 - acc: 0.8606 Epoch 48/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3419 - acc: 0.8589 Epoch 49/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3411 - acc: 0.8589 Epoch 50/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3419 - acc: 0.8575 Epoch 51/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3403 - acc: 0.8617 Epoch 52/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3400 - acc: 0.8597 Epoch 53/100 8000/8000 [==============================] - 1s 155us/step - loss: 0.3409 - acc: 0.8596 Epoch 54/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3402 - acc: 0.8611 Epoch 55/100 8000/8000 [==============================] - 1s 158us/step - loss: 0.3401 - acc: 0.8591 Epoch 56/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3403 - acc: 0.8582 Epoch 57/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3404 - acc: 0.8622 Epoch 58/100 8000/8000 [==============================] - 1s 163us/step - loss: 0.3406 - acc: 0.8577 Epoch 59/100 8000/8000 [==============================] - 1s 160us/step - loss: 0.3394 - acc: 0.8592 Epoch 60/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3393 - acc: 0.8621 Epoch 61/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3411 - acc: 0.8601 Epoch 62/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3397 - acc: 0.8589 Epoch 63/100 8000/8000 [==============================] - 1s 160us/step - loss: 0.3400 - acc: 0.8590 Epoch 64/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3405 - acc: 0.8590 Epoch 65/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3395 - acc: 0.8606 Epoch 66/100 8000/8000 [==============================] - 1s 149us/step - loss: 0.3392 - acc: 0.8579 Epoch 67/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3397 - acc: 0.8590 Epoch 68/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3396 - acc: 0.8602 Epoch 69/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3390 - acc: 0.8610 Epoch 70/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3395 - acc: 0.8595 Epoch 71/100 8000/8000 [==============================] - 1s 165us/step - loss: 0.3389 - acc: 0.8597 Epoch 72/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3391 - acc: 0.8601 Epoch 73/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3389 - acc: 0.8605 Epoch 74/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3395 - acc: 0.8600 Epoch 75/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3391 - acc: 0.8600 Epoch 76/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3397 - acc: 0.8621 Epoch 77/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3385 - acc: 0.8614 Epoch 78/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3393 - acc: 0.8585 Epoch 79/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3390 - acc: 0.8597 Epoch 80/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3388 - acc: 0.8615 Epoch 81/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3395 - acc: 0.8622 Epoch 82/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3387 - acc: 0.8597 Epoch 83/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3385 - acc: 0.8602 Epoch 84/100 8000/8000 [==============================] - 1s 173us/step - loss: 0.3388 - acc: 0.8589 Epoch 85/100 8000/8000 [==============================] - 1s 166us/step - loss: 0.3389 - acc: 0.8617 Epoch 86/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3385 - acc: 0.8625 Epoch 87/100 8000/8000 [==============================] - 1s 154us/step - loss: 0.3379 - acc: 0.8579 Epoch 88/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3391 - acc: 0.8601 Epoch 89/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3378 - acc: 0.8601 Epoch 90/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3388 - acc: 0.8611 Epoch 91/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3384 - acc: 0.8596 Epoch 92/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3381 - acc: 0.8596 Epoch 93/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3380 - acc: 0.8576 Epoch 94/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3379 - acc: 0.8582 Epoch 95/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3376 - acc: 0.8591 Epoch 96/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3374 - acc: 0.8619 Epoch 97/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3370 - acc: 0.8626 Epoch 98/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3355 - acc: 0.8580 Epoch 99/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3361 - acc: 0.8625 Epoch 100/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3354 - acc: 0.8619 Out[13]: <keras.callbacks.History at 0x232a0752320> Predicting on the test data Predicting the test set result. The prediction result will give you probability of the customer leaving the company. We will convert that probability into binary 0 and 1. In [14]: # Predicting the Test set results y_pred = classifier . predict ( X_test ) y_pred = ( y_pred > 0.5 ) Confusion matrix of test set and prediction accuracy This is the final step where we are evaluating our model performance. We already have original results and thus we can build confusion matrix to check the accuracy of model. In [15]: # Creating the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix ( y_test , y_pred ) cm Out[15]: array([[1477, 111], [ 186, 226]], dtype=int64) In [16]: test_accuracy = np . sum ([ cm [ 0 , 0 ], cm [ 1 , 1 ]]) / np . sum ( cm ) test_accuracy Out[16]: 0.8515 We achieved 85.15% accuracy on the test set which is quite good. Adjusting the neural network Let's changing the structure of the network to see what happens. Let's add another hidden layer. In [18]: #Initializing Neural Network classifier2 = Sequential () # Adding the input layer and the first hidden layer classifier2 . add ( Dense ( activation = \"relu\" , input_dim = 11 , units = 8 , kernel_initializer = \"uniform\" )) # Adding the second hidden layer classifier2 . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the third hidden layer classifier2 . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the output layer classifier2 . add ( Dense ( activation = \"sigmoid\" , units = 1 , kernel_initializer = \"uniform\" )) # Compiling Neural Network classifier2 . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) # Fitting our model classifier2 . fit ( X_train , y_train , batch_size = 10 , epochs = 100 ) # Predicting the Test set results y_pred2 = classifier2 . predict ( X_test ) y_pred2 = ( y_pred2 > 0.5 ) cm2 = confusion_matrix ( y_test , y_pred2 ) test_accuracy2 = np . sum ([ cm2 [ 0 , 0 ], cm2 [ 1 , 1 ]]) / np . sum ( cm2 ) test_accuracy2 Epoch 1/100 8000/8000 [==============================] - 2s 280us/step - loss: 0.4712 - acc: 0.7961 Epoch 2/100 8000/8000 [==============================] - 1s 186us/step - loss: 0.4183 - acc: 0.7969 Epoch 3/100 8000/8000 [==============================] - 2s 226us/step - loss: 0.4090 - acc: 0.8270 Epoch 4/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3973 - acc: 0.8306 Epoch 5/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3861 - acc: 0.8331 Epoch 6/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3772 - acc: 0.8431 Epoch 7/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3696 - acc: 0.8480 Epoch 8/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3659 - acc: 0.8495 Epoch 9/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3629 - acc: 0.8541 Epoch 10/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3613 - acc: 0.8555 Epoch 11/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3595 - acc: 0.8529 Epoch 12/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3587 - acc: 0.8561 Epoch 13/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3570 - acc: 0.8552 Epoch 14/100 8000/8000 [==============================] - 2s 217us/step - loss: 0.3554 - acc: 0.8560 Epoch 15/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3546 - acc: 0.8561 Epoch 16/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3544 - acc: 0.8564 Epoch 17/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3532 - acc: 0.8571 Epoch 18/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3517 - acc: 0.8595 Epoch 19/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3513 - acc: 0.8582 Epoch 20/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3516 - acc: 0.8562 Epoch 21/100 8000/8000 [==============================] - 2s 211us/step - loss: 0.3506 - acc: 0.8579 Epoch 22/100 8000/8000 [==============================] - 2s 204us/step - loss: 0.3485 - acc: 0.8589 Epoch 23/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3500 - acc: 0.8586 Epoch 24/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3505 - acc: 0.8579 Epoch 25/100 8000/8000 [==============================] - 2s 215us/step - loss: 0.3487 - acc: 0.8581 Epoch 26/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3484 - acc: 0.8594 Epoch 27/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3488 - acc: 0.8602 Epoch 28/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3486 - acc: 0.8574 Epoch 29/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3477 - acc: 0.8592 Epoch 30/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3480 - acc: 0.8590 Epoch 31/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3473 - acc: 0.8589 Epoch 32/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3458 - acc: 0.8587 Epoch 33/100 8000/8000 [==============================] - 2s 205us/step - loss: 0.3461 - acc: 0.8591 Epoch 34/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3452 - acc: 0.8616 Epoch 35/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3453 - acc: 0.8560 Epoch 36/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3454 - acc: 0.8581 Epoch 37/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3454 - acc: 0.8599 Epoch 38/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3454 - acc: 0.8585 Epoch 39/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3441 - acc: 0.8604 Epoch 40/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3443 - acc: 0.8596 Epoch 41/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3433 - acc: 0.8582 Epoch 42/100 8000/8000 [==============================] - 1s 187us/step - loss: 0.3435 - acc: 0.8586 Epoch 43/100 8000/8000 [==============================] - 2s 215us/step - loss: 0.3438 - acc: 0.8594 Epoch 44/100 8000/8000 [==============================] - 2s 206us/step - loss: 0.3436 - acc: 0.8590 Epoch 45/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3431 - acc: 0.8612 Epoch 46/100 8000/8000 [==============================] - 1s 187us/step - loss: 0.3431 - acc: 0.8599 Epoch 47/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3440 - acc: 0.8596 Epoch 48/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3427 - acc: 0.8602 Epoch 49/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3433 - acc: 0.8606 Epoch 50/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3415 - acc: 0.8621 Epoch 51/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3430 - acc: 0.8587 Epoch 52/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3418 - acc: 0.8600 Epoch 53/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3427 - acc: 0.8611 Epoch 54/100 8000/8000 [==============================] - 2s 217us/step - loss: 0.3415 - acc: 0.8612 Epoch 55/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3423 - acc: 0.8599 Epoch 56/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3421 - acc: 0.8604 Epoch 57/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3417 - acc: 0.8609 0s - loss: 0.3412 - a Epoch 58/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3426 - acc: 0.8599 Epoch 59/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3414 - acc: 0.8605 Epoch 60/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3424 - acc: 0.8607 Epoch 61/100 8000/8000 [==============================] - 2s 193us/step - loss: 0.3413 - acc: 0.8616 Epoch 62/100 8000/8000 [==============================] - 2s 199us/step - loss: 0.3418 - acc: 0.8611 Epoch 63/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3408 - acc: 0.8604 Epoch 64/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3411 - acc: 0.8626 Epoch 65/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3418 - acc: 0.8625 Epoch 66/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3411 - acc: 0.8621 Epoch 67/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3412 - acc: 0.8591 Epoch 68/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3404 - acc: 0.8607 Epoch 69/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3406 - acc: 0.8619 Epoch 70/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3407 - acc: 0.8590 Epoch 71/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3417 - acc: 0.8601 Epoch 72/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3414 - acc: 0.8615 Epoch 73/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3407 - acc: 0.8607 Epoch 74/100 8000/8000 [==============================] - 2s 222us/step - loss: 0.3413 - acc: 0.8615 Epoch 75/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3411 - acc: 0.8620 Epoch 76/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3394 - acc: 0.8610 Epoch 77/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3405 - acc: 0.8626 Epoch 78/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3403 - acc: 0.8586 Epoch 79/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3399 - acc: 0.8624 Epoch 80/100 8000/8000 [==============================] - 2s 204us/step - loss: 0.3404 - acc: 0.8600 0s - loss: 0.3438 - a Epoch 81/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3392 - acc: 0.8632 Epoch 82/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3399 - acc: 0.8617 Epoch 83/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3395 - acc: 0.8622 Epoch 84/100 8000/8000 [==============================] - 2s 222us/step - loss: 0.3389 - acc: 0.8611 Epoch 85/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3401 - acc: 0.8606 Epoch 86/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3397 - acc: 0.8631 Epoch 87/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3388 - acc: 0.8626 Epoch 88/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3404 - acc: 0.8616 Epoch 89/100 8000/8000 [==============================] - 2s 193us/step - loss: 0.3398 - acc: 0.8629 Epoch 90/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3394 - acc: 0.8609 Epoch 91/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3397 - acc: 0.8607 Epoch 92/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3396 - acc: 0.8611 Epoch 93/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3391 - acc: 0.8632 Epoch 94/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3395 - acc: 0.8619 Epoch 95/100 8000/8000 [==============================] - 2s 219us/step - loss: 0.3390 - acc: 0.8614 Epoch 96/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3389 - acc: 0.8632 Epoch 97/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3394 - acc: 0.8634 Epoch 98/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3396 - acc: 0.8620 Epoch 99/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3376 - acc: 0.8625 Epoch 100/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3392 - acc: 0.8605 Out[18]: 0.8555 Now the prediction accuracy on test set has increased a little bit from 85.15% to 85.55%. Adjusting the architect of the neural network may achieve better results.","tags":"Blog","url":"https://ericchen23.github.io/2018/08/Churn-rate-prediction.html","loc":"https://ericchen23.github.io/2018/08/Churn-rate-prediction.html"},{"title":"SAT Results Analysis NYC","text":"Explore the SAT test in NYC The SAT, or Scholastic Aptitude Test, is a test that high schoolers take in the US before applying to college. Colleges take the test scores into account when making admissions decisions, so it's fairly important to do well on. The test is divided into 3 sections, each of which is scored out of 800 points. The total score is out of 2400 (although this has changed back and forth a few times, the scores in this dataset are out of 2400). High schools are often ranked by their average SAT scores, and high SAT scores are considered a sign of how good a school district is. Combine 7 related datasets together. Datasets descriptions can be found in Readme. Briefly, the datasets we use are listed as follows: SAT results - contains average SAT scores on the three subjects by each school and the number of SAT takers. High School Directory - contains information of each high school. Math test results contains math test results for each school. Class size - class size of each high school. AP test results - Advanced Placement exam results for each high school. Passing AP exams can get you college credit in the US. Graduation outcomes - percentage of graduated students and other outcome information. Demographics and accountability - demographic information for each school. School district maps - contains information on the layout of the school districts, so that we can map them out. School survey - surveys of parents, teachers, and students at each school (scores). Showcase: Data cleaning Data wrangling Data preprocessing Mapping and visulization Insights Understanding the data We first do a quick look at the data, i.e. what contains in each file. We read in all files and see what attributes are in each dataset. In [1]: import pandas as pd import numpy as np files = [ 'ap_college_board.csv' , 'class_size.csv' , 'demographics_and_accountability.csv' , 'graduation_outcomes.csv' , 'high_school_directory.csv' , 'math_test_results.csv' , 'sat_results.csv' ] data = {} for f in files : d = pd . read_csv ( \"data/ {} \" . format ( f )) data [ f . replace ( \".csv\" , \"\" )] = d In [2]: for k , v in data . items (): print ( \" \\n \" + k + \" \\n \" ) print ( v . head ( 5 )) ap_college_board DBN SchoolName AP Test Takers \\ 0 01M448 UNIVERSITY NEIGHBORHOOD H.S. 39 1 01M450 EAST SIDE COMMUNITY HS 19 2 01M515 LOWER EASTSIDE PREP 24 3 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 4 02M296 High School of Hospitality Management s Total Exams Taken Number of Exams with scores 3 4 or 5 0 49 10 1 21 s 2 26 24 3 377 191 4 s s class_size CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE \\ 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) \\ 0 - - 1 - - 2 - - 3 - - 4 - - SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED \\ 0 - 19.0 1 - 21.0 2 - 17.0 3 - 17.0 4 - 15.0 NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS \\ 0 1.0 19.0 19.0 1 1.0 21.0 21.0 2 1.0 17.0 17.0 3 1.0 17.0 17.0 4 1.0 15.0 15.0 SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO 0 19.0 ATS NaN 1 21.0 ATS NaN 2 17.0 ATS NaN 3 17.0 ATS NaN 4 15.0 ATS NaN demographics_and_accountability DBN Name schoolyear fl_percent frl_percent \\ 0 01M015 P.S. 015 ROBERTO CLEMENTE 20052006 89.4 NaN 1 01M015 P.S. 015 ROBERTO CLEMENTE 20062007 89.4 NaN 2 01M015 P.S. 015 ROBERTO CLEMENTE 20072008 89.4 NaN 3 01M015 P.S. 015 ROBERTO CLEMENTE 20082009 89.4 NaN 4 01M015 P.S. 015 ROBERTO CLEMENTE 20092010 96.5 total_enrollment prek k grade1 grade2 ... black_num black_per \\ 0 281 15 36 40 33 ... 74 26.3 1 243 15 29 39 38 ... 68 28.0 2 261 18 43 39 36 ... 77 29.5 3 252 17 37 44 32 ... 75 29.8 4 208 16 40 28 32 ... 67 32.2 hispanic_num hispanic_per white_num white_per male_num male_per female_num \\ 0 189 67.3 5 1.8 158.0 56.2 123.0 1 153 63.0 4 1.6 140.0 57.6 103.0 2 157 60.2 7 2.7 143.0 54.8 118.0 3 149 59.1 7 2.8 149.0 59.1 103.0 4 118 56.7 6 2.9 124.0 59.6 84.0 female_per 0 43.8 1 42.4 2 45.2 3 40.9 4 40.4 [5 rows x 38 columns] graduation_outcomes Demographic DBN School Name Cohort \\ 0 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2003 1 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2004 2 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2005 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 4 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 Aug Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n \\ 0 5 s s s 1 55 37 67.3% 17 2 64 43 67.2% 27 3 78 43 55.1% 36 4 78 44 56.4% 37 Total Regents - % of cohort Total Regents - % of grads \\ 0 s s 1 30.9% 45.9% 2 42.2% 62.8% 3 46.2% 83.7% 4 47.4% 84.1% ... Regents w/o Advanced - n \\ 0 ... s 1 ... 17 2 ... 27 3 ... 36 4 ... 37 Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads \\ 0 s s 1 30.9% 45.9% 2 42.2% 62.8% 3 46.2% 83.7% 4 47.4% 84.1% Local - n Local - % of cohort Local - % of grads Still Enrolled - n \\ 0 s s s s 1 20 36.4% 54.1% 15 2 16 25% 37.200000000000003% 9 3 7 9% 16.3% 16 4 7 9% 15.9% 15 Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 0 s s s 1 27.3% 3 5.5% 2 14.1% 9 14.1% 3 20.5% 11 14.1% 4 19.2% 11 14.1% [5 rows x 23 columns] high_school_directory dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max \\ 0 NaN NaN 1 NaN NaN 2 NaN NaN 3 9.0 14.0 4 NaN NaN ... priority08 priority09 \\ 0 ... NaN NaN 1 ... NaN NaN 2 ... NaN NaN 3 ... NaN NaN 4 ... NaN NaN priority10 Location 1 \\ 0 NaN 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 1 NaN 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 2 NaN 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 3 NaN 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 4 NaN 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... Community Board Council District Census Tract BIN BBL \\ 0 9.0 35.0 213.0 3029686.0 3.011870e+09 1 3.0 16.0 135.0 2004526.0 2.026340e+09 2 4.0 14.0 209.0 2008336.0 2.028590e+09 3 1.0 1.0 29.0 1001388.0 1.001130e+09 4 8.0 24.0 1267.0 4539721.0 4.068580e+09 NTA 0 Crown Heights South ... 1 Morrisania-Melrose ... 2 West Concourse ... 3 Chinatown ... 4 Pomonok-Flushing Heights-Hillcrest ... [5 rows x 64 columns] math_test_results DBN Grade Year Category Number Tested Mean Scale Score Level 1 # \\ 0 01M015 3 2006 All Students 39 667 2 1 01M015 3 2007 All Students 31 672 2 2 01M015 3 2008 All Students 37 668 0 3 01M015 3 2009 All Students 33 668 0 4 01M015 3 2010 All Students 26 677 6 Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # Level 4 % \\ 0 5.1% 11 28.2% 20 51.3% 6 15.4% 1 6.5% 3 9.7% 22 71% 4 12.9% 2 0% 6 16.2% 29 78.4% 2 5.4% 3 0% 4 12.1% 28 84.8% 1 3% 4 23.1% 12 46.2% 6 23.1% 2 7.7% Level 3+4 # Level 3+4 % 0 26 66.7% 1 26 83.9% 2 31 83.8% 3 29 87.9% 4 8 30.8% sat_results DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score \\ 0 29 355 404 1 91 383 423 2 70 377 402 3 7 414 401 4 44 390 433 SAT Writing Avg. Score 0 363 1 366 2 370 3 359 4 384 Combine Files into 1 Single Dataset DBN column appears in all the datasets, it is a unique code for each school in NYC. The problem now is that two of the datasets, class_size, and hs_directory, don't have a DBN field. In the high_school_directory data, it's just named dbn, so we can just rename the column, or copy it over into a new column called DBN. In the class_size data, we'll need to try a different approach. Most DBN in the dataset looks like \"01M448\". For example: In [3]: data [ \"ap_college_board\" ][ \"DBN\" ] . head ( 3 ) Out[3]: 0 01M448 1 01M450 2 01M515 Name: DBN, dtype: object Although the class_size dataset does not contain the DBN columnn, it has certain columns that can be used to build a DBN column. Let's look at the columns in class_size. The first 3 columns are just what we need. In [4]: data [ \"class_size\" ] . head () Out[4]: CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED - - - 19.0 1.0 19.0 19.0 19.0 ATS NaN 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT - - - 21.0 1.0 21.0 21.0 21.0 ATS NaN 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED - - - 17.0 1.0 17.0 17.0 17.0 ATS NaN 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT - - - 17.0 1.0 17.0 17.0 17.0 ATS NaN 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED - - - 15.0 1.0 15.0 15.0 15.0 ATS NaN Now we have enough information to build the DBN columns for class_size and high_school_directory. We add a new column DBN to these two datasets. In [5]: data [ \"class_size\" ][ \"DBN\" ] = ( data [ \"class_size\" ] . apply ( lambda x : \" {0:02d}{1} \" . format ( x [ \"CSD\" ], x [ \"SCHOOL CODE\" ]), axis = 1 )) data [ \"high_school_directory\" ][ \"DBN\" ] = data [ \"high_school_directory\" ][ \"dbn\" ] Adding in the survey data. Student, parent, and teacher suverys about the quality of schools. One of the most potentially interesting datasets to look at is the dataset on student, parent, and teacher surveys about the quality of schools. These surveys include information about the perceived safety of each school, academic standards, and more. Before we combine our datasets, let's add in the survey data. In this case, we'll add the survey data into our data dictionary, and then combine all the datasets afterwards. The survey data consists of 2 files, one for all schools, and one for school district 75. In [6]: survey1 = pd . read_excel ( \"data/survey_all.xlsx\" , sheet_name = 'Sheet1' ) survey2 = pd . read_excel ( \"data/survey_d75.xlsx\" , sheet_name = 'Sheet1' ) print ( survey1 . head ()) print ( survey2 . head ()) dbn sch_type location \\ 0 01M015 Elementary School P.S. 015 Roberto Clemente 1 01M019 Elementary School P.S. 019 Asher Levy 2 01M020 Elementary School P.S. 020 Anna Silver 3 01M034 Elementary / Middle School P.S. 034 Franklin D. Roosevelt 4 01M063 Elementary School P.S. 063 William McKinley enrollment borough principal studentsurvey rr_s rr_t \\ 0 198 MANHATTAN Thomas Staebell No NaN 93 1 286 MANHATTAN JACQUELINE FLANAGAN No NaN 69 2 539 MANHATTAN James Lee No NaN 59 3 396 MANHATTAN Joyce Stallings Harte Yes 91.0 48 4 172 MANHATTAN DARLENE DESPEIGNES No NaN 55 rr_p ... s_N_q14e_3 s_N_q14e_4 s_N_q14f_1 s_N_q14f_2 \\ 0 63 ... NaN NaN NaN NaN 1 33 ... NaN NaN NaN NaN 2 44 ... NaN NaN NaN NaN 3 39 ... 26.0 14.0 28.0 58.0 4 73 ... NaN NaN NaN NaN s_N_q14f_3 s_N_q14f_4 s_N_q14g_1 s_N_q14g_2 s_N_q14g_3 s_N_q14g_4 0 NaN NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN NaN 3 39.0 25.0 46.0 41.0 17.0 12.0 4 NaN NaN NaN NaN NaN NaN [5 rows x 1976 columns] dbn sch_type location enrollment borough principal \\ 0 75K004 ES P.S. K004 320 BROOKLYN Deborah Evans 1 75K036 ES/MS/HS P.S. K036 292 BROOKLYN Johanna Schneider 2 75K053 ES/MS/HS P.S. K053 375 BROOKLYN AMY BLUTSTEIN 3 75K077 ES/MS/HS P.S. K077 294 BROOKLYN Merryl Redner-Cohen 4 75K140 ES/MS P.S. K140 282 BROOKLYN Michelle Carpenter studentsurvey rr_s rr_t rr_p ... s_q14_2 s_q14_3 s_q14_4 \\ 0 NO NaN 72 33 ... NaN NaN NaN 1 Yes 8.0 20 9 ... 36.0 18.0 45.0 2 Yes 83.0 79 56 ... 11.0 15.0 10.0 3 Yes 76.0 73 54 ... 8.0 4.0 10.0 4 Yes 88.0 87 42 ... 33.0 41.0 9.0 s_q14_5 s_q14_6 s_q14_7 s_q14_8 s_q14_9 s_q14_10 s_q14_11 0 NaN NaN NaN NaN NaN NaN NaN 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 15.0 11.0 6.0 18.0 4.0 1.0 1.0 3 13.0 17.0 13.0 10.0 8.0 12.0 2.0 4 0.0 0.0 0.0 0.0 0.0 0.0 3.0 [5 rows x 1800 columns] In [7]: survey1 [ \"d75\" ] = False survey2 [ \"d75\" ] = True survey = pd . concat ([ survey1 , survey2 ], axis = 0 , sort = False ) print ( survey1 . shape ) print ( survey2 . shape ) print ( survey . shape ) survey . head () (1597, 1977) (56, 1801) (1653, 2852) Out[7]: dbn sch_type location enrollment borough principal studentsurvey rr_s rr_t rr_p ... s_q14_2 s_q14_3 s_q14_4 s_q14_5 s_q14_6 s_q14_7 s_q14_8 s_q14_9 s_q14_10 s_q14_11 0 01M015 Elementary School P.S. 015 Roberto Clemente 198 MANHATTAN Thomas Staebell No NaN 93 63 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 01M019 Elementary School P.S. 019 Asher Levy 286 MANHATTAN JACQUELINE FLANAGAN No NaN 69 33 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 01M020 Elementary School P.S. 020 Anna Silver 539 MANHATTAN James Lee No NaN 59 44 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 01M034 Elementary / Middle School P.S. 034 Franklin D. Roosevelt 396 MANHATTAN Joyce Stallings Harte Yes 91.0 48 39 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 01M063 Elementary School P.S. 063 William McKinley 172 MANHATTAN DARLENE DESPEIGNES No NaN 55 73 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 2852 columns Too many columns are in the survey data, and most of them are extraneous. By looking at the column dictionary of the survey data, let's just keep the important fields and remove any extraneous columns. In [8]: import warnings warnings . filterwarnings ( 'ignore' ) survey [ \"DBN\" ] = survey [ \"dbn\" ] survey_fields = [ \"DBN\" , \"rr_s\" , \"rr_t\" , \"rr_p\" , \"N_s\" , \"N_t\" , \"N_p\" , \"saf_p_11\" , \"com_p_11\" , \"eng_p_11\" , \"aca_p_11\" , \"saf_t_11\" , \"com_t_11\" , \"eng_t_10\" , \"aca_t_11\" , \"saf_s_11\" , \"com_s_11\" , \"eng_s_11\" , \"aca_s_11\" , \"saf_tot_11\" , \"com_tot_11\" , \"eng_tot_11\" , \"aca_tot_11\" ,] survey = survey . loc [:, survey_fields ] data [ \"survey\" ] = survey survey . shape Out[8]: (1653, 23) Condensing data, more preprocessing The datasets such as class_size , demographics_and_accountabilities, have multiple rows for each school, whereas the datasets such as sat_results and high_school_directory only has one row per school. We need to find a way to combine these datasets together. For example: In [9]: print ( data [ \"demographics_and_accountability\" ] . head ()) print ( data [ \"class_size\" ] . head ()) DBN Name schoolyear fl_percent frl_percent \\ 0 01M015 P.S. 015 ROBERTO CLEMENTE 20052006 89.4 NaN 1 01M015 P.S. 015 ROBERTO CLEMENTE 20062007 89.4 NaN 2 01M015 P.S. 015 ROBERTO CLEMENTE 20072008 89.4 NaN 3 01M015 P.S. 015 ROBERTO CLEMENTE 20082009 89.4 NaN 4 01M015 P.S. 015 ROBERTO CLEMENTE 20092010 96.5 total_enrollment prek k grade1 grade2 ... black_num black_per \\ 0 281 15 36 40 33 ... 74 26.3 1 243 15 29 39 38 ... 68 28.0 2 261 18 43 39 36 ... 77 29.5 3 252 17 37 44 32 ... 75 29.8 4 208 16 40 28 32 ... 67 32.2 hispanic_num hispanic_per white_num white_per male_num male_per female_num \\ 0 189 67.3 5 1.8 158.0 56.2 123.0 1 153 63.0 4 1.6 140.0 57.6 103.0 2 157 60.2 7 2.7 143.0 54.8 118.0 3 149 59.1 7 2.8 149.0 59.1 103.0 4 118 56.7 6 2.9 124.0 59.6 84.0 female_per 0 43.8 1 42.4 2 45.2 3 40.9 4 40.4 [5 rows x 38 columns] CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE \\ 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) \\ 0 - - 1 - - 2 - - 3 - - 4 - - SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED \\ 0 - 19.0 1 - 21.0 2 - 17.0 3 - 17.0 4 - 15.0 NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS \\ 0 1.0 19.0 19.0 1 1.0 21.0 21.0 2 1.0 17.0 17.0 3 1.0 17.0 17.0 4 1.0 15.0 15.0 SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO DBN 0 19.0 ATS NaN 01M015 1 21.0 ATS NaN 01M015 2 17.0 ATS NaN 01M015 3 17.0 ATS NaN 01M015 4 15.0 ATS NaN 01M015 In [10]: print ( data [ \"sat_results\" ] . head ()) print ( data [ \"high_school_directory\" ] . head ()) DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score \\ 0 29 355 404 1 91 383 423 2 70 377 402 3 7 414 401 4 44 390 433 SAT Writing Avg. Score 0 363 1 366 2 370 3 359 4 384 dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max ... priority09 priority10 \\ 0 NaN NaN ... NaN NaN 1 NaN NaN ... NaN NaN 2 NaN NaN ... NaN NaN 3 9.0 14.0 ... NaN NaN 4 NaN NaN ... NaN NaN Location 1 Community Board \\ 0 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 9.0 1 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 3.0 2 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 4.0 3 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 1.0 4 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... 8.0 Council District Census Tract BIN BBL \\ 0 35.0 213.0 3029686.0 3.011870e+09 1 16.0 135.0 2004526.0 2.026340e+09 2 14.0 209.0 2008336.0 2.028590e+09 3 1.0 29.0 1001388.0 1.001130e+09 4 24.0 1267.0 4539721.0 4.068580e+09 NTA DBN 0 Crown Heights South ... 17K548 1 Morrisania-Melrose ... 09X543 2 West Concourse ... 09X327 3 Chinatown ... 02M280 4 Pomonok-Flushing Heights-Hillcrest ... 28Q680 [5 rows x 65 columns] In order to concatenate these datasets into 1 dataset, we need to make every data set contains only one row per school. If not, we cannnot compare SAT results to the class size. We can accomplish this by first understanding the data better, then by doing some aggregation. With the class_size dataset, it looks like GRADE and PROGRAM TYPE have multiple values for each school. By restricting each field to a single value, we can filter most of the duplicate rows. In the below code, we: Only select values from class_size where the GRADE field is 09-12. Only select values from class_size where the PROGRAM TYPE field is GEN ED. Group the class_size dataset by DBN, and take the average of each column. -Essentially, we'll find the average class_size values for each school. Reset the index, so DBN is added back in as a column. In [11]: class_size = data [ \"class_size\" ] # we only keep the high school data, which is grade 9 to 12 with general education class_size = class_size [ class_size [ \"GRADE \" ] == \"09-12\" ] class_size = class_size [ class_size [ \"PROGRAM TYPE\" ] == \"GEN ED\" ] #the aggregation by mean will drop non-numerical columns class_size = class_size . groupby ( \"DBN\" ) . agg ( np . mean ) class_size . reset_index ( inplace = True ) data [ \"class_size\" ] = class_size In [12]: # condensing the demographics_and_accountability dataset # we choose the school year of 20112012 demographics_and_accountability = data [ \"demographics_and_accountability\" ] demographics_and_accountability = demographics_and_accountability [ demographics_and_accountability [ \"schoolyear\" ] == 20112012 ] data [ \"demographics_and_accountability\" ] = demographics_and_accountability In [13]: #condensing the math_test_results dataset # we choose the year of 2011, and Grade 8 data [ \"math_test_results\" ] = data [ \"math_test_results\" ][ data [ \"math_test_results\" ][ \"Year\" ] == 2011 ] data [ \"math_test_results\" ] = data [ \"math_test_results\" ][ data [ \"math_test_results\" ][ \"Grade\" ] == '8' ] data [ \"math_test_results\" ] . head () Out[13]: DBN Grade Year Category Number Tested Mean Scale Score Level 1 # Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # Level 4 % Level 3+4 # Level 3+4 % 111 01M034 8 2011 All Students 48 646 15 31.3% 22 45.8% 11 22.9% 0 0% 11 22.9% 280 01M140 8 2011 All Students 61 665 1 1.6% 43 70.5% 17 27.9% 0 0% 17 27.9% 346 01M184 8 2011 All Students 49 727 0 0% 0 0% 5 10.2% 44 89.8% 49 100% 388 01M188 8 2011 All Students 49 658 10 20.4% 26 53.1% 10 20.4% 3 6.1% 13 26.5% 411 01M292 8 2011 All Students 49 650 15 30.6% 25 51% 7 14.3% 2 4.1% 9 18.4% In [14]: # condesing the graduation_outcomes dataset data [ \"graduation_outcomes\" ] = data [ \"graduation_outcomes\" ][ data [ \"graduation_outcomes\" ][ \"Cohort\" ] == \"2006\" ] data [ \"graduation_outcomes\" ] = data [ \"graduation_outcomes\" ][ data [ \"graduation_outcomes\" ][ \"Demographic\" ] == \"Total Cohort\" ] data [ \"graduation_outcomes\" ] . head () Out[14]: Demographic DBN School Name Cohort Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n Total Regents - % of cohort Total Regents - % of grads ... Regents w/o Advanced - n Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads Local - n Local - % of cohort Local - % of grads Still Enrolled - n Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 78 43 55.1% 36 46.2% 83.7% ... 36 46.2% 83.7% 7 9% 16.3% 16 20.5% 11 14.1% 10 Total Cohort 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2006 124 53 42.7% 42 33.9% 79.2% ... 34 27.4% 64.2% 11 8.9% 20.8% 46 37.1% 20 16.100000000000001% 17 Total Cohort 01M450 EAST SIDE COMMUNITY SCHOOL 2006 90 70 77.8% 67 74.400000000000006% 95.7% ... 67 74.400000000000006% 95.7% 3 3.3% 4.3% 15 16.7% 5 5.6% 24 Total Cohort 01M509 MARTA VALLE HIGH SCHOOL 2006 84 47 56% 40 47.6% 85.1% ... 23 27.4% 48.9% 7 8.300000000000001% 14.9% 25 29.8% 5 6% 31 Total Cohort 01M515 LOWER EAST SIDE PREPARATORY HIGH SCHO 2006 193 105 54.4% 91 47.2% 86.7% ... 22 11.4% 21% 14 7.3% 13.3% 53 27.5% 35 18.100000000000001% 5 rows × 23 columns Feature engineering For the sat_results dataset, we add a new column to compute sat_score to store the total score of 'SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score' In [15]: cols = [ 'SAT Math Avg. Score' , 'SAT Critical Reading Avg. Score' , 'SAT Writing Avg. Score' ] # convert string to numerical data for c in cols : data [ \"sat_results\" ][ c ] = data [ \"sat_results\" ][ c ] . convert_objects ( convert_numeric = True ) data [ \"sat_results\" ][ \"sat_score\" ] = ( data [ \"sat_results\" ][ cols [ 0 ]] + data [ \"sat_results\" ][ cols [ 1 ]] + data [ \"sat_results\" ][ cols [ 2 ]]) data [ \"sat_results\" ][ \"sat_score\" ] . head () Out[15]: 0 1122.0 1 1172.0 2 1149.0 3 1174.0 4 1207.0 Name: sat_score, dtype: float64 Now, we are finding the coordinates of each school so that we can make maps. The coordinates can be parsed out from the \"Location 1\" column in the high_school_directory dataset. In [16]: data [ \"high_school_directory\" ][ 'latitude' ] = data [ \"high_school_directory\" ][ 'Location 1' ] . apply ( lambda x : x . split ( \" \\n \" )[ - 1 ] . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . split ( \", \" )[ 0 ]) data [ \"high_school_directory\" ][ 'longitude' ] = data [ \"high_school_directory\" ][ 'Location 1' ] . apply ( lambda x : x . split ( \" \\n \" )[ - 1 ] . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . split ( \", \" )[ 1 ]) # convert string to numerical data for c in [ \"latitude\" , \"longitude\" ]: data [ \"high_school_directory\" ][ c ] = data [ \"high_school_directory\" ][ c ] . convert_objects ( convert_numeric = True ) print ( data [ \"high_school_directory\" ][ 'latitude' ] . head ()) print ( data [ \"high_school_directory\" ][ 'longitude' ] . head ()) 0 40.670299 1 40.827603 2 40.842414 3 40.710679 4 40.718810 Name: latitude, dtype: float64 0 -73.961648 1 -73.904475 2 -73.916162 3 -74.000807 4 -73.806500 Name: longitude, dtype: float64 We have done the preliminaries now. Let's see what are in each dataset. In [17]: for k , v in data . items (): print ( \" \\n \" + k + \" \\n \" ) print ( v . head ()) ap_college_board DBN SchoolName AP Test Takers \\ 0 01M448 UNIVERSITY NEIGHBORHOOD H.S. 39 1 01M450 EAST SIDE COMMUNITY HS 19 2 01M515 LOWER EASTSIDE PREP 24 3 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 4 02M296 High School of Hospitality Management s Total Exams Taken Number of Exams with scores 3 4 or 5 0 49 10 1 21 s 2 26 24 3 377 191 4 s s class_size DBN CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS \\ 0 01M292 1 88.0000 4.000000 1 01M332 1 46.0000 2.000000 2 01M378 1 33.0000 1.000000 3 01M448 1 105.6875 4.750000 4 01M450 1 57.6000 2.733333 AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS \\ 0 22.564286 18.50 26.571429 1 22.000000 21.00 23.500000 2 33.000000 33.00 33.000000 3 22.231250 18.25 27.062500 4 21.200000 19.40 22.866667 SCHOOLWIDE PUPIL-TEACHER RATIO 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN demographics_and_accountability DBN Name schoolyear \\ 6 01M015 P.S. 015 ROBERTO CLEMENTE 20112012 13 01M019 P.S. 019 ASHER LEVY 20112012 20 01M020 PS 020 ANNA SILVER 20112012 27 01M034 PS 034 FRANKLIN D ROOSEVELT 20112012 35 01M063 PS 063 WILLIAM MCKINLEY 20112012 fl_percent frl_percent total_enrollment prek k grade1 grade2 \\ 6 NaN 89.4 189 13 31 35 28 13 NaN 61.5 328 32 46 52 54 20 NaN 92.5 626 52 102 121 87 27 NaN 99.7 401 14 34 38 36 35 NaN 78.9 176 18 20 30 21 ... black_num black_per hispanic_num hispanic_per white_num \\ 6 ... 63 33.3 109 57.7 4 13 ... 81 24.7 158 48.2 28 20 ... 55 8.8 357 57.0 16 27 ... 90 22.4 275 68.6 8 35 ... 41 23.3 110 62.5 15 white_per male_num male_per female_num female_per 6 2.1 97.0 51.3 92.0 48.7 13 8.5 147.0 44.8 181.0 55.2 20 2.6 330.0 52.7 296.0 47.3 27 2.0 204.0 50.9 197.0 49.1 35 8.5 97.0 55.1 79.0 44.9 [5 rows x 38 columns] graduation_outcomes Demographic DBN School Name Cohort \\ 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 10 Total Cohort 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2006 17 Total Cohort 01M450 EAST SIDE COMMUNITY SCHOOL 2006 24 Total Cohort 01M509 MARTA VALLE HIGH SCHOOL 2006 31 Total Cohort 01M515 LOWER EAST SIDE PREPARATORY HIGH SCHO 2006 Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n \\ 3 78 43 55.1% 36 10 124 53 42.7% 42 17 90 70 77.8% 67 24 84 47 56% 40 31 193 105 54.4% 91 Total Regents - % of cohort Total Regents - % of grads \\ 3 46.2% 83.7% 10 33.9% 79.2% 17 74.400000000000006% 95.7% 24 47.6% 85.1% 31 47.2% 86.7% ... Regents w/o Advanced - n \\ 3 ... 36 10 ... 34 17 ... 67 24 ... 23 31 ... 22 Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads \\ 3 46.2% 83.7% 10 27.4% 64.2% 17 74.400000000000006% 95.7% 24 27.4% 48.9% 31 11.4% 21% Local - n Local - % of cohort Local - % of grads Still Enrolled - n \\ 3 7 9% 16.3% 16 10 11 8.9% 20.8% 46 17 3 3.3% 4.3% 15 24 7 8.300000000000001% 14.9% 25 31 14 7.3% 13.3% 53 Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 3 20.5% 11 14.1% 10 37.1% 20 16.100000000000001% 17 16.7% 5 5.6% 24 29.8% 5 6% 31 27.5% 35 18.100000000000001% [5 rows x 23 columns] high_school_directory dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max ... \\ 0 NaN NaN ... 1 NaN NaN ... 2 NaN NaN ... 3 9.0 14.0 ... 4 NaN NaN ... Location 1 Community Board \\ 0 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 9.0 1 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 3.0 2 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 4.0 3 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 1.0 4 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... 8.0 Council District Census Tract BIN BBL \\ 0 35.0 213.0 3029686.0 3.011870e+09 1 16.0 135.0 2004526.0 2.026340e+09 2 14.0 209.0 2008336.0 2.028590e+09 3 1.0 29.0 1001388.0 1.001130e+09 4 24.0 1267.0 4539721.0 4.068580e+09 NTA DBN latitude \\ 0 Crown Heights South ... 17K548 40.670299 1 Morrisania-Melrose ... 09X543 40.827603 2 West Concourse ... 09X327 40.842414 3 Chinatown ... 02M280 40.710679 4 Pomonok-Flushing Heights-Hillcrest ... 28Q680 40.718810 longitude 0 -73.961648 1 -73.904475 2 -73.916162 3 -74.000807 4 -73.806500 [5 rows x 67 columns] math_test_results DBN Grade Year Category Number Tested Mean Scale Score \\ 111 01M034 8 2011 All Students 48 646 280 01M140 8 2011 All Students 61 665 346 01M184 8 2011 All Students 49 727 388 01M188 8 2011 All Students 49 658 411 01M292 8 2011 All Students 49 650 Level 1 # Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # \\ 111 15 31.3% 22 45.8% 11 22.9% 0 280 1 1.6% 43 70.5% 17 27.9% 0 346 0 0% 0 0% 5 10.2% 44 388 10 20.4% 26 53.1% 10 20.4% 3 411 15 30.6% 25 51% 7 14.3% 2 Level 4 % Level 3+4 # Level 3+4 % 111 0% 11 22.9% 280 0% 17 27.9% 346 89.8% 49 100% 388 6.1% 13 26.5% 411 4.1% 9 18.4% sat_results DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score \\ 0 29 355.0 1 91 383.0 2 70 377.0 3 7 414.0 4 44 390.0 SAT Math Avg. Score SAT Writing Avg. Score sat_score 0 404.0 363.0 1122.0 1 423.0 366.0 1172.0 2 402.0 370.0 1149.0 3 401.0 359.0 1174.0 4 433.0 384.0 1207.0 survey DBN rr_s rr_t rr_p N_s N_t N_p saf_p_11 com_p_11 eng_p_11 \\ 0 01M015 NaN 93 63 NaN 27.0 104.0 NaN NaN NaN 1 01M019 NaN 69 33 NaN 24.0 89.0 NaN NaN NaN 2 01M020 NaN 59 44 NaN 32.0 207.0 NaN NaN NaN 3 01M034 91.0 48 39 159.0 22.0 119.0 NaN NaN NaN 4 01M063 NaN 55 73 NaN 17.0 117.0 NaN NaN NaN ... eng_t_10 aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 \\ 0 ... 7.9 NaN NaN NaN NaN NaN 1 ... 7.8 NaN NaN NaN NaN NaN 2 ... 6.9 NaN NaN NaN NaN NaN 3 ... 6.7 NaN NaN NaN NaN NaN 4 ... 7.5 NaN NaN NaN NaN NaN saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN [5 rows x 23 columns] Combining all datasets together From the above display, we see that each dataset has a column of \"DBN\". We can use this column to combine all datasets into one. Handle missing values during combining When we join them, it's important to note that some of the datasets are missing high schools that exist in the sat_results dataset. To resolve this, we'll need to merge the datasets that have missing rows using the outer join strategy, so we don't lose data. It's a common thing to have missing values. In [18]: flat_data_names = [ k for k , v in data . items ()] flat_data = [ data [ k ] for k in flat_data_names ] # flat_data[0] is our first dataset which is ap_college_board full = flat_data [ 0 ] for i , f in enumerate ( flat_data [ 1 :]): name = flat_data_names [ i + 1 ] print ( name ) print ( len ( f [ \"DBN\" ]) - len ( f [ \"DBN\" ] . unique ())) join_type = \"inner\" if name in [ \"sat_results\" , \"ap_college_board\" , \"graduation_outcomes\" ]: join_type = \"outer\" if name not in [ \"ap_college_board\" ]: full = full . merge ( f , on = \"DBN\" , how = join_type ) full . shape class_size 0 demographics_and_accountability 0 graduation_outcomes 0 high_school_directory 0 math_test_results 0 sat_results 0 survey 0 Out[18]: (455, 180) In [19]: full . head () Out[19]: DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS ... eng_t_10 aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 0 01M450 EAST SIDE COMMUNITY HS 19 21 s 1.0 57.600000 2.733333 21.200000 19.400000 ... 8.1 NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 377 191 1.0 156.368421 6.157895 25.510526 19.473684 ... 6.6 NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 02M408 PROFESSIONAL PERFORMING ARTS 20 20 15 2.0 104.882353 3.529412 28.952941 25.117647 ... 8.1 NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 02M655 LIFE SCIENCES SECONDARY SCHL 50 90 10 2.0 138.052632 5.789474 23.921053 16.684211 ... 6.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 03M415 WADLEIGH ARTS HIGH SCHOOL 65 73 s 3.0 142.846154 6.000000 23.600000 18.307692 ... 5.7 NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 180 columns Adding missing values The full dataset contains almost all the information we need for analysis. But, there are a few missing pieces. We may want to correlate the AP (Advanced Placement) exam results with SAT results, but first we need to convert those columns to numbers, then fill in the missing values. In [20]: cols = [ 'AP Test Takers ' , 'Total Exams Taken' , 'Number of Exams with scores 3 4 or 5' ] for col in cols : full [ col ] = full [ col ] . infer_objects () full [ cols ] = full [ cols ] . fillna ( value = 0 ) Add a new column \"school_dist\" that indicates the school district of each school. It can be extracted from the \"DBN\" column. In [21]: full [ \"school_dist\" ] = full [ \"DBN\" ] . apply ( lambda x : x [: 2 ]) Finally, fill any missing values with the mean of the column. In [22]: full = full . fillna ( full . mean ()) full . head () Out[22]: DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS ... aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 school_dist 0 01M450 EAST SIDE COMMUNITY HS 19 21 s 1.0 57.600000 2.733333 21.200000 19.400000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 01 1 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 377 191 1.0 156.368421 6.157895 25.510526 19.473684 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 01 2 02M408 PROFESSIONAL PERFORMING ARTS 20 20 15 2.0 104.882353 3.529412 28.952941 25.117647 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 02 3 02M655 LIFE SCIENCES SECONDARY SCHL 50 90 10 2.0 138.052632 5.789474 23.921053 16.684211 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 02 4 03M415 WADLEIGH ARTS HIGH SCHOOL 65 73 s 3.0 142.846154 6.000000 23.600000 18.307692 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 03 5 rows × 181 columns Computing variable correlations A good way to explore a dataset and see what columns are related to the one you care about is to compute correlations. This will tell you which columns are closely related to the column you're interested in. The sat_score column is what we care about. So let's see the its correlations with other columns. In [23]: full . corr ()[ 'sat_score' ] Out[23]: CSD 0.053884 NUMBER OF STUDENTS / SEATS FILLED 0.082401 NUMBER OF SECTIONS 0.069660 AVERAGE CLASS SIZE 0.085135 SIZE OF SMALLEST CLASS 0.048874 SIZE OF LARGEST CLASS 0.091046 SCHOOLWIDE PUPIL-TEACHER RATIO NaN schoolyear NaN frl_percent -0.188389 total_enrollment 0.138870 ell_num -0.123795 ell_percent -0.135156 sped_num -0.064685 sped_percent -0.157326 asian_num 0.193204 asian_per 0.191027 black_num 0.027135 black_per -0.064892 hispanic_num -0.054458 hispanic_per -0.102325 white_num 0.166775 white_per 0.168373 male_num 0.084583 male_per -0.071738 female_num 0.177495 female_per 0.071738 Total Cohort 0.061225 grade_span_min -0.048926 grade_span_max NaN expgrade_span_min NaN ... latitude -0.061575 longitude -0.006395 Year NaN Number Tested 0.089128 SAT Critical Reading Avg. Score 0.976192 SAT Math Avg. Score 0.953759 SAT Writing Avg. Score 0.981303 sat_score 1.000000 rr_s 0.283358 rr_t -0.056380 rr_p 0.057937 N_s 0.393010 N_t 0.242898 N_p 0.371894 saf_p_11 NaN com_p_11 NaN eng_p_11 NaN aca_p_11 NaN saf_t_11 NaN com_t_11 NaN eng_t_10 0.018455 aca_t_11 NaN saf_s_11 NaN com_s_11 NaN eng_s_11 NaN aca_s_11 NaN saf_tot_11 NaN com_tot_11 NaN eng_tot_11 NaN aca_tot_11 NaN Name: sat_score, Length: 69, dtype: float64 There are so many variables, it's hard to find interesting patterns. Let's see the top 10 biggest absolute correlations. In [24]: full . corr ()[ 'sat_score' ] . abs () . sort_values ( ascending = False ) . head ( 10 ) Out[24]: sat_score 1.000000 SAT Writing Avg. Score 0.981303 SAT Critical Reading Avg. Score 0.976192 SAT Math Avg. Score 0.953759 N_s 0.393010 N_p 0.371894 rr_s 0.283358 N_t 0.242898 asian_num 0.193204 asian_per 0.191027 Name: sat_score, dtype: float64 It is easy to recognize that the most correlated columns to the sat_score column are the test scores of each subjects. Of course, this is true. Because, the sat_score is the sum of the three subjects. We could use a pair plot to visualize the correlations. In [25]: import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline plt . style . use ( 'ggplot' ) sns . set ( style = \"ticks\" , color_codes = True ) cols = [ 'sat_score' , 'SAT Writing Avg. Score' , 'SAT Critical Reading Avg. Score' , 'SAT Math Avg. Score' ] vis_corr = full [ cols ] sns . pairplot ( vis_corr ) plt . show () Gender influence Now let's look at how gender correlates to the sat_score by investigating the female percentage and male percentage. We can find something interesting that the female percentage is positively correlated to sat_score , whereas male percentage is negatively correlated to sat_score . Females are somehow better at studying than male students. LOL. In [26]: print ( full . corr ()[ 'sat_score' ][[ 'female_per' , 'male_per' ]]) full . corr ()[ 'sat_score' ][[ 'female_per' , 'male_per' ]] . plot . bar () female_per 0.071738 male_per -0.071738 Name: sat_score, dtype: float64 Out[26]: <matplotlib.axes._subplots.AxesSubplot at 0xa27dda0> Race influence We have the data of percentage from four races (white, black, asian, hispanic). Let's find out how they are affecting the sat_score . I don't judge too much on races, just providing the facts. Findings: Asian and White percentages have positve correlation with the sat_score , whereas Black and Hispanic percentages have negative correlations with the sat_score . Asian students has the highest positve correlation among the four groups. Hispanic students has the highest negative correlations among the four groups. In [27]: full . corr ()[ 'sat_score' ][[ 'white_per' , 'black_per' , 'asian_per' , 'hispanic_per' ]] Out[27]: white_per 0.168373 black_per -0.064892 asian_per 0.191027 hispanic_per -0.102325 Name: sat_score, dtype: float64 In [28]: full . corr ()[ 'sat_score' ][[ 'white_per' , 'black_per' , 'asian_per' , 'hispanic_per' ]] . plot . bar ( color = 'b' ) Out[28]: <matplotlib.axes._subplots.AxesSubplot at 0x9e20f60> Atittude of respondents' influence Now let's see how students', parents', and teachers' ratings influence the SAT scores of the school. Findings: Students' ratings have a strong positive correlation to the SAT results. It might be due to students have a better understanding of their schools' performance. Parents' ratings also have a positive correlation to the SAT results. Teachers' ratings have a negative correlation which is interesting. In [29]: # rr_s students' ratings # rr_p parents' ratings # rr_t teachers' ratings full . corr ()[ 'sat_score' ][[ 'rr_s' , 'rr_p' , 'rr_t' ]] Out[29]: rr_s 0.283358 rr_p 0.057937 rr_t -0.056380 Name: sat_score, dtype: float64 In [30]: # visulize the correlations of respondents' ratings to the SAT scores full . corr ()[ 'sat_score' ][[ 'rr_s' , 'rr_p' , 'rr_t' ]] . plot ( kind = 'bar' , color = 'b' ) Out[30]: <matplotlib.axes._subplots.AxesSubplot at 0xba73748> Foreign language speaker differences Foreign language speaker means that English is not their first language. In other words, they are English language learners. In [31]: print ( full . corr ()[ 'sat_score' ][ 'ell_percent' ]) full . plot . scatter ( x = 'ell_percent' , y = 'sat_score' ) -0.13515582997474967 Out[31]: <matplotlib.axes._subplots.AxesSubplot at 0x9e5e898> Setting the context Map We'll map out the positions of the schools, which will give us a better visualization. This is done by: Setup a map centered on NYC. Add a marker for each school on the map. Display the map. In [32]: import folium from folium import plugins schools_map = folium . Map ( location = [ 40.7425 , - 73.9250 ], zoom_start = 10 ) marker_cluster = plugins . MarkerCluster () . add_to ( schools_map ) full [ \"school_name\" ] . fillna ( \"Missing\" , inplace = True ) full [ \"DBN\" ] . fillna ( \"Missing\" , inplace = True ) for row in full . iterrows (): folium . Marker ( location = [ row [ 1 ][ 'latitude' ], row [ 1 ][ 'longitude' ]], popup = \" {} \" . format ( row [ 1 ][ 'DBN' ])) . add_to ( marker_cluster ) # schools_map.createmap('schools.html') schools_map Out[32]: Let's use a heatmap to better visualize the assembly of schools in this area. In [33]: schools_heatmap = folium . Map ( location = [ full [ 'latitude' ] . mean (), full [ 'longitude' ] . mean ()], zoom_start = 10 ) schools_heatmap . add_children ( plugins . HeatMap ([[ row [ \"latitude\" ], row [ \"longitude\" ]] for name , row in full . iterrows ()])) schools_heatmap . save ( \"generated_maps/heatmap.html\" ) schools_heatmap Out[33]: District level mapping We can investigate the SAT results at district level, by: Grouping full dataset by district Computing the average of each column for each school district Converting the 'school_dist' field to remvoe leading 0s, so we can match our geographic district data. In [34]: district_data = full . groupby ( \"school_dist\" ) . agg ( np . mean ) district_data . reset_index ( inplace = True ) district_data [ \"school_dist\" ] = district_data [ \"school_dist\" ] . apply ( lambda x : str ( int ( x ))) In [35]: def show_district_map ( col , df ): geo_path = 'data/school_districts.geojson' districts = folium . Map ( location = [ full [ 'latitude' ] . mean (), full [ 'longitude' ] . mean ()], zoom_start = 10 ) districts . choropleth ( geo_data = geo_path , data = df , columns = [ 'school_dist' , col ], key_on = 'feature.properties.school_dist' , fill_color = 'YlGn' , fill_opacity = 0.7 , line_opacity = 0.2 , ) # folium.GeoJson(geo_path, name = 'geojson').add_to(districts) districts . save ( \"generated_maps/ {} .html\" . format ( col )) return districts sat_districs = show_district_map ( \"sat_score\" , district_data ) from IPython.display import IFrame IFrame ( 'generated_maps/sat_score.html' , width = 1000 , height = 400 ) Out[35]: Now we can clearly seen on the distribution of SAT scores in the NYC. Dark green areas on the map indicate high average scores. The color close to yellow indicates low average SAT score areas. In [36]: # English language learners distribution ell_districs = show_district_map ( \"ell_percent\" , district_data ) IFrame ( 'generated_maps/ell_percent.html' , width = 1000 , height = 400 ) Out[36]:","tags":"Blog","url":"https://ericchen23.github.io/2018/08/SAT-analysis.html","loc":"https://ericchen23.github.io/2018/08/SAT-analysis.html"},{"title":"Hello World","text":"Hello World! This is my first post in my blog. My blog posts are mainly made using Jupyter Notebook and Pelican. I would like make this post a short introduction. My name is Hao Chen, who is desperate for getting into the field of Data Science. I'd like to use this blog as a place to share my experience and projects throughout the challenging but joyful process.","tags":"misc","url":"https://ericchen23.github.io/2018/08/hello-world.html","loc":"https://ericchen23.github.io/2018/08/hello-world.html"}]};