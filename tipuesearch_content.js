var tipuesearch = {"pages":[{"title":"Covid-19 Classifier: Classification on Lung CT Scans","text":"Covid-19 Classifier: Classification on Lung CT Scans In this post, we will build an Covid-19 image classifier on lung CT scan data. This is a Kaggle dataset, you can download the data using this link or use Kaggle API. This is the Part I of the Covid-19 Series. This dataset contains 20 cases of Covid-19. It has 4 folders and 1 metadata: ct_scans/ : ct scans data infection_mask/ : infection masks for the ct scans data lung_mask/ : lung masks for the ct scans lung_and_infection_mask/ : lung and infection masks for ct scans the metadata contains data file paths in the above four directories. First, we import necessary libraries and modify the paths in the metadata (because I am using Google Colab rather than Kaggle). In [1]: # imoprt libraries import pandas as pd import numpy as np import glob import matplotlib.pyplot as plt import tensorflow as tf import sys import random import warnings warnings . filterwarnings ( 'ignore' ) import cv2 import os from PIL import Image import PIL from sklearn.model_selection import train_test_split from sklearn.utils import class_weight import nibabel as nib from pylab import rcParams from tensorflow.keras.callbacks import Callback from tensorflow.keras import datasets , layers , models from tensorflow.keras.models import Model , load_model , Sequential from tensorflow.keras.layers import Input , BatchNormalization , Activation , Dense , Dropout , Flatten from tensorflow.keras.layers import Conv2D , MaxPooling2D , GlobalMaxPooling2D from tensorflow.keras.layers import concatenate , add from tensorflow.keras.callbacks import ModelCheckpoint from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array , load_img from tensorflow.keras import backend as K # Read metadata and modify image path raw_data = pd . read_csv ( './metadata.csv' ) raw_data = raw_data . replace ( '../input/covid19-ct-scans/' , './' , regex = True ) raw_data . shape Out[1]: (20, 4) Preprocessing Images CLAHE Enhance Used (CLAHE) Contrast Limited Adaptive Histogram Equalization to enhance the contrast of the images since medical images suffer a lot from the contrast problems. In [2]: def clahe_enhancer ( img , demo = False ): img = np . uint8 ( img * 255 ) clahe = cv2 . createCLAHE ( clipLimit = 3.0 , tileGridSize = ( 8 , 8 )) clahe_img = clahe . apply ( img ) if demo : img_flattened = img . flatten () clahe_img_flattened = clahe_img . flatten () fig = plt . figure () rcParams [ 'figure.figsize' ] = 10 , 10 plt . subplot ( 2 , 2 , 1 ) plt . imshow ( img , cmap = 'bone' ) plt . title ( \"Original CT-Scan\" ) plt . subplot ( 2 , 2 , 2 ) plt . hist ( img_flattened ) plt . title ( \"Histogram of Original CT-Scan\" ) plt . subplot ( 2 , 2 , 3 ) plt . imshow ( clahe_img , cmap = 'bone' ) plt . title ( \"CLAHE Enhanced CT-Scan\" ) plt . subplot ( 2 , 2 , 4 ) plt . hist ( clahe_img_flattened ) plt . title ( \"Histogram of CLAHE Enhanced CT-Scan\" ) return clahe_img Here is an example of CLAHE enhancement on an original image. From the above figure, we can find out another issue with the original image data. There is too much redundant information in the original images, i.e. the black space, the big porting of body part. We know that for Covid-19 diagnostics, we need to extract as much information as possible from the left and right lungs. So the Region of Interest in this case is the lung regions in the image. We need only the ROIs for Covid-19 detection. So we need properly crop the original image. Cropping Each CT scan in our dataset has its corresponding lungs mask. We can use the lungs mask to find out the ROI for cropping. So we can image for a possible complete Covid-19 diagonsis pipeline can be: First, semantic segmentation to get the lungs mask. Second, using the lungs mask to crop the ROIs. Then, send the ROIs to a classifier for Covid-19 diagnosis. We define a function to crop the lungs region in the lungs mask image. In [3]: def cropper ( test_img ): test_img = test_img * 255 test_img = np . uint8 ( test_img ) # ret, thresh = cv2.threshold(test_img, 50, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # ret, thresh = cv2.threshold(test_img, ret, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) contours , hierarchy = cv2 . findContours ( test_img , cv2 . RETR_TREE , cv2 . CHAIN_APPROX_SIMPLE ) areas = [ cv2 . contourArea ( c ) for c in contours ] x = np . argsort ( areas ) max_index = x [ x . size - 1 ] cnt1 = contours [ max_index ] second_max_index = x [ x . size - 2 ] cnt2 = contours [ second_max_index ] x , y , w , h = cv2 . boundingRect ( cnt1 ) p , q , r , s = cv2 . boundingRect ( cnt2 ) cropped1 = test_img [ y : y + h , x : x + w ] cropped1 = cv2 . resize ( cropped1 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) cropped2 = test_img [ q : q + s , p : p + r ] cropped2 = cv2 . resize ( cropped2 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) if x < p : fused = np . concatenate (( cropped1 , cropped2 ), axis = 1 ) else : fused = np . concatenate (( cropped2 , cropped1 ), axis = 1 ) # super_cropped = test_img[y+7:y+h-20, x+25:x+w-25] points_lung1 = [] points_lung2 = [] points_lung1 . append ( x ); points_lung1 . append ( y ); points_lung1 . append ( w ); points_lung1 . append ( h ) points_lung2 . append ( p ); points_lung2 . append ( q ); points_lung2 . append ( r ); points_lung2 . append ( s ) return ( fused , points_lung1 , points_lung2 ) Following is an example image after CLAHE enhance, cropping and resizing. Load and Prepare Data Define a function to read .nii files The dataset contains CT scans with masks of 20 cases of Covid-19. There are 20 .nii files in each folder of the dataset. Each .nii file contains around 180 slices (images). Total slices are 3520. These have been sliced out by 20% in the front and by 20% in the last of each file since in general these didn't had any infection masks and some didn't had the lungs, removed as noise. Also, images had pixel values in a very large range. We need to normalize the pixel values. In [4]: all_points1 = [] all_points2 = [] def read_nii ( filepath , data , string ): ''' Reads .nii file and returns pixel array ''' global all_points1 global all_points2 ct_scan = nib . load ( filepath ) array = ct_scan . get_fdata () array = np . rot90 ( np . array ( array )) slices = array . shape [ 2 ] array = array [:,:, round ( slices * 0.2 ): round ( slices * 0.8 )] array = np . reshape ( np . rollaxis ( array , 2 ),( array . shape [ 2 ], array . shape [ 0 ], array . shape [ 1 ], 1 )) if string == \"lungs\" : all_points1 = [] all_points2 = [] for img_no in range ( 0 , array . shape [ 0 ]): if string == 'lungs' and np . unique ( array [ img_no ]) . size == 1 : continue img = cv2 . resize ( array [ img_no ], dsize = ( img_size , img_size ), interpolation = cv2 . INTER_AREA ) # nomalize img xmax , xmin = img . max (), img . min () img = ( img - xmin ) / ( xmax - xmin ) if string == 'lungs' : # img = np.uint8(img*255) img [ img > 0 ] = 1 img , points1 , points2 = cropper ( img ) all_points1 . append (( points1 [ 0 ], points1 [ 1 ], points1 [ 2 ], points1 [ 3 ])) all_points2 . append (( points2 [ 0 ], points2 [ 1 ], points2 [ 2 ], points2 [ 3 ])) continue if string == \"cts\" and img_no < len ( all_points1 ): img = clahe_enhancer ( img ) a , b , c , d = all_points1 [ img_no ] e , f , g , h = all_points2 [ img_no ] img1 = img [ b : b + d , a : a + c ] img1 = cv2 . resize ( img1 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) img2 = img [ f : f + h , e : e + g ] img2 = cv2 . resize ( img2 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) if a < e : img = np . concatenate (( img1 , img2 ), axis = 1 ) else : img = np . concatenate (( img2 , img1 ), axis = 1 ) if string == \"infections\" and img_no < len ( all_points1 ): a , b , c , d = all_points1 [ img_no ] e , f , g , h = all_points2 [ img_no ] img = np . uint8 ( img * 255 ) img1 = img [ b : b + d , a : a + c ] img1 = cv2 . resize ( img1 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) img2 = img [ f : f + h , e : e + g ] img2 = cv2 . resize ( img2 , dsize = ( 125 , 250 ), interpolation = cv2 . INTER_AREA ) if a < e : img = np . concatenate (( img1 , img2 ), axis = 1 ) else : img = np . concatenate (( img2 , img1 ), axis = 1 ) img = cv2 . resize ( img , dsize = ( img_size , img_size ), interpolation = cv2 . INTER_LINEAR ) data . append ( img ) Load Image Data Start loading the data, we need to read in the lungs mask data first to get the ROIs for other images. We have 2112 CT scan images in total. We dropped first 20% and last 20% scans in each .nii file. In [5]: cts = [] lungs = [] infections = [] img_size = 224 for i in range ( 0 , raw_data . shape [ 0 ]): read_nii ( raw_data . loc [ i , 'lung_mask' ], lungs , 'lungs' ) read_nii ( raw_data . loc [ i , 'ct_scan' ], cts , 'cts' ) read_nii ( raw_data . loc [ i , 'infection_mask' ], infections , 'infections' ) print ( len ( cts ) , len ( infections )) 2112 2112 Load Target Label The labels of CT images are dependant on their infection masks. If an CT image has an empty infection mask (all black), then its label is \"0\", denoted as \"Normal\". On the contrary, if the infeciont mask for an CT image is not empty, then its label is \"1\", denoted as \"Infected\". As we can see, the target label in the dataset is not balanced. We need to take this into account when training the classifier. In [6]: # load target label y_label = [] for i in range ( 0 , len ( infections )): if len ( np . unique ( infections [ i ])) != 1 : y_label . append ( 1 ) else : y_label . append ( 0 ) print ( y_label . count ( 0 ), y_label . count ( 1 )) 497 1615 Prepare the Data We prepare the data in the following steps: Convert data to numpy array Split data into training and validation data (0.7:0.3) Set up data augmentation generator to diversity our data and avoid overfitting. In [7]: # convert to np array cts = np . array ( cts ) . astype ( 'uint8' ) cts = cts . reshape ( len ( cts ), img_size , img_size , 1 ) y_label = np . array ( y_label ) # split data x_train , x_valid , y_train , y_valid = train_test_split ( cts , y_label , test_size = 0.3 , random_state = 42 ) #data augmentation aug = ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True , fill_mode = \"nearest\" ) Build Model We build the neural network using 3 Conv Blocks, each block has two 3x3 convolutional layers with 16, 32 , 64 filters. The structure of the model can be viewed from here . We also set the batch_size, epochs and model checkpoint. In [8]: model = Sequential () model . add ( Conv2D ( 16 , ( 3 , 3 ), activation = 'relu' , padding = \"same\" , input_shape = ( img_size , img_size , 1 ))) model . add ( BatchNormalization ()) model . add ( Conv2D ( 16 , ( 3 , 3 ), padding = \"same\" , activation = 'relu' )) model . add ( BatchNormalization ()) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , padding = \"same\" )) model . add ( BatchNormalization ()) model . add ( Conv2D ( 32 , ( 3 , 3 ), padding = \"same\" , activation = 'relu' )) model . add ( BatchNormalization ()) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' , padding = \"same\" )) model . add ( BatchNormalization ()) model . add ( Conv2D ( 64 , ( 3 , 3 ), padding = \"same\" , activation = 'relu' )) model . add ( BatchNormalization ()) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) model . add ( Flatten ()) model . add ( Dense ( 64 , activation = 'relu' )) model . add ( Dropout ( 0.4 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) batch_size = 32 epochs = 50 best_val_auc = - 1 #model checkpoint filepath_acc = \"covid_weights_val_acc.hdf5\" checkpoint_acc = ModelCheckpoint ( filepath_acc , monitor = 'val_acc' , verbose = 1 , save_best_only = True , mode = 'max' ) model . compile ( loss = 'binary_crossentropy' , optimizer = Adam ( lr = 0.0003 ), metrics = [ \"acc\" ]) Training Model The dataset is not balanced. We need to take this into account during model training. We calculate the class weights and pass the weights into the model training procedure. In [ ]: # calculate class weights weights = class_weight . compute_class_weight ( 'balanced' , np . unique ( y_train ), y_train ) weights = dict ( enumerate ( weights )) # train model results = model . fit ( aug . flow ( x_train , y_train , batch_size = batch_size ), epochs = epochs , validation_data = ( x_valid , y_valid ) , steps_per_epoch = len ( x_train ) // batch_size , callbacks = [ checkpoint_acc ], class_weight = weights ) Epoch 48/50 46/46 [==============================] - ETA: 0s - loss: 0.2097 - acc: 0.9267 Epoch 00048: val_acc did not improve from 0.94322 46/46 [==============================] - 6s 137ms/step - loss: 0.2097 - acc: 0.9267 - val_loss: 0.1506 - val_acc: 0.9432 Epoch 49/50 46/46 [==============================] - ETA: 0s - loss: 0.1827 - acc: 0.9315 Epoch 00049: val_acc improved from 0.94322 to 0.94637, saving model to covid_weights_val_acc.hdf5 46/46 [==============================] - 6s 139ms/step - loss: 0.1827 - acc: 0.9315 - val_loss: 0.1450 - val_acc: 0.9464 Epoch 50/50 46/46 [==============================] - ETA: 0s - loss: 0.2033 - acc: 0.9232 Epoch 00050: val_acc did not improve from 0.94637 46/46 [==============================] - 6s 137ms/step - loss: 0.2033 - acc: 0.9232 - val_loss: 0.2401 - val_acc: 0.900 Model Performance We will use the validation data to evaluate the model performance. We load the checkpoint with best validation accuracy. We can see the test loss and accuracy from the output below. In [10]: model . load_weights ( \"covid_weights_val_acc.hdf5\" ) score = model . evaluate ( x_valid , y_valid , batch_size = 32 ) print ( \"test loss:\" , score [ 0 ], \" \\n test accuracy:\" , score [ 1 ]) 20/20 [==============================] - 0s 20ms/step - loss: 0.1450 - acc: 0.9464 test loss: 0.14500181376934052 test accuracy: 0.9463722109794617 Train and Validataion Loss In [11]: rcParams [ 'figure.figsize' ] = 10 , 7 plt . grid ( 'True' ) plt . plot ( results . history [ 'loss' ], color = 'm' ) plt . plot ( results . history [ 'val_loss' ], color = 'k' ) plt . title ( 'Loss' ) plt . ylabel ( 'loss' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) plt . show () Train and Vilidation Accuracy In [12]: rcParams [ 'figure.figsize' ] = 10 , 7 plt . grid ( 'True' ) plt . plot ( results . history [ 'acc' ], color = 'm' ) plt . plot ( results . history [ 'val_acc' ], color = 'k' ) plt . title ( 'Accuracy' ) plt . ylabel ( 'accuracy' ) plt . xlabel ( 'epoch' ) plt . legend ([ 'train' , 'test' ], loc = 'upper left' ) plt . show () Inference Next, we'll see a couple of inference examples on our test data. We randomly select 5 images from test data and check our model's inference on these images. In [26]: from google.colab.patches import cv2_imshow predictions = model . predict ( x_valid ) predictions = np . array ( predictions . flatten ()) def plot_inference (): rand = np . random . randint ( 0 , len ( x_valid ), size = 5 ) x_data = [] for i in rand : x_test = cv2 . cvtColor ( x_valid [ i ], cv2 . COLOR_GRAY2BGR ) round_prediction = np . round ( predictions [ i ]) if round_prediction == 1 : cv2 . putText ( x_test , \"Infected: \" + str ( round ( predictions [ i ], 3 )), ( 3 , 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 0 , 255 ), 2 ) else : cv2 . putText ( x_test , \"Normal: \" + str ( round ( 1 - predictions [ i ], 3 )), ( 3 , 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 2 ) x_data . append ( x_test ) x_data = np . concatenate ( x_data , axis = 1 ) cv2_imshow ( x_data ) plot_inference () In [33]: plot_inference () Summary In this post, we built a Covid-19 Classifer using CT scans data. We also learnt how to parse medical images in the format of .nii file. The performance of our Covid-19 Classifier works pretty well. It has a test accuracy slightly over 94%. This is the Part I of my Covid-19 series, stay tuned for PartII and maybe even Part III. The code and trained model weights are available in this GitHub Repo . Reference https://www.kaggle.com/andrewmvd/covid19-ct-scans https://github.com/deadskull7/One-Stop-for-COVID-19-Infection-and-Lung-Segmentation-plus-Classification/blob/master/Notebooks/task2_covid19_classifcation.ipynb","tags":"Computer Vision","url":"https://haochen23.github.io/2020/06/covid-19-classifier.html","loc":"https://haochen23.github.io/2020/06/covid-19-classifier.html"},{"title":"NLP with State-of-the-Art Language Models","text":"NLP with State-of-the-Art Language Models In this post, we'll see how to use state-of-the-art language models to perform downstream NLP tasks with Transformers. Transformers (previously known as pytorch-transformers) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL...) for Natural Laguage Processing. Transformers currently support 19 primary architectures with variations in model depth and size. These models are all based on the Transformer structure. Details about transformer can be found in this paper - Attention Is All You Need . There are thousands of pretained models including community models available in transformers. Leveraging State-of-the-Art Language Models on NLP tasks Install Transformers !pip install transformers Transformers Pipeline API Transformers' pipeline() method provides a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including: Sentence Classification (Sentiment Analysis) : Indicate if the overall sentence is either positive or negative, i.e. binary classification task or logitic regression task . Token Classification (Named Entity Recognition, Part-of-Speech tagging) : For each sub-entities ( tokens ) in the input, assign them a label, i.e. classification task. Question-Answering : Provided a tuple ( question , context ) the model should find the span of text in content answering the question . Mask-Filling : Suggests possible word(s) to fill the masked input with respect to the provided context . Summarization : Summarizes the input article to a shorter article. Translation : Translates the input from a language to another language. Feature Extraction : Maps the input to a higher, multi-dimensional space learned from the data. Pipelines encapsulate the overall process of every NLP process: Tokenization : Split the initial input into multiple sub-entities with ... properties (i.e. tokens). Inference : Maps every tokens into a more meaningful representation. Decoding : Use the above representation to generate and/or extract the final output for the underlying task. The pipeline() method can be used in three ways: Using the default model and tokenizer by only specifying the task name. from transformers import pipeline pipeline ( \"<task-name>\" ) Using user defined model by specifying task -name and model-name pipeline ( \"<task-name>\" , model = \"<model_name>\" ) Using user-defined model and tokenizer pipeline ( '<task-name>' , model = '<model name>' , tokenizer = '<tokenizer_name>' ) Usually the defualt models work pretty well on specific tasks, you can also use your own model by providing your model path to the model parameter. Note : Not all tasks are supported by every pretrained model in transformers.For example, Summarization task is only supported by bart and t5 models. You can go to this page to check which models support a specific task. In [1]: from transformers import pipeline Usage Examples I will only present you with the default models for each task, so you have an idea whether to use the default model and tokenizer or choose another one based on your requirements. Don't forget to check models supportability to the NLP tasks in this page 1. Sentence Classification - Sentiment Analysis The default model for Sentiment Analysis is DistilBERT uncased version - a smaller, faster version of BERT. In [2]: nlp_sentiment = pipeline ( 'sentiment-analysis' ) nlp_sentiment ( 'What a game for Kobe Bryant!' ) Out[2]: [{'label': 'POSITIVE', 'score': 0.9981902837753296}] 2. Named Entity Recognition The default model for Name Entity Recognition is Bert (bert-large-cased). In [3]: nlp_ner = pipeline ( 'ner' ) nlp_ner ( 'What a game for Kobe Bryant !' ) Out[3]: [{'entity': 'I-PER', 'index': 5, 'score': 0.9990301132202148, 'word': 'Kobe'}, {'entity': 'I-PER', 'index': 6, 'score': 0.9992470741271973, 'word': 'Bryant'}] 3. Question Answering The default model for Question Answering is DistilBERT and it's using bert-base-cased tokenizer. In [4]: nlp_qa = pipeline ( 'question-answering' ) nlp_qa ( context = 'Kobe Bryant was an American professional basketball player.' , question = 'Who is Kobe Bryant ?' ) Out[4]: {'answer': 'an American professional basketball player.', 'end': 58, 'score': 0.6634031543230492, 'start': 16} 4. Text Generation - Mask Prediction The default model for Text Generation is DistilRoBERTa. In [5]: nlp_fill_mask = pipeline ( 'fill-mask' ) nlp_fill_mask ( 'Kobe Bryant was an American professional basketball' + nlp_fill_mask . tokenizer . mask_token ) Out[5]: [{'score': 0.5135236978530884, 'sequence': '<s> Kobe Bryant was an American professional basketball player</s>', 'token': 869}, {'score': 0.13336943089962006, 'sequence': '<s> Kobe Bryant was an American professional basketball legend</s>', 'token': 7875}, {'score': 0.10051079839468002, 'sequence': '<s> Kobe Bryant was an American professional basketball coach</s>', 'token': 704}, {'score': 0.07933259010314941, 'sequence': '<s> Kobe Bryant was an American professional basketball star</s>', 'token': 999}, {'score': 0.05176172032952309, 'sequence': '<s> Kobe Bryant was an American professional basketball superstar</s>', 'token': 10896}] 5. Text Summarization As mentioned earlies, Summarization is currently supported by Bart and T5 . And the default model is bart-large-cnn . In [6]: TEXT_TO_SUMMARIZE = \"\"\" Kobe Bean Bryant was an American professional basketball player. As a shooting guard, Bryant entered the National Basketball Association (NBA) directly from high school, and played his entire 20-season professional career in the league with the Los Angeles Lakers. Bryant won many accolades: five NBA championships, 18-time All-Star, 15-time member of the All-NBA Team, 12-time member of the All-Defensive Team, 2008 NBA Most Valuable Player (MVP), two-time NBA Finals MVP winner. Widely regarded as one of the greatest players of all time, he led the NBA in scoring during two seasons, ranks fourth on the league's all-time regular season scoring and all-time postseason scoring lists. Bryant was the son of former NBA player Joe Bryant. He attended Lower Merion High School in Pennsylvania, where he was recognized as the top high-school basketball player in the country. Upon graduation, he declared for the 1996 NBA draft and was selected by the Charlotte Hornets with the 13th overall pick; the Hornets then traded him to the Lakers. As a rookie, Bryant earned himself a reputation as a high-flyer and a fan favorite by winning the 1997 Slam Dunk Contest, and he was named an All-Star by his second season. Despite a feud with teammate Shaquille O'Neal, the pair led the Lakers to three consecutive NBA championships from 2000 to 2002. In 2003, Bryant was accused of sexual assault by a 19-year-old hotel clerk. Criminal charges were brought and then dropped after the accuser refused to testify, with a civil suit later settled out of court. Bryant denied the assault charge, but admitted to a sexual encounter and issued a public apology. \"\"\" nlp_summarizer = pipeline ( 'summarization' ) nlp_summarizer ( TEXT_TO_SUMMARIZE , max_length = 30 ) Out[6]: [{'summary_text': 'Kobe Bean Bryant was an American professional basketball player. He played for the Los Angeles Lakers for his entire 20-season professional career.'}] 6. Translation Translation is currently supported by T5 for the language mappings English-to-French ( translation_en_to_fr ), English-to-German ( translation_en_to_de ) and English-to-Romanian ( translation_en_to_ro ). In [7]: # English to French translator = pipeline ( 'translation_en_to_fr' ) translator ( \"Kobe Bean Bryant was an American professional basketball player. As a shooting guard, Bryant entered the National Basketball Association (NBA) directly from high school, and played his entire 20-season professional career in the league with the Los Angeles Lakers. \" ) Out[7]: [{'translation_text': 'Kobe Bean Bryant est un joueur de basketball professionnel américain qui, en tant que gardien de tir, est entré dans la National Basketball Association (NBA) dès son école secondaire et a joué toute sa carrière professionnelle pendant 20 saisons avec les Los Angeles Lakers.'}] In [8]: # English to German translator = pipeline ( 'translation_en_to_de' ) translator ( \"Kobe Bean Bryant was an American professional basketball player. As a shooting guard, Bryant entered the National Basketball Association (NBA) directly from high school, and played his entire 20-season professional career in the league with the Los Angeles Lakers. \" ) Out[8]: [{'translation_text': 'Kobe Bean Bryant ist ein US-amerikanischer Basketballspieler.'}] 7. Text Generation Text generation is currently supported by GPT-2, OpenAi-GPT, TransfoXL, XLNet, CTRL and Reformer. And the default model is GPT-2. In [9]: text_generator = pipeline ( \"text-generation\" ) text_generator ( \"It's a sunny day. Let's \" ) Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence Out[9]: [{'generated_text': \"It's a sunny day. Let's go get some tea. I want to make these as quickly as possible because of your advice. You know how it is sometimes difficult to create a new relationship. Don't be afraid to talk and to talk.\"}] 8. Features Extraction Feature Extraction outputs' a 3D tensor (Samples, Tokens, Embeddings for each token). These embeddings can then be used as input features to other models, e.g. to a classifer for sentiment analysis. In [10]: import numpy as np nlp_features = pipeline ( 'feature-extraction' ) output = nlp_features ( 'Kobe Bryant was an American professional basketball player.' ) np . array ( output ) . shape Out[10]: (1, 11, 768) Summary In this post, we've seen how to use Transfomers pipeline API to perform various NLP downstream tasks. It's really a cool thing that we can leverage State-of-the-Art language models with only one or two lines of code. Transformers makes language models easy to use for everyone.","tags":"NLP","url":"https://haochen23.github.io/2020/06/transformers-pipeline.html","loc":"https://haochen23.github.io/2020/06/transformers-pipeline.html"},{"title":"Beagle Detector: Fine-tune Faster-RCNN","text":"Beagle Detector: Fine-tune Faster-RCNN In this post, we'll see how to create a Beagle Detector using Faster-RCNN. Faster-RCNN is the state-of-the-art object detection model in terms of detection accuracy. The beagle dataset we are using today is the same as the previous post . This dataset is originally created and prepared for instance segmentation tasks by meself. But it has all the necessary information in the annotations file for creating an object detector. So, we will stick to this dataset. The only thing that matters is how you should parse the annotations which I will show you soon. Let's get started. Understanding the Data The dataset I prepared contains a total number of 100 beagle images which I scraped from Google Image. 75 of them are used for training and 25 of them are used for validation. The dataset was annotated using VGG Image Annotator ( VIA ), and it labels all the images and exports it to a single JSON file. Because the dataset is originally created for instance segmentation task, the JSON file contains the whole mask information (polygon points) rather than bounding boxes as shown below. So need to parse the polygon points to obtain corresponding bounding boxes. Load the Data Use the following code to download the dataset. !git clone https://github.com/haochen23/fine-tune-MaskRcnn.git !cp -r fine-tune-MaskRcnn/beagle/ ./beagle/ The dataset is structured as follows: beagle/ train/ 00000000.jpg 00000002.jpg 00000005.jpg ... via_region_data.json val/ 00000168.jpg 00000170.jpg 00000176.jpg ... via_region_data.json The via_region_data.json contains the annotation data for train/ or val/ . Define a Dataset Class to Parse the Data All the mask polygon points data are store in the \"region\" field. So we can get bounding box information by picking up xmin, xmax, ymin, ymax as in the code. In [1]: import os import numpy as np import torch import torch.utils.data from PIL import Image import json class BeagleDataset ( torch . utils . data . Dataset ): def __init__ ( self , data_dir , transforms = None ): self . data_dir = data_dir self . transforms = transforms # load the annotations file, it also contain information of image names # load annotations annotations1 = json . load ( open ( os . path . join ( data_dir , \"via_region_data.json\" ))) self . annotations = list ( annotations1 . values ()) # don't need the dict keys def __getitem__ ( self , idx ): # get the image path from the annoations data img_name = self . annotations [ idx ][ \"filename\" ] img_path = os . path . join ( self . data_dir , img_name ) img = Image . open ( img_path ) . convert ( \"RGB\" ) # first id is the background, objects count from 1 obj_ids = np . array ( range ( len ( self . annotations [ idx ][ \"regions\" ]))) + 1 # get bounding box coordinates for each object num_objs = len ( obj_ids ) boxes = [] for i in range ( num_objs ): xmin = np . min ( self . annotations [ idx ][ \"regions\" ][ i ][ \"shape_attributes\" ][ \"all_points_x\" ]) xmax = np . max ( self . annotations [ idx ][ \"regions\" ][ i ][ \"shape_attributes\" ][ \"all_points_x\" ]) ymin = np . min ( self . annotations [ idx ][ \"regions\" ][ i ][ \"shape_attributes\" ][ \"all_points_y\" ]) ymax = np . max ( self . annotations [ idx ][ \"regions\" ][ i ][ \"shape_attributes\" ][ \"all_points_y\" ]) boxes . append ([ xmin , ymin , xmax , ymax ]) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) # there is only one class labels = torch . ones (( num_objs ,), dtype = torch . int64 ) image_id = torch . tensor ([ idx ]) area = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) # suppose all instances are not crowd iscrowd = torch . zeros (( num_objs ,), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"image_id\" ] = image_id target [ \"area\" ] = area target [ \"iscrowd\" ] = iscrowd if self . transforms is not None : img , target = self . transforms ( img , target ) return img , target def __len__ ( self ): return len ( self . annotations ) Define Model Architecture As we are fine-tuning Faster-RCNN, we need to replace its box_predictor with a new one, which only predict two classes - background and beagle. In [2]: import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def build_model ( num_classes ): # load an instance segmentation model pre-trained on COCO model = torchvision . models . detection . fasterrcnn_resnet50_fpn ( pretrained = True ) # get the number of input features for the classifier in_features = model . roi_heads . box_predictor . cls_score . in_features # replace the pre-trained head with a new one model . roi_heads . box_predictor = FastRCNNPredictor ( in_features , num_classes ) return model Training the Model The training process is the same as the previous post , so we'll make it short. We again will use some helper functions from PyTorch repo to train the model. I've put everthing together in one repo , including all the data, code and helper functions, feel free to use it. Load Data and Transform In [3]: from engine import train_one_epoch , evaluate import utils import transforms as T def get_transform ( train ): transforms = [] # converts the image, a PIL image, into a PyTorch Tensor transforms . append ( T . ToTensor ()) if train : # during training, randomly flip the training images # and ground-truth for data augmentation transforms . append ( T . RandomHorizontalFlip ( 0.5 )) return T . Compose ( transforms ) # use our dataset and defined transformations dataset = BeagleDataset ( 'beagle/train' , get_transform ( train = True )) dataset_test = BeagleDataset ( 'beagle/val' , get_transform ( train = False )) # define training and validation data loaders data_loader = torch . utils . data . DataLoader ( dataset , batch_size = 2 , shuffle = True , num_workers = 4 , collate_fn = utils . collate_fn ) data_loader_test = torch . utils . data . DataLoader ( dataset_test , batch_size = 1 , shuffle = False , num_workers = 4 , collate_fn = utils . collate_fn ) Initialize Model and Optimizer In [4]: device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) # our dataset has two classes only - background and beagle num_classes = 2 # get the model using our helper function model = build_model ( num_classes ) # move model to the right device model . to ( device ) # construct an optimizer params = [ p for p in model . parameters () if p . requires_grad ] optimizer = torch . optim . SGD ( params , lr = 0.005 , momentum = 0.9 , weight_decay = 0.0005 ) # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs lr_scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 3 , gamma = 0.1 ) Start Training Use the following code block to train the model, we train 10 epochs. This trainig process may take a while. In [5]: # number of epochs num_epochs = 10 for epoch in range ( num_epochs ): # train for one epoch, printing every 10 iterations train_one_epoch ( model , optimizer , data_loader , device , epoch , print_freq = 10 ) # update the learning rate lr_scheduler . step () # evaluate on the test dataset evaluate ( model , data_loader_test , device = device ) Inference Now our model is ready for making inference. We need to define a few util functions in order to visualize the results. The code below is well explained by the comments. In [ ]: # set to evaluation mode torch . save ( model , 'faster-rcnn-beagle.pt' ) model . eval () CLASS_NAMES = [ '__background__' , 'beagle' ] device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . to ( device ) In [7]: % matplotlib inline from PIL import Image import matplotlib.pyplot as plt import torch import torchvision.transforms as T import torchvision import numpy as np import cv2 import random import warnings warnings . filterwarnings ( 'ignore' ) def get_prediction ( img_path , confidence ): \"\"\" get_prediction parameters: - img_path - path of the input image - confidence - threshold value for prediction score method: - Image is obtained from the image path - the image is converted to image tensor using PyTorch's Transforms - image is passed through the model to get the predictions - class, box coordinates are obtained, but only prediction score > threshold are chosen. \"\"\" img = Image . open ( img_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) . to ( device ) pred = model ([ img ]) pred_class = [ CLASS_NAMES [ i ] for i in list ( pred [ 0 ][ 'labels' ] . cpu () . numpy ())] pred_boxes = [[( i [ 0 ], i [ 1 ]), ( i [ 2 ], i [ 3 ])] for i in list ( pred [ 0 ][ 'boxes' ] . detach () . cpu () . numpy ())] pred_score = list ( pred [ 0 ][ 'scores' ] . detach () . cpu () . numpy ()) pred_t = [ pred_score . index ( x ) for x in pred_score if x > confidence ][ - 1 ] pred_boxes = pred_boxes [: pred_t + 1 ] pred_class = pred_class [: pred_t + 1 ] pred_score = pred_score [: pred_t + 1 ] return pred_boxes , pred_class , pred_score def detect_object ( img_path , confidence = 0.5 , rect_th = 2 , text_size = 1 , text_th = 1 ): \"\"\" object_detection_api parameters: - img_path - path of the input image - confidence - threshold value for prediction score - rect_th - thickness of bounding box - text_size - size of the class label text - text_th - thichness of the text method: - prediction is obtained from get_prediction method - for each prediction, bounding box is drawn and text is written with opencv - the final image is displayed \"\"\" boxes , pred_cls , pred_score = get_prediction ( img_path , confidence ) img = cv2 . imread ( img_path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # print(len(boxes)) for i in range ( len ( boxes )): cv2 . rectangle ( img , boxes [ i ][ 0 ], boxes [ i ][ 1 ], color = ( 0 , 255 , 0 ), thickness = rect_th ) cv2 . putText ( img , pred_cls [ i ] + \": \" + str ( round ( pred_score [ i ], 3 )), boxes [ i ][ 0 ], cv2 . FONT_HERSHEY_SIMPLEX , text_size , ( 0 , 255 , 0 ), thickness = text_th ) plt . figure ( figsize = ( 20 , 30 )) plt . imshow ( img ) plt . xticks ([]) plt . yticks ([]) plt . show () Example 1 In [8]: ! wget -nv https://www.macmillandictionary.com/external/slideshow/full/beagle_full.jpg -O beagle.jpg detect_object ( 'beagle.jpg' , confidence = 0.7 ) 2020-06-25 04:15:49 URL:https://www.macmillandictionary.com/external/slideshow/full/beagle_full.jpg [131324/131324] -> \"beagle.jpg\" [1] Example 2 In [9]: ! wget -nv https://blackpugsite.com/wp-content/uploads/2019/08/pug-vs-beagle.jpg -O beagle2.jpg detect_object ( './beagle2.jpg' , confidence = 0.7 ) 2020-06-25 04:15:53 URL:https://blackpugsite.com/wp-content/uploads/2019/08/pug-vs-beagle.jpg [17838/17838] -> \"beagle2.jpg\" [1] Example 3 In [10]: ! wget -nv https://cdn.pixabay.com/photo/2018/12/15/16/01/beagle-3877115_960_720.jpg -O beagle3.jpg detect_object ( './beagle3.jpg' , confidence = 0.7 ) 2020-06-25 04:15:56 URL:https://cdn.pixabay.com/photo/2018/12/15/16/01/beagle-3877115_960_720.jpg [167591/167591] -> \"beagle3.jpg\" [1] From the above three examples, we see that our beagle detector is pretty much a dog detector. It can hardly tell the difference between beagle and other breeds. The main reason for this is that the training images in our training data contains only beagle images, it hardly learned feature from other dog breeds. Ways to Improve The most obvious way to improve our beagle detector is to add more training data, training data contains different dog breeds to an comparative amount to beagle images. In this way, the detector can learn features from other dogs and tell the difference between beagle and other dog breeds. Summary In this post, we've seen how to fine-tune an Faster-RCNN object detector on a custom dataset. And I've also shown you how to parse json annotated data. Although there are plenty of room to improve our model, the general workflow of fine-tuning Faster-RCNN detector is presented in details. The code in this post is available in this GitHub Repo .","tags":"Computer Vision","url":"https://haochen23.github.io/2020/06/fine-tune-faster-rcnn-pytorch.html","loc":"https://haochen23.github.io/2020/06/fine-tune-faster-rcnn-pytorch.html"},{"title":"Fine-tune PyTorch Pre-trained Mask-RCNN","text":"Fine-tuning Mask-RCNN using PyTorch In this post, I'll show you how fine-tune Mask-RCNN on a custom dataset. Fine-tune Mask-RCNN is very useful, you can use it to segment specific object and make cool applications. In a previous post , we've tried fine-tune Mask-RCNN using matterport's implementation. We've seen how to prepare a dataset using VGG Image Annotator (ViA) and how parse json annotations. This time, we are using PyTorch to train a custom Mask-RCNN. And we are using a different dataset which has mask images (.png files) as . So, we can practice our skills in dealing with different data types. Without any futher ado, let's get into it. We are using the Pedestrian Detection and Segmentation Dataset from Penn-Fudan Database. It contains 170 images with 345 instances of pedestrians. Customize the Dataset First, use the following command to download and unzip the dataset. !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip . !unzip PennFudanPed.zip The data is structured as follows PennFudanPed/ PedMasks/ FudanPed00001_mask.png FudanPed00002_mask.png FudanPed00003_mask.png FudanPed00004_mask.png ... PNGImages/ FudanPed00001.png FudanPed00002.png FudanPed00003.png FudanPed00004.png Let's look at one example from the dataset and it's corresponding segmentation mask. In [1]: from PIL import Image Image . open ( 'PennFudanPed/PNGImages/FudanPed00020.png' ) Out[1]: In [2]: mask = Image . open ( 'PennFudanPed/PedMasks/FudanPed00020_mask.png' ) # each mask instance has a different color, from zero to N, where # N is the number of instances. In order to make visualization easier, # let's adda color palette to the mask. mask . putpalette ([ 0 , 0 , 0 , # black background 255 , 0 , 0 , # index 1 is red 255 , 255 , 0 , # index 2 is yellow 255 , 153 , 0 , # index 3 is orange 200 , 200 , 200 , # index 4 ]) mask Out[2]: Define a Dataset Class to load the data The dataset class should inherit from the standard torch.utils.data.Dataset class, and implement __len__ and __getitem__ . The only specificity that we require is that the dataset __getitem__ should return: image: a PIL Image of size (H, W) target: a dict containing the following fields boxes ( FloatTensor[N, 4] ): the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H labels ( Int64Tensor[N] ): the label for each bounding box image_id ( Int64Tensor[1] ): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation area ( Tensor[N] ): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes. iscrowd ( UInt8Tensor[N] ): instances with iscrowd=True will be ignored during evaluation. (optionally) masks ( UInt8Tensor[N, H, W] ): The segmentation masks for each one of the objects (optionally) keypoints ( FloatTensor[N, K, 3] ): For each one of the N objects, it contains the K keypoints in [x, y, visibility] format, defining the object. visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt references/detection/transforms.py for your new keypoint representation If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools. In [3]: import os import numpy as np import torch import torch.utils.data from PIL import Image class PedestrianDataset ( torch . utils . data . Dataset ): def __init__ ( self , root , transforms = None ): self . root = root self . transforms = transforms # load all image files, sorting them to # ensure that they are aligned self . imgs = list ( sorted ( os . listdir ( os . path . join ( root , \"PNGImages\" )))) self . masks = list ( sorted ( os . listdir ( os . path . join ( root , \"PedMasks\" )))) def __getitem__ ( self , idx ): # load images ad masks img_path = os . path . join ( self . root , \"PNGImages\" , self . imgs [ idx ]) mask_path = os . path . join ( self . root , \"PedMasks\" , self . masks [ idx ]) img = Image . open ( img_path ) . convert ( \"RGB\" ) # note that we haven't converted the mask to RGB, # because each color corresponds to a different instance # with 0 being background mask = Image . open ( mask_path ) mask = np . array ( mask ) # instances are encoded as different colors obj_ids = np . unique ( mask ) # first id is the background, so remove it obj_ids = obj_ids [ 1 :] # split the color-encoded mask into a set # of binary masks masks = mask == obj_ids [:, None , None ] # get bounding box coordinates for each mask num_objs = len ( obj_ids ) boxes = [] for i in range ( num_objs ): pos = np . where ( masks [ i ]) xmin = np . min ( pos [ 1 ]) xmax = np . max ( pos [ 1 ]) ymin = np . min ( pos [ 0 ]) ymax = np . max ( pos [ 0 ]) boxes . append ([ xmin , ymin , xmax , ymax ]) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) # there is only one class labels = torch . ones (( num_objs ,), dtype = torch . int64 ) masks = torch . as_tensor ( masks , dtype = torch . uint8 ) image_id = torch . tensor ([ idx ]) area = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) # suppose all instances are not crowd iscrowd = torch . zeros (( num_objs ,), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"masks\" ] = masks target [ \"image_id\" ] = image_id target [ \"area\" ] = area target [ \"iscrowd\" ] = iscrowd if self . transforms is not None : img , target = self . transforms ( img , target ) return img , target def __len__ ( self ): return len ( self . imgs ) Define Model Architecture As we want to fine-tune Mask-RCNN, we need to modify its pre-trained head with a new one. For Mask-RCNN, because it has an object-detecor (box_predictor) and a mask_predictor. So, we need to modify both of them to adapt to our dataset. In [4]: import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def build_model ( num_classes ): # load an instance segmentation model pre-trained on COCO model = torchvision . models . detection . maskrcnn_resnet50_fpn ( pretrained = True ) # get the number of input features for the classifier in_features = model . roi_heads . box_predictor . cls_score . in_features # replace the pre-trained head with a new one model . roi_heads . box_predictor = FastRCNNPredictor ( in_features , num_classes ) # Stop here if you are fine-tunning Faster-RCNN # now get the number of input features for the mask classifier in_features_mask = model . roi_heads . mask_predictor . conv5_mask . in_channels hidden_layer = 256 # and replace the mask predictor with a new one model . roi_heads . mask_predictor = MaskRCNNPredictor ( in_features_mask , hidden_layer , num_classes ) return model Training the Model For training process, we will use some helper function from PyTorch Github Repo. They are located in references/detection/ we will use references/detection/engine.py , references/detection/utils.py and references/detection/transforms.py . !git clone https://github.com/pytorch/vision.git %cd vision !git checkout v0.3.0 !cp references/detection/utils.py ../ !cp references/detection/transforms.py ../ !cp references/detection/coco_eval.py ../ !cp references/detection/engine.py ../ !cp references/detection/coco_utils.py ../ Load Data and Transform In [5]: from engine import train_one_epoch , evaluate import utils import transforms as T def get_transform ( train ): transforms = [] # converts the image, a PIL image, into a PyTorch Tensor transforms . append ( T . ToTensor ()) if train : # during training, randomly flip the training images # and ground-truth for data augmentation transforms . append ( T . RandomHorizontalFlip ( 0.5 )) return T . Compose ( transforms ) # use our dataset and defined transformations dataset = PedestrianDataset ( 'PennFudanPed' , get_transform ( train = True )) dataset_test = PedestrianDataset ( 'PennFudanPed' , get_transform ( train = False )) # split the dataset in train and test set torch . manual_seed ( 1 ) indices = torch . randperm ( len ( dataset )) . tolist () dataset = torch . utils . data . Subset ( dataset , indices [: - 50 ]) dataset_test = torch . utils . data . Subset ( dataset_test , indices [ - 50 :]) # define training and validation data loaders data_loader = torch . utils . data . DataLoader ( dataset , batch_size = 2 , shuffle = True , num_workers = 4 , collate_fn = utils . collate_fn ) data_loader_test = torch . utils . data . DataLoader ( dataset_test , batch_size = 1 , shuffle = False , num_workers = 4 , collate_fn = utils . collate_fn ) Initialize Model and Optimizer In [6]: device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) # our dataset has two classes only - background and person num_classes = 2 # get the model using our helper function model = build_model ( num_classes ) # move model to the right device model . to ( device ) # construct an optimizer params = [ p for p in model . parameters () if p . requires_grad ] optimizer = torch . optim . SGD ( params , lr = 0.005 , momentum = 0.9 , weight_decay = 0.0005 ) # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs lr_scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 3 , gamma = 0.1 ) Start Training Use the following code block to train the model, we train 10 epochs. This trainig process may take a while. # number of epochs num_epochs = 10 for epoch in range ( num_epochs ): # train for one epoch, printing every 10 iterations train_one_epoch ( model , optimizer , data_loader , device , epoch , print_freq = 10 ) # update the learning rate lr_scheduler . step () # evaluate on the test dataset evaluate ( model , data_loader_test , device = device ) After training 10 epochs, we see the log below showing our model's performance on bounding box prediction and segmentation mask predition. IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.829 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.991 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.953 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.518 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.840 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.381 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.873 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.873 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.787 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.879 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.760 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.991 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.931 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.358 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.771 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.349 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.806 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.806 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.725 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812 Good. Now, you have a customized Mask-RCNN model, you can save it for future use. torch . save ( model , 'mask-rcnn-pedestrian.pt) Inference Now our model is ready for making inference. We need to define a few util functions in order to visualize the results. The code below is well explained by the comments. In [ ]: # set to evaluation mode model . eval () CLASS_NAMES = [ '__background__' , 'pedestrian' ] device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) model . to ( device ) In [8]: % matplotlib inline from PIL import Image import matplotlib.pyplot as plt import torch import torchvision.transforms as T import torchvision import numpy as np import cv2 import random import warnings warnings . filterwarnings ( 'ignore' ) def get_coloured_mask ( mask ): \"\"\" random_colour_masks parameters: - image - predicted masks method: - the masks of each predicted object is given random colour for visualization \"\"\" colours = [[ 0 , 255 , 0 ],[ 0 , 0 , 255 ],[ 255 , 0 , 0 ],[ 0 , 255 , 255 ],[ 255 , 255 , 0 ],[ 255 , 0 , 255 ],[ 80 , 70 , 180 ],[ 250 , 80 , 190 ],[ 245 , 145 , 50 ],[ 70 , 150 , 250 ],[ 50 , 190 , 190 ]] r = np . zeros_like ( mask ) . astype ( np . uint8 ) g = np . zeros_like ( mask ) . astype ( np . uint8 ) b = np . zeros_like ( mask ) . astype ( np . uint8 ) r [ mask == 1 ], g [ mask == 1 ], b [ mask == 1 ] = colours [ random . randrange ( 0 , 10 )] coloured_mask = np . stack ([ r , g , b ], axis = 2 ) return coloured_mask def get_prediction ( img_path , confidence ): \"\"\" get_prediction parameters: - img_path - path of the input image - confidence - threshold to keep the prediction or not method: - Image is obtained from the image path - the image is converted to image tensor using PyTorch's Transforms - image is passed through the model to get the predictions - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks ie: eg. segment of cat is made 1 and rest of the image is made 0 \"\"\" img = Image . open ( img_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) img = img . to ( device ) pred = model ([ img ]) pred_score = list ( pred [ 0 ][ 'scores' ] . detach () . cpu () . numpy ()) pred_t = [ pred_score . index ( x ) for x in pred_score if x > confidence ][ - 1 ] masks = ( pred [ 0 ][ 'masks' ] > 0.5 ) . squeeze () . detach () . cpu () . numpy () # print(pred[0]['labels'].numpy().max()) pred_class = [ CLASS_NAMES [ i ] for i in list ( pred [ 0 ][ 'labels' ] . cpu () . numpy ())] pred_boxes = [[( i [ 0 ], i [ 1 ]), ( i [ 2 ], i [ 3 ])] for i in list ( pred [ 0 ][ 'boxes' ] . detach () . cpu () . numpy ())] masks = masks [: pred_t + 1 ] pred_boxes = pred_boxes [: pred_t + 1 ] pred_class = pred_class [: pred_t + 1 ] return masks , pred_boxes , pred_class def segment_instance ( img_path , confidence = 0.5 , rect_th = 2 , text_size = 2 , text_th = 2 ): \"\"\" segment_instance parameters: - img_path - path to input image - confidence- confidence to keep the prediction or not - rect_th - rect thickness - text_size - text_th - text thickness method: - prediction is obtained by get_prediction - each mask is given random color - each mask is added to the image in the ration 1:0.8 with opencv - final output is displayed \"\"\" masks , boxes , pred_cls = get_prediction ( img_path , confidence ) img = cv2 . imread ( img_path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) for i in range ( len ( masks )): rgb_mask = get_coloured_mask ( masks [ i ]) img = cv2 . addWeighted ( img , 1 , rgb_mask , 0.5 , 0 ) cv2 . rectangle ( img , boxes [ i ][ 0 ], boxes [ i ][ 1 ], color = ( 0 , 255 , 0 ), thickness = rect_th ) cv2 . putText ( img , pred_cls [ i ], boxes [ i ][ 0 ], cv2 . FONT_HERSHEY_SIMPLEX , text_size , ( 0 , 255 , 0 ), thickness = text_th ) plt . figure ( figsize = ( 20 , 30 )) plt . imshow ( img ) plt . xticks ([]) plt . yticks ([]) plt . show () Now we are ready to go. Let's see a few examples from our model's inference. Example 1 In [9]: ! wget -nv https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg -O pedestrian.jpg segment_instance ( './pedestrian.jpg' , confidence = 0.7 ) 2020-06-24 08:24:26 URL:https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg [409534/409534] -> \"pedestrian.jpg\" [1] Example 2 In [10]: ! wget -nv https://p0.pikrepo.com/preview/356/253/woman-standing-under-umbrella-beside-pedestrian-lane-with-car-on-road-screenshot.jpg -O pedestrian2.jpg segment_instance ( './pedestrian2.jpg' , confidence = 0.7 ) 2020-06-24 08:24:36 URL:https://p0.pikrepo.com/preview/356/253/woman-standing-under-umbrella-beside-pedestrian-lane-with-car-on-road-screenshot.jpg [103583/103583] -> \"pedestrian2.jpg\" [1] Example 3 In [11]: ! wget -nv https://p0.pikrepo.com/preview/577/359/man-in-white-dress-shirt-and-brown-pants-walking-on-pedestrian-lane-during-daytime.jpg -O pedestrian3.jpg segment_instance ( './pedestrian3.jpg' , confidence = 0.7 ) 2020-06-24 08:24:45 URL:https://p0.pikrepo.com/preview/577/359/man-in-white-dress-shirt-and-brown-pants-walking-on-pedestrian-lane-during-daytime.jpg [103869/103869] -> \"pedestrian3.jpg\" [1] It looks like our customized model works pretty well. Summary In this post, we've how to fine-tune Mask-RCNN on a custom dataset using PyTorch pre-trained model. A customized Mask-RCNN can really make cool apps. I'll prepare a dataset for image segmentation in the future when I have time. So stay tuned. Reference: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html","tags":"Computer Vision","url":"https://haochen23.github.io/2020/06/fine-tune-mask-rcnn-pytorch.html","loc":"https://haochen23.github.io/2020/06/fine-tune-mask-rcnn-pytorch.html"},{"title":"Train Mask-RCNN on a Custom Dataset","text":"Fine-tune Mask-RCNN on a Custom Dataset In an earlier post , we've seen how to use a pretrained Mask-RCNN model using PyTorch. Although it is quite useful in some cases, we sometimes or our desired applications only needs to segment an specific class of object which may not exist in the COCO categories. Therefore, we need to train a customized Mask-RCNN model to meet out demand. In this post, We will see how to fune-tune Mask-RCNN on a custom dataset. I will cover the processing pipeline from how to prepare a custom dataset to model funtuning and evaluation. It will be very useful, so keep reading. I've prepared a very small Beagle dataset, and of course I've also put the annotated data in the dataset. Feel free to download it from this link . Step 1: Preparing the Dataset The dataset I prepared contains a total number of 100 beagle images which I scraped from Google Image. 75 of them are used for training and 25 of them are used for validation. I used VGG Image Annotator ( VIA ) to annotate the training and validation images. Its a simple tool and it labels all the images and exports it to a single JSON file. {:height=\"60%\" width=\"60%\"} Step 2: Install Dependencies Fisrt we need to downgrade tensorflow to 1.15.0 and keras to 2.2.5 in order to use Matterport's implementation of Mask-RCNN. I do this because I'm using Google Colab to do the experiment. ! pip install tensorflow - gpu == 1.15 . 0 ! pip install keras == 2.2 . 5 Then we clone matterport's implementation of Mask-RCNN and download the pretraind weights trained on COCO dataset. We are going to fine-tune the weights using our own dataset. ! git clone https : // github . com / matterport / Mask_RCNN . git % cd Mask_RCNN / ! python setup . py install ! wget https : // github . com / matterport / Mask_RCNN / releases / download / v2 . 0 / mask_rcnn_coco . h5 I've also cloned my prepared dataset to Google Colad. If you're not using Google Colab, you don't need to do that. This repo also contains the beagle.py which used for configure the model, load data, train and evaluate the model. I refereced this article . % cd .. ! git clone https : // github . com / haochen23 / fine - tune - MaskRcnn . git % cd fine - tune - MaskRcnn / Step 3: Modify beagle.py for Our Own Dataset Fisrt, modify the following 3 functions in beagle.py : def load_custom ( self , dataset_dir , subset ): def load_mask ( self , image_id ): def image_reference ( self , image_id ): Raplace 'beagle' with your custom class name in these functions. Second, modify class CustomConfig ( Config ): \"\"\"Configuration for training on the toy dataset. Derives from the base Config class and overrides some values. \"\"\" # Give the configuration a recognizable name NAME = \"beagle\" IMAGES_PER_GPU = 1 # Number of classes (including background) NUM_CLASSES = 1 + 1 # Background + beagle # Number of training steps per epoch STEPS_PER_EPOCH = 100 # Skip detections with < 90% confidence DETECTION_MIN_CONFIDENCE = 0.9 Step 4: Training Now we are ready to train the mode. If you don't have a GPU, you can also use Google Colab. I only trained the model for 10 epochs, you can modify the number of epochs in beagle.py . ! python3 beagle . py train -- dataset = beagle -- weights = coco Step 5: Inference using the Trained Model In [33]: % matplotlib inline import os import sys import random import math import re import time import numpy as np import tensorflow as tf import matplotlib import matplotlib.pyplot as plt import matplotlib.patches as patches ROOT_DIR = os . path . abspath ( \"../\" ) from mrcnn import utils from mrcnn import visualize from mrcnn.visualize import display_images import mrcnn.model as modellib from mrcnn.model import log import beagle MODEL_DIR = os . path . join ( ROOT_DIR , \"logs\" ) MODEL_WEIGHTS_PATH = ROOT_DIR + \"/beagle_mask_rcnn_coco.h5\" Setup configurations In [34]: config = beagle . CustomConfig () BEAGLE_DIR = ROOT_DIR + \"/fine-tune-MaskRcnn/beagle\" In [35]: # Override the training configurations with a few # changes for inferencing. class InferenceConfig ( config . __class__ ): # Run detection on one image at a time GPU_COUNT = 1 IMAGES_PER_GPU = 1 config = InferenceConfig () config . display () Configurations: BACKBONE resnet101 BACKBONE_STRIDES [4, 8, 16, 32, 64] BATCH_SIZE 1 BBOX_STD_DEV [0.1 0.1 0.2 0.2] COMPUTE_BACKBONE_SHAPE None DETECTION_MAX_INSTANCES 100 DETECTION_MIN_CONFIDENCE 0.9 DETECTION_NMS_THRESHOLD 0.3 FPN_CLASSIF_FC_LAYERS_SIZE 1024 GPU_COUNT 1 GRADIENT_CLIP_NORM 5.0 IMAGES_PER_GPU 1 IMAGE_CHANNEL_COUNT 3 IMAGE_MAX_DIM 1024 IMAGE_META_SIZE 14 IMAGE_MIN_DIM 800 IMAGE_MIN_SCALE 0 IMAGE_RESIZE_MODE square IMAGE_SHAPE [1024 1024 3] LEARNING_MOMENTUM 0.9 LEARNING_RATE 0.001 LOSS_WEIGHTS {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0} MASK_POOL_SIZE 14 MASK_SHAPE [28, 28] MAX_GT_INSTANCES 100 MEAN_PIXEL [123.7 116.8 103.9] MINI_MASK_SHAPE (56, 56) NAME beagle NUM_CLASSES 2 POOL_SIZE 7 POST_NMS_ROIS_INFERENCE 1000 POST_NMS_ROIS_TRAINING 2000 PRE_NMS_LIMIT 6000 ROI_POSITIVE_RATIO 0.33 RPN_ANCHOR_RATIOS [0.5, 1, 2] RPN_ANCHOR_SCALES (32, 64, 128, 256, 512) RPN_ANCHOR_STRIDE 1 RPN_BBOX_STD_DEV [0.1 0.1 0.2 0.2] RPN_NMS_THRESHOLD 0.7 RPN_TRAIN_ANCHORS_PER_IMAGE 256 STEPS_PER_EPOCH 100 TOP_DOWN_PYRAMID_SIZE 256 TRAIN_BN False TRAIN_ROIS_PER_IMAGE 200 USE_MINI_MASK True USE_RPN_ROIS True VALIDATION_STEPS 50 WEIGHT_DECAY 0.0001 In [36]: # set target device DEVICE = \"/gpu:0\" # /cpu:0 or /gpu:0 In [37]: def get_ax ( rows = 1 , cols = 1 , size = 16 ): \"\"\"Return a Matplotlib Axes array to be used in all visualizations in the notebook. Provide a central point to control graph sizes. Adjust the size attribute to control how big to render images \"\"\" _ , ax = plt . subplots ( rows , cols , figsize = ( size * cols , size * rows )) return ax Load validation set In [38]: dataset = beagle . CustomDataset () dataset . load_custom ( BEAGLE_DIR , \"val\" ) # Must call before using the dataset dataset . prepare () print ( \"Images: {} \\n Classes: {} \" . format ( len ( dataset . image_ids ), dataset . class_names )) Images: 25 Classes: ['BG', 'beagle'] Create model in inference mode and load our trained weights In [39]: # Create model in inference mode with tf . device ( DEVICE ): model = modellib . MaskRCNN ( mode = \"inference\" , model_dir = MODEL_DIR , config = config ) In [41]: weights_path = \"../logs/beagle20200618T0317/mask_rcnn_beagle_0010.h5\" # Load weights print ( \"Loading weights \" , weights_path ) model . load_weights ( weights_path , by_name = True ) Loading weights ../logs/beagle20200618T0317/mask_rcnn_beagle_0010.h5 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. Re-starting from epoch 10 Inference on test images In [44]: image_id = random . choice ( dataset . image_ids ) image , image_meta , gt_class_id , gt_bbox , gt_mask = \\ modellib . load_image_gt ( dataset , config , image_id , use_mini_mask = False ) info = dataset . image_info [ image_id ] print ( \"image ID: {} . {} ( {} ) {} \" . format ( info [ \"source\" ], info [ \"id\" ], image_id , dataset . image_reference ( image_id ))) # Run object detection results = model . detect ([ image ], verbose = 1 ) # Display results ax = get_ax ( 1 ) r = results [ 0 ] visualize . display_instances ( image , r [ 'rois' ], r [ 'masks' ], r [ 'class_ids' ], dataset . class_names , r [ 'scores' ], ax = ax , title = \"Predictions\" ) log ( \"gt_class_id\" , gt_class_id ) log ( \"gt_bbox\" , gt_bbox ) log ( \"gt_mask\" , gt_mask ) image ID: beagle.00000228.jpg (20) /content/fine-tune-MaskRcnn/beagle/val/00000228.jpg Processing 1 images image shape: (1024, 1024, 3) min: 0.00000 max: 255.00000 uint8 molded_images shape: (1, 1024, 1024, 3) min: -123.70000 max: 151.10000 float64 image_metas shape: (1, 14) min: 0.00000 max: 1024.00000 int64 anchors shape: (1, 261888, 4) min: -0.35390 max: 1.29134 float32 gt_class_id shape: (1,) min: 1.00000 max: 1.00000 int32 gt_bbox shape: (1, 4) min: 135.00000 max: 900.00000 int32 gt_mask shape: (1024, 1024, 1) min: 0.00000 max: 1.00000 bool In [52]: image_id = random . choice ( dataset . image_ids ) image , image_meta , gt_class_id , gt_bbox , gt_mask = \\ modellib . load_image_gt ( dataset , config , image_id , use_mini_mask = False ) info = dataset . image_info [ image_id ] print ( \"image ID: {} . {} ( {} ) {} \" . format ( info [ \"source\" ], info [ \"id\" ], image_id , dataset . image_reference ( image_id ))) # Run object detection results = model . detect ([ image ], verbose = 1 ) # Display results ax = get_ax ( 1 ) r = results [ 0 ] visualize . display_instances ( image , r [ 'rois' ], r [ 'masks' ], r [ 'class_ids' ], dataset . class_names , r [ 'scores' ], ax = ax , title = \"Predictions\" ) log ( \"gt_class_id\" , gt_class_id ) log ( \"gt_bbox\" , gt_bbox ) log ( \"gt_mask\" , gt_mask ) image ID: beagle.00000248.jpg (24) /content/fine-tune-MaskRcnn/beagle/val/00000248.jpg Processing 1 images image shape: (1024, 1024, 3) min: 0.00000 max: 254.00000 uint8 molded_images shape: (1, 1024, 1024, 3) min: -123.70000 max: 150.10000 float64 image_metas shape: (1, 14) min: 0.00000 max: 1024.00000 int64 anchors shape: (1, 261888, 4) min: -0.35390 max: 1.29134 float32 gt_class_id shape: (1,) min: 1.00000 max: 1.00000 int32 gt_bbox shape: (1, 4) min: 335.00000 max: 824.00000 int32 gt_mask shape: (1024, 1024, 1) min: 0.00000 max: 1.00000 bool Summary In this post, we've how to fine-tune a custom Mask-RCNN model on my prepared Beagle dataset. I've walked you through the entire training process, from preparing the dataset to how to perform inference using your own model. I hope you guys find this post useful. The code and dataset used in this post are availbe in my GitHub Repo .","tags":"Computer Vision","url":"https://haochen23.github.io/2020/06/fine-tune-mask-rcnn.html","loc":"https://haochen23.github.io/2020/06/fine-tune-mask-rcnn.html"},{"title":"Foetal Head Segmentation on Ultrasound Images using Residual U-Net","text":"Foetal Head Segmentation on Ultrasound Images using Residual U-Net Measuring foetal head circumference is one of the most accurate method to estimate gestational age after the first trimester (first three months of pregnancy). To automate this procedure, the most important step is segment the foetal head area in ultrasound images. Then we can fit an ellipse on the head area, and build a regressor using the head mask to estimate gestational age. In this post, we will use a U-Net architecture with residual blocks to make head segmentations. U-Net performs very well on medical images. And due to its relatively compact structure, its speed is much faster than Mask-RCNN. U-Net Structure The U-Net architecture is shown below. The provided model is basically a convolutional auto-encoder, but with a twist - it has skip connections from encoder layers to decoder layers that are on the same \"level\". In this post, we will use residual blocks to replace the simple convolutional blocks to increase the complexity of the model. Note: the number of filters we use is different from the above picture. Find details in the model building section. We are using a custom dataset with around 1000 training images and training masks, and 300 test ultrasound images. In [1]: # import necessary libraries import os import re import numpy as np import pandas as pd import matplotlib.pyplot as plt import cv2 from PIL import Image from skimage.transform import resize from sklearn.model_selection import train_test_split import tensorflow.keras import tensorflow as tf from tensorflow.keras import backend as K K . set_image_data_format ( 'channels_last' ) Understanding the Dataset The foetal head ultrasound image dataset has two directories, train and test . The train folder contains the 999 training images and their corresponding masks. The test folder contains 335 test images. The sturcture are as follows: foetal-head-us/ train/ 000_HC.png 000_HC_Mask.png 001_HC.png 001_HC_Mask.png ... test/ 000_HC.png 001_HC.png ... The training masks are named as training image name + '_Mask.png\" Load the Data Set File Path In [2]: # training data path path_train = \"foetal-head-us/train/\" file_list_train = sorted ( os . listdir ( path_train )) #test data path path_test = \"foetal-head-us/test/\" file_list_test = sorted ( os . listdir ( path_test )) Separate Training Images and Masks In [3]: train_image = [] train_mask = [] for idx , item in enumerate ( file_list_train ): if idx % 2 == 0 : train_image . append ( item ) else : train_mask . append ( item ) print ( \"Number of US training images is {} \" . format ( len ( train_image ))) print ( \"Number of US training masks is {} \" . format ( len ( train_mask ))) print ( train_image [: 5 ], \" \\n \" , train_mask [: 5 ]) Number of US training images is 999 Number of US training masks is 999 ['000_HC.png', '001_HC.png', '002_HC.png', '003_HC.png', '004_HC.png'] ['000_HC_Mask.png', '001_HC_Mask.png', '002_HC_Mask.png', '003_HC_Mask.png', '004_HC_Mask.png'] Visualize an Example of Training Data Following is an example from the training data. The image on the left is the ultrasound image, the image in the middle is its mask, image on the right shows an overlapping of the training mask and training image. In [4]: # Display the first image and mask of the first subject. image1 = np . array ( Image . open ( path_train + \"009_HC.png\" )) image1_mask = np . array ( Image . open ( path_train + \"009_HC_Mask.png\" )) # binary inverse the mask image1_mask = np . ma . masked_where ( image1_mask == 0 , image1_mask ) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 16 , 12 )) ax [ 0 ] . imshow ( image1 , cmap = 'gray' ) ax [ 1 ] . imshow ( image1_mask , cmap = 'gray' ) ax [ 2 ] . imshow ( image1 , cmap = 'gray' , interpolation = 'none' ) ax [ 2 ] . imshow ( image1_mask , cmap = 'jet' , interpolation = 'none' , alpha = 0.7 ) Out[4]: <matplotlib.image.AxesImage at 0x7f13e90489e8> Read Data Store the training images in X , and training masks in y . Note the images in this dataset are not of the same shape. In [5]: ## Storing data X = [] y = [] for image , mask in zip ( train_image , train_mask ): X . append ( np . array ( Image . open ( path_train + image ))) y . append ( np . array ( Image . open ( path_train + mask ))) X = np . array ( X ) y = np . array ( y ) print ( \"X_shape : \" , X . shape ) print ( \"y_shape : \" , y . shape ) X_shape : (999,) y_shape : (999,) Model building and training Since the whole data size is quite big, it may lead to over-memory if we load whole data on X and y as we did earier. So we use data generator that allow us to load a few of data and to use them to train our model. Before that, we first use only 100 data to check our model works well In [6]: from tensorflow.keras.models import Model , load_model from tensorflow.keras.layers import Input , BatchNormalization , Activation , Add from tensorflow.keras.layers import Dropout , Lambda from tensorflow.keras.layers import Conv2D , Conv2DTranspose from tensorflow.keras.layers import MaxPooling2D from tensorflow.keras.layers import concatenate from tensorflow.keras.optimizers import Adam from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint # set target image size IMG_HEIGHT = 224 IMG_WIDTH = 224 Define Metrics We first define metrics for training our model. We use the dice coefficient and dice loss. Dice coeffient is the ratio between 2 * intersection (of true mask and predicted mask) and the sum of true and predicted masks. I refered to this site . In [7]: smooth = 1. # define loss function and metrics def dice_coef ( y_true , y_pred ): y_true_f = K . flatten ( y_true ) y_pred_f = K . flatten ( y_pred ) intersection = K . sum ( y_true_f * y_pred_f ) return ( 2. * intersection + smooth ) / ( K . sum ( y_true_f ) + K . sum ( y_pred_f ) + smooth ) def dice_coef_loss ( y_true , y_pred ): return - dice_coef ( y_true , y_pred ) In [8]: # define building blocks def BatchActivate ( x ): x = BatchNormalization ()( x ) x = Activation ( 'relu' )( x ) return x def convolution_block ( x , filters , size , strides = ( 1 , 1 ), padding = \"same\" , activation = True ): x = Conv2D ( filters , size , strides = strides , padding = padding )( x ) if activation == True : x = BatchActivate ( x ) return x def residual_block ( blockInput , num_filters = 16 , batch_activate = False ): x = BatchActivate ( blockInput ) x = convolution_block ( x , num_filters , ( 3 , 3 )) x = convolution_block ( x , num_filters , ( 3 , 3 ), activation = False ) x = Add ()([ x , blockInput ]) if batch_activate : x = BatchActivate ( x ) return x In [9]: # Build U-Net model def build_model ( input_layer , start_neurons , DropoutRatio = 0.5 ): conv1 = Conv2D ( start_neurons * 1 , ( 3 , 3 ), activation = None , padding = \"same\" )( input_layer ) conv1 = residual_block ( conv1 , start_neurons * 1 ) conv1 = residual_block ( conv1 , start_neurons * 1 , True ) pool1 = MaxPooling2D (( 2 , 2 ))( conv1 ) pool1 = Dropout ( DropoutRatio / 2 )( pool1 ) conv2 = Conv2D ( start_neurons * 2 , ( 3 , 3 ), activation = None , padding = \"same\" )( pool1 ) conv2 = residual_block ( conv2 , start_neurons * 2 ) conv2 = residual_block ( conv2 , start_neurons * 2 , True ) pool2 = MaxPooling2D (( 2 , 2 ))( conv2 ) pool2 = Dropout ( DropoutRatio )( pool2 ) conv3 = Conv2D ( start_neurons * 4 , ( 3 , 3 ), activation = None , padding = \"same\" )( pool2 ) conv3 = residual_block ( conv3 , start_neurons * 4 ) conv3 = residual_block ( conv3 , start_neurons * 4 , True ) pool3 = MaxPooling2D (( 2 , 2 ))( conv3 ) pool3 = Dropout ( DropoutRatio )( pool3 ) conv4 = Conv2D ( start_neurons * 8 , ( 3 , 3 ), activation = None , padding = \"same\" )( pool3 ) conv4 = residual_block ( conv4 , start_neurons * 8 ) conv4 = residual_block ( conv4 , start_neurons * 8 , True ) pool4 = MaxPooling2D (( 2 , 2 ))( conv4 ) pool4 = Dropout ( DropoutRatio )( pool4 ) convm = Conv2D ( start_neurons * 16 , ( 3 , 3 ), activation = None , padding = \"same\" )( pool4 ) convm = residual_block ( convm , start_neurons * 16 ) convm = residual_block ( convm , start_neurons * 16 , True ) deconv4 = Conv2DTranspose ( start_neurons * 8 , ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" )( convm ) uconv4 = concatenate ([ deconv4 , conv4 ]) uconv4 = Dropout ( DropoutRatio )( uconv4 ) uconv4 = Conv2D ( start_neurons * 8 , ( 3 , 3 ), activation = None , padding = \"same\" )( uconv4 ) uconv4 = residual_block ( uconv4 , start_neurons * 8 ) uconv4 = residual_block ( uconv4 , start_neurons * 8 , True ) deconv3 = Conv2DTranspose ( start_neurons * 4 , ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" )( uconv4 ) uconv3 = concatenate ([ deconv3 , conv3 ]) uconv3 = Dropout ( DropoutRatio )( uconv3 ) uconv3 = Conv2D ( start_neurons * 4 , ( 3 , 3 ), activation = None , padding = \"same\" )( uconv3 ) uconv3 = residual_block ( uconv3 , start_neurons * 4 ) uconv3 = residual_block ( uconv3 , start_neurons * 4 , True ) deconv2 = Conv2DTranspose ( start_neurons * 2 , ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" )( uconv3 ) uconv2 = concatenate ([ deconv2 , conv2 ]) uconv2 = Conv2D ( start_neurons * 2 , ( 3 , 3 ), activation = None , padding = \"same\" )( uconv2 ) uconv2 = residual_block ( uconv2 , start_neurons * 2 ) uconv2 = residual_block ( uconv2 , start_neurons * 2 , True ) deconv1 = Conv2DTranspose ( start_neurons * 1 , ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" )( uconv2 ) uconv1 = concatenate ([ deconv1 , conv1 ]) uconv1 = Conv2D ( start_neurons * 1 , ( 3 , 3 ), activation = None , padding = \"same\" )( uconv1 ) uconv1 = residual_block ( uconv1 , start_neurons * 1 ) uconv1 = residual_block ( uconv1 , start_neurons * 1 , True ) output_layer = Conv2D ( 1 , ( 1 , 1 ), padding = \"same\" , activation = \"sigmoid\" )( uconv1 ) return output_layer The full Res-U-Net model with start_neurons set to 16 can be viewed in this link . Training on the entire dataset Define the image_generator We define a generator to read and preprocess images and masks. The preprocessing includes: convert images to grayscale, resize image to 224x224, normalize pixel values to the range [0, 1]. In [10]: def Generator ( X_list , y_list , batch_size = 16 ): c = 0 while ( True ): X = np . empty (( batch_size , IMG_HEIGHT , IMG_WIDTH ), dtype = 'float32' ) y = np . empty (( batch_size , IMG_HEIGHT , IMG_WIDTH ), dtype = 'float32' ) for i in range ( c , c + batch_size ): image = X_list [ i ] image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2GRAY ) mask = y_list [ i ] mask = cv2 . cvtColor ( mask , cv2 . COLOR_BGR2GRAY ) image = cv2 . resize ( image , ( IMG_HEIGHT , IMG_WIDTH ), interpolation = cv2 . INTER_AREA ) mask = cv2 . resize ( mask , ( IMG_HEIGHT , IMG_WIDTH ), interpolation = cv2 . INTER_AREA ) X [ i - c ] = image y [ i - c ] = mask X = X [:,:,:, np . newaxis ] / 255 y = y [:,:,:, np . newaxis ] / 255 c += batch_size if ( c + batch_size >= len ( X_list )): c = 0 yield X , y Data augementation we simply perform horizontal flip, vertical flip, horizontal-vertcial flip, and vertical-horizontal flip to increase the training data. After data augmentation, we obtained about 5000 training images and masks. In [11]: train_img_aug = [] train_mask_aug = [] def augmentation ( imgs , masks ): for img , mask in zip ( imgs , masks ): img = cv2 . imread ( \"foetal-head-us/train/\" + img ) mask = cv2 . imread ( \"foetal-head-us/train/\" + mask ) train_img_aug . append ( img ) train_mask_aug . append ( mask ) img_lr = np . fliplr ( img ) mask_lr = np . fliplr ( mask ) img_up = np . flipud ( img ) mask_up = np . flipud ( mask ) img_lr_up = np . flipud ( img_lr ) mask_lr_up = np . flipud ( mask_lr ) img_up_lr = np . fliplr ( img_up ) mask_up_lr = np . fliplr ( mask_up ) train_img_aug . append ( img_lr ) train_mask_aug . append ( mask_lr ) train_img_aug . append ( img_up ) train_mask_aug . append ( mask_up ) train_img_aug . append ( img_lr_up ) train_mask_aug . append ( mask_lr_up ) train_img_aug . append ( img_up_lr ) train_mask_aug . append ( mask_up_lr ) In [12]: augmentation ( train_image , train_mask ) print ( len ( train_image )) print ( len ( train_img_aug )) 999 4995 Start Training We split the training data into training set and validation set (0.7: 0.3). We set the traning parameters and define a callback function to save the model with best val_dice_coef. In [13]: #split training data X_train , X_val , y_train , y_val = train_test_split ( train_img_aug , train_mask_aug , test_size = 0.3 , random_state = 1 ) # set training parameters epochs = 50 batch_size = 16 steps_per_epoch = int ( len ( X_train ) / batch_size ) validation_steps = int ( len ( X_val ) / batch_size ) train_gen = Generator ( X_train , y_train , batch_size = batch_size ) val_gen = Generator ( X_val , y_val , batch_size = batch_size ) # initialize our model inputs = Input (( IMG_HEIGHT , IMG_WIDTH , 1 )) output_layer = build_model ( inputs , 16 , 0.5 ) # Define callbacks to save model with best val_dice_coef checkpointer = ModelCheckpoint ( filepath = 'best_model_224_res.h5' , monitor = 'val_dice_coef' , verbose = 1 , save_best_only = True , mode = 'max' ) model = Model ( inputs = [ inputs ], outputs = [ output_layer ]) model . compile ( optimizer = Adam ( lr = 3e-5 ), loss = dice_coef_loss , metrics = [ dice_coef ]) In [14]: results = model . fit_generator ( train_gen , steps_per_epoch = steps_per_epoch , epochs = epochs , validation_data = val_gen , validation_steps = validation_steps , callbacks = [ checkpointer ]) Epoch 48/50 249/249 [==============================] - 61s 246ms/step - loss: -0.9918 - dice_coef: 0.9918 - val_loss: -0.9743 - val_dice_coef: 0.9743 Epoch 00048: val_dice_coef did not improve from 0.97469 Epoch 49/50 249/249 [==============================] - 61s 246ms/step - loss: -0.9918 - dice_coef: 0.9918 - val_loss: -0.9744 - val_dice_coef: 0.9744 Epoch 00049: val_dice_coef did not improve from 0.97469 Epoch 50/50 249/249 [==============================] - 63s 253ms/step - loss: -0.9917 - dice_coef: 0.9917 - val_loss: -0.9747 - val_dice_coef: 0.9747 Model Performance We evaluate the model performance by looking at the training and validation dice coefficient of the training process. In [16]: fig , loss_ax = plt . subplots () acc_ax = loss_ax . twinx () loss_ax . plot ( results . history [ 'loss' ], 'y' , label = 'train loss' ) loss_ax . plot ( results . history [ 'val_loss' ], 'r' , label = 'val loss' ) acc_ax . plot ( results . history [ 'dice_coef' ], 'b' , label = 'train dice coef' ) acc_ax . plot ( results . history [ 'val_dice_coef' ], 'g' , label = 'val dice coef' ) loss_ax . set_xlabel ( 'epoch' ) loss_ax . set_ylabel ( 'loss' ) acc_ax . set_ylabel ( 'accuray' ) loss_ax . legend ( loc = 'upper left' ) acc_ax . legend ( loc = 'lower left' ) plt . show () Make predictions on the test data In [17]: test_list = os . listdir ( \"foetal-head-us/test\" ) print ( \"The number of test data : \" , len ( test_list )) test_list [: 5 ] The number of test data : 335 Out[17]: ['191_HC.png', '010_HC.png', '287_HC.png', '166_HC.png', '044_HC.png'] In [18]: X_test = np . empty (( len ( test_list ), IMG_HEIGHT , IMG_WIDTH ), dtype = 'float32' ) for i , item in enumerate ( test_list ): image = cv2 . imread ( \"foetal-head-us/test/\" + item , 0 ) image = cv2 . resize ( image , ( IMG_HEIGHT , IMG_WIDTH ), interpolation = cv2 . INTER_AREA ) X_test [ i ] = image X_test = X_test [:,:,:, np . newaxis ] / 255 y_pred = model . predict ( X_test ) Visualize test results I select 10 out of 335 test images to display. In [19]: for test , pred in zip ( X_test [ 180 : 190 ], y_pred [ 180 : 190 ]): fig , ax = plt . subplots ( 1 , 3 , figsize = ( 16 , 12 )) test = test . reshape (( IMG_HEIGHT , IMG_WIDTH )) pred = pred . reshape (( IMG_HEIGHT , IMG_WIDTH )) pred = pred > 0.5 pred = np . ma . masked_where ( pred == 0 , pred ) ax [ 0 ] . imshow ( test , cmap = 'gray' ) ax [ 1 ] . imshow ( pred , cmap = 'gray' ) ax [ 2 ] . imshow ( test , cmap = 'gray' , interpolation = 'none' ) ax [ 2 ] . imshow ( pred , cmap = 'jet' , interpolation = 'none' , alpha = 0.7 ) From the above resutls, we can see our trained model have pretty good performance on the test images. The predicted masks are good representations of the head areas in the original ultrasound images. Summary In this post, we've built a U-Net with Residual blocks to predict feotal head masks in ultrasound images. U-Net in general, has a good performance on medical images. It is good for semantic segmentation, although it can do instance segmentation as well. For instance segmentation, Mask-RCNN would be a better choice. I will write posts about how to use pretrained, and fine-tune Mask-RCNN recently. If you're interested, stay tuned. The code in this post is in this GitHub Repo .","tags":"Computer Vision","url":"https://haochen23.github.io/2020/05/unet-segmentation.html","loc":"https://haochen23.github.io/2020/05/unet-segmentation.html"},{"title":"Instance Segmentation using Mask-RCNN and PyTorch","text":"Instance Segmentation using Mask-RCNN and PyTorch Instance Segmentation is a combination of 2 problems Object Detection Semantic Segmentation In this post, we will explore Mask-RCNN object detector with Pytorch. We will use the pretrained Mask-RCNN model with Resnet50 as the backbone. Understanding model inputs and outputs: The pretrained Faster-RCNN ResNet-50 model we are going to use expects the input image tensor to be in the form [n, c, h, w] where n is the number of images c is the number of channels , for RGB images its 3 h is the height of the image w is the widht of the image The model will return boxes (Tensor[N, 4]): the predicted boxes in [x0, y0, x1, y1] format, with values between 0 and H and 0 and W labels (Tensor[N]): the predicted labels for each image scores (Tensor[N]): the scores or each prediction masks (Tensor[N, H, W]): the predicted masks for each instance, in 0-1 range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (mask >= 0.5)labels Load model Now, we are loading the pretrained Mask-RCNN Resnet50 model, and also loading the COCO dataset category names. In [0]: # import necessary libraries % matplotlib inline from PIL import Image import matplotlib.pyplot as plt import torch import torchvision.transforms as T import torchvision import numpy as np import cv2 import random import warnings warnings . filterwarnings ( 'ignore' ) In [2]: # load model model = torchvision . models . detection . maskrcnn_resnet50_fpn ( pretrained = True ) # set to evaluation mode model . eval () # load COCO category names COCO_CLASS_NAMES = [ '__background__' , 'person' , 'bicycle' , 'car' , 'motorcycle' , 'airplane' , 'bus' , 'train' , 'truck' , 'boat' , 'traffic light' , 'fire hydrant' , 'N/A' , 'stop sign' , 'parking meter' , 'bench' , 'bird' , 'cat' , 'dog' , 'horse' , 'sheep' , 'cow' , 'elephant' , 'bear' , 'zebra' , 'giraffe' , 'N/A' , 'backpack' , 'umbrella' , 'N/A' , 'N/A' , 'handbag' , 'tie' , 'suitcase' , 'frisbee' , 'skis' , 'snowboard' , 'sports ball' , 'kite' , 'baseball bat' , 'baseball glove' , 'skateboard' , 'surfboard' , 'tennis racket' , 'bottle' , 'N/A' , 'wine glass' , 'cup' , 'fork' , 'knife' , 'spoon' , 'bowl' , 'banana' , 'apple' , 'sandwich' , 'orange' , 'broccoli' , 'carrot' , 'hot dog' , 'pizza' , 'donut' , 'cake' , 'chair' , 'couch' , 'potted plant' , 'bed' , 'N/A' , 'dining table' , 'N/A' , 'N/A' , 'toilet' , 'N/A' , 'tv' , 'laptop' , 'mouse' , 'remote' , 'keyboard' , 'cell phone' , 'microwave' , 'oven' , 'toaster' , 'sink' , 'refrigerator' , 'N/A' , 'book' , 'clock' , 'vase' , 'scissors' , 'teddy bear' , 'hair drier' , 'toothbrush' ] Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth [(0.8131868131868139, 0.0, 1.0), (0.0, 1.0, 0.9670329670329672), (1.0, 0.2637362637362637, 0.0), (1.0, 0.5934065934065934, 0.0), (0.9450549450549453, 0.0, 1.0), (0.0, 0.703296703296703, 1.0), (0.4175824175824179, 0.0, 1.0), (0.41758241758241743, 1.0, 0.0), (0.0, 1.0, 0.7692307692307692), (0.6813186813186813, 1.0, 0.0), (0.0, 0.5054945054945055, 1.0), (0.4835164835164836, 1.0, 0.0), (1.0, 0.0, 0.197802197802198), (1.0, 0.6593406593406593, 0.0), (0.0, 1.0, 0.4395604395604398), (0.4835164835164836, 0.0, 1.0), (1.0, 0.0, 0.0659340659340657), (0.0, 0.6373626373626373, 1.0), (1.0, 0.3296703296703296, 0.0), (0.6813186813186816, 0.0, 1.0), (0.3516483516483515, 1.0, 0.0), (0.0, 1.0, 0.5054945054945055), (1.0, 0.0, 0.9230769230769234), (1.0, 0.0, 0.7912087912087911), (0.0219780219780219, 1.0, 0.0), (1.0, 0.0, 0.5274725274725274), (0.945054945054945, 1.0, 0.0), (0.0, 1.0, 0.901098901098901), (1.0, 0.0, 0.3296703296703303), (0.8791208791208796, 0.0, 1.0), (0.0, 0.3076923076923075, 1.0), (0.0, 1.0, 0.5714285714285712), (0.7472527472527473, 1.0, 0.0), (1.0, 0.0, 0.7252747252747254), (0.0, 1.0, 0.37362637362637363), (0.2197802197802199, 0.0, 1.0), (1.0, 0.0, 0.2637362637362637), (0.6153846153846154, 1.0, 0.0), (1.0, 0.46153846153846156, 0.0), (0.0, 0.9010989010989006, 1.0), (0.6153846153846159, 0.0, 1.0), (1.0, 0.7252747252747253, 0.0), (1.0, 0.39560439560439564, 0.0), (0.0, 1.0, 0.7032967032967035), (1.0, 0.0, 0.989010989010989), (1.0, 0.0, 0.6593406593406597), (0.0219780219780219, 0.0, 1.0), (0.8131868131868132, 1.0, 0.0), (1.0, 0.0, 0.1318681318681323), (0.0, 0.1758241758241761, 1.0), (0.0, 0.5714285714285716, 1.0), (0.0, 1.0, 0.8351648351648349), (1.0, 0.0, 0.395604395604396), (0.5494505494505495, 1.0, 0.0), (0.0, 0.1098901098901095, 1.0), (0.0, 1.0, 0.30769230769230793), (0.0, 1.0, 0.0439560439560438), (0.15384615384615374, 1.0, 0.0), (0.2857142857142856, 0.0, 1.0), (1.0, 0.9230769230769231, 0.0), (0.0, 1.0, 0.17582417582417564), (0.3516483516483513, 0.0, 1.0), (0.0, 0.9670329670329672, 1.0), (0.0, 0.4395604395604398, 1.0), (0.2197802197802199, 1.0, 0.0), (1.0, 0.0, 0.5934065934065931), (1.0, 0.06593406593406592, 0.0), (0.5494505494505493, 0.0, 1.0), (1.0, 0.13186813186813184, 0.0), (1.0, 0.0, 0.8571428571428577), (0.0, 1.0, 0.6373626373626373), (1.0, 0.8571428571428571, 0.0), (0.08791208791208804, 1.0, 0.0), (0.0, 1.0, 0.2417582417582418), (1.0, 0.0, 0.4615384615384617), (1.0, 0.0, 0.0), (0.0, 0.3736263736263741, 1.0), (0.0, 0.7692307692307692, 1.0), (1.0, 0.19780219780219777, 0.0), (0.0879120879120876, 0.0, 1.0), (1.0, 0.7912087912087913, 0.0), (0.2857142857142858, 1.0, 0.0), (0.0, 0.8351648351648349, 1.0), (0.0, 0.0439560439560438, 1.0), (0.0, 0.2417582417582418, 1.0), (0.1538461538461533, 0.0, 1.0), (1.0, 0.989010989010989, 0.0), (0.0, 1.0, 0.10989010989010994), (1.0, 0.5274725274725275, 0.0), (0.7472527472527473, 0.0, 1.0), (0.8791208791208791, 1.0, 0.0)] Instance segmentation pipeline We define three util functions used for model inference. get_colored_mask get the colored mask for a specific class label in the image get_prediction take the img_path, and confidence as input, and returns predicted bounding boxes, classes, and masks. segment_instance uses the get_prediction function and gives the visualization result. In [0]: def get_coloured_mask ( mask ): \"\"\" random_colour_masks parameters: - image - predicted masks method: - the masks of each predicted object is given random colour for visualization \"\"\" colours = [[ 0 , 255 , 0 ],[ 0 , 0 , 255 ],[ 255 , 0 , 0 ],[ 0 , 255 , 255 ],[ 255 , 255 , 0 ],[ 255 , 0 , 255 ],[ 80 , 70 , 180 ],[ 250 , 80 , 190 ],[ 245 , 145 , 50 ],[ 70 , 150 , 250 ],[ 50 , 190 , 190 ]] r = np . zeros_like ( mask ) . astype ( np . uint8 ) g = np . zeros_like ( mask ) . astype ( np . uint8 ) b = np . zeros_like ( mask ) . astype ( np . uint8 ) r [ mask == 1 ], g [ mask == 1 ], b [ mask == 1 ] = colours [ random . randrange ( 0 , 10 )] coloured_mask = np . stack ([ r , g , b ], axis = 2 ) return coloured_mask In [0]: def get_prediction ( img_path , confidence ): \"\"\" get_prediction parameters: - img_path - path of the input image - confidence - threshold to keep the prediction or not method: - Image is obtained from the image path - the image is converted to image tensor using PyTorch's Transforms - image is passed through the model to get the predictions - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks ie: eg. segment of cat is made 1 and rest of the image is made 0 \"\"\" img = Image . open ( img_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) pred = model ([ img ]) pred_score = list ( pred [ 0 ][ 'scores' ] . detach () . numpy ()) pred_t = [ pred_score . index ( x ) for x in pred_score if x > confidence ][ - 1 ] masks = ( pred [ 0 ][ 'masks' ] > 0.5 ) . squeeze () . detach () . cpu () . numpy () # print(pred[0]['labels'].numpy().max()) pred_class = [ COCO_CLASS_NAMES [ i ] for i in list ( pred [ 0 ][ 'labels' ] . numpy ())] pred_boxes = [[( i [ 0 ], i [ 1 ]), ( i [ 2 ], i [ 3 ])] for i in list ( pred [ 0 ][ 'boxes' ] . detach () . numpy ())] masks = masks [: pred_t + 1 ] pred_boxes = pred_boxes [: pred_t + 1 ] pred_class = pred_class [: pred_t + 1 ] return masks , pred_boxes , pred_class In [0]: def segment_instance ( img_path , confidence = 0.5 , rect_th = 2 , text_size = 2 , text_th = 2 ): \"\"\" segment_instance parameters: - img_path - path to input image - confidence- confidence to keep the prediction or not - rect_th - rect thickness - text_size - text_th - text thickness method: - prediction is obtained by get_prediction - each mask is given random color - each mask is added to the image in the ration 1:0.8 with opencv - final output is displayed \"\"\" masks , boxes , pred_cls = get_prediction ( img_path , confidence ) img = cv2 . imread ( img_path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) for i in range ( len ( masks )): rgb_mask = get_coloured_mask ( masks [ i ]) img = cv2 . addWeighted ( img , 1 , rgb_mask , 0.5 , 0 ) cv2 . rectangle ( img , boxes [ i ][ 0 ], boxes [ i ][ 1 ], color = ( 0 , 255 , 0 ), thickness = rect_th ) cv2 . putText ( img , pred_cls [ i ], boxes [ i ][ 0 ], cv2 . FONT_HERSHEY_SIMPLEX , text_size , ( 0 , 255 , 0 ), thickness = text_th ) plt . figure ( figsize = ( 20 , 30 )) plt . imshow ( img ) plt . xticks ([]) plt . yticks ([]) plt . show () Making predictions Now we are ready to use the model to do inference. Let's look at a few examples. Here we are using the same examples as we used in the Faster-RCNN object detection post. As Mask-RCNN and Faster-RCNN share the same structure, so similar prediction results are expected. Example 1 In [16]: ! wget -nv https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me.jpg -O traffic.jpg segment_instance ( './traffic.jpg' , confidence = 0.7 ) 2020-06-14 22:49:51 URL:https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me_800.jpg?cached=1522560655 [409997/409997] -> \"traffic.jpg\" [1] The result is a bit surprising. We not only detected the three cars in the picture, but also detect the person in the car which is very indistinct. Example 2 In [17]: ! wget -nv https://pixnio.com/free-images/2018/12/10/2018-12-10-18-38-14-1196x900.jpg -O traffic2.jpg segment_instance ( './traffic2.jpg' , confidence = 0.7 ) 2020-06-14 23:04:05 URL:https://pixnio.com/free-images/2018/12/10/2018-12-10-18-38-14-1196x900.jpg [189333/189333] -> \"traffic2.jpg\" [1] It looks like we are getting quite accurate predictions with the model. Example 3 In [19]: ! wget -nv https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg -O pedestrian.jpg segment_instance ( './pedestrian.jpg' , confidence = 0.7 ) 2020-06-14 23:04:51 URL:https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg [409534/409534] -> \"pedestrian.jpg\" [1] Comparing inference time for CPU and GPU Let's take a look at the inference time of the model for CPU and GPU. I am using Google Colab to do the experiment. In [0]: import time def check_inference_time ( image_path , gpu = False ): model = torchvision . models . detection . maskrcnn_resnet50_fpn ( pretrained = True ) model . eval () img = Image . open ( image_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) if gpu : model . cuda () img = img . cuda () else : model . cpu () img = img . cpu () start_time = time . time () pred = model ([ img ]) end_time = time . time () return end_time - start_time In [22]: cpu_time = sum ([ check_inference_time ( './traffic.jpg' , gpu = False ) for _ in range ( 10 )]) / 10.0 gpu_time = sum ([ check_inference_time ( './traffic.jpg' , gpu = True ) for _ in range ( 10 )]) / 10.0 print ( ' \\n\\n Average Time take by the model with GPU = {} s \\n Average Time take by the model with CPU = {} s' . format ( gpu_time , cpu_time )) Average Time take by the model with GPU = 0.18893978595733643s Average Time take by the model with CPU = 5.276700663566589s In [23]: plt . bar ([ 0.1 , 0.2 ], [ cpu_time , gpu_time ], width = 0.08 ) plt . ylabel ( 'Time/s' ) plt . xticks ([ 0.1 , 0.2 ], [ 'CPU' , 'GPU' ]) plt . title ( 'Inference time of Mask-RCNN with Resnet-50 backbone on CPU and GPU' ) plt . show ()","tags":"Computer Vision","url":"https://haochen23.github.io/2020/05/instance-segmentation-mask-rcnn.html","loc":"https://haochen23.github.io/2020/05/instance-segmentation-mask-rcnn.html"},{"title":"NLP Sincereness Detector using Pytorch","text":"NLP Sincereness Detector using Pytorch In this post, we will use Pytorch to train a NLP Sincereness Detector which will detect whether a question is asked sincerely or not. I will walk you through data preparation, model architecture, model training and evaluation. I've put all the code in my github repo including the dataset. Feel free to clone or download it. Understanding the Problem The dataset we are using is a Quora questions dataset. Quora wants to keep track of insincere questions on their platform so as to make users feel safe while sharing their knowledge. An insincere question in this context is defined as a question intended to make a statement rather than looking for helpful answers. To break this down further, here are some characteristics that can signify that a particular question is insincere: Has a non-neutral tone Is disparaging or inflammatory Isn't grounded in reality Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers The dataset contains the question that was asked, and a flag denoting whether it was identified as insincere (target = 1). Load and Prepare the Data First, clone the my repo from Github. And change to the cloned directory. I will only show you the important part of the code, detailed implementation please find in the source code in the repo. ! git clone https : // github . com / haochen23 / pytorch - nlp - classifier . git % cd pytorch - nlp - classifier / Use the following function to load and prepare the dataset. # Adopted from utils.py def load_data ( file_path ): ''' load and prepare dataset to training and validation iterator ''' TEXT = data . Field ( tokenize = \"spacy\" , batch_first = True , include_lengths = True ) LABEL = data . LabelField ( dtype = torch . float , batch_first = True ) fields = [( None , None ), ( 'text' , TEXT ), ( 'label' , LABEL )] total_data = data . TabularDataset ( path = file_path , format = \"csv\" , fields = fields , skip_header = True ) # split data train_data , valid_data = total_data . split ( split_ratio = 0.7 , random_state = random . seed ( seed )) # initialize glove embeddings TEXT . build_vocab ( train_data , min_freq = 3 , vectors = \"glove.6B.100d\" ) LABEL . build_vocab ( train_data ) batch_size = config . BATCH_SIZE device = config . device train_iterator , valid_iterator = data . BucketIterator . splits ( ( train_data , valid_data ), batch_size = batch_size , sort_key = lambda x : len ( x . text ), sort_within_batch = True , device = device ) return train_iterator , valid_iterator , TEXT , LABEL This function prepares the data and return 4 objects: train_iterator iterator for training data used for model training valid_iterator iterator for validation data used for model validation during training TEXT torch.data.Field object, contains all textual information of train_data LABEL torch.data.LabelField object, contains all label information of train_data Model Architecture We define a classifier class which inherited from torch.nn.Module . In this class we define two functions: __init__ : we will define all the layers that will be used in our model. forward : forward pass of our model. Then, let's take a look the layers we used in the model. Embedding layer : Embeddings are extremely important for any NLP related task since it represents a word in a numerical format. Embedding layer creates a look up table where each row represents an embedding of a word. The embedding layer converts the integer sequence into a dense vector representation. LSTM : LSTM is a improvement of RNN that is capable of capturing long term dependencies. Linear : Linear layer refers to dense layer. Pack Padding : ack padding is used to define the dynamic recurrent neural network. Without pack padding, the padding inputs are also processed by the rnn and returns the hidden state of the padded element. This an awesome wrapper that does not show the inputs that are padded. It simply ignores the values and returns the hidden state of the non padded element. Now let's see the code that defines model architecture. # Adopted from classifer.py class classifer ( nn . Module ): #define all the layers used in the model def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim , n_layers , bidirectional , dropout ): super () . __init__ () # embedding layer self . embedding = nn . Embedding ( vocab_size , embedding_dim ) # lstm layer self . lstm = nn . LSTM ( embedding_dim , hidden_dim , num_layers = n_layers , bidirectional = bidirectional , dropout = dropout , batch_first = True ) # dense layer self . fc = nn . Linear ( hidden_dim * 2 , output_dim ) # activation self . act = nn . Sigmoid () def forward ( self , text , text_lengths ): # text=[batch size, sent_length] embedded = self . embedding ( text ) # embedded = [batch size, sent_len, emb dim] #packed sequence packed_embedded = nn . utils . rnn . pack_padded_sequence ( embedded , text_lengths , batch_first = True ) packed_output , ( hidden , cell ) = self . lstm ( packed_embedded ) # concat the final forward and backward hidden state hidden = torch . cat (( hidden [ - 2 ,:,:], hidden [ - 1 ,:,:]), dim = 1 ) dense_outputs = self . fc ( hidden ) #final activation outputs = self . act ( dense_outputs ) return outputs Define Training Process The following two functions are used for traning and validating during the model training process. The comments in the code pretty much explains the use of the two functions. # Adopted from train.py def train ( model , iterator , optimizer , criterion ): # initialize every epoch epoch_loss = 0 epoch_acc = 0 # set the model in training mode model . train () for batch in iterator : #reset the gradients after every batch optimizer . zero_grad () # retrieve text and num of words text , text_lengths = batch . text # convert to 1D tensor predictions = model ( text , text_lengths ) . squeeze () # compute the loss loss = criterion ( predictions , batch . label ) # print(loss.item()) # compute binary accuracy acc = binary_accuracy ( predictions , batch . label ) # print(acc.item()) # backpropogate the loss and compute the gradients loss . backward () # update weights optimizer . step () # loss and accuracy epoch_loss += loss . item () epoch_acc += acc . item () return epoch_loss / len ( iterator ), epoch_acc / len ( iterator ) def evaluate ( model , iterator , criterion ): # initialize every epoch epoch_loss = 0 epoch_acc = 0 #set model to eval mode model . eval () # deactivates auto_grad with torch . no_grad (): for batch in iterator : # retrieve text and num of words text , text_lengths = batch . text # convert to 1D tensor predictions = model ( text , text_lengths ) . squeeze () loss = criterion ( predictions , batch . label ) acc = binary_accuracy ( predictions , batch . label ) # keep track of loss and accuracy epoch_loss += loss . item () epoch_acc += acc . item () return epoch_loss / len ( iterator ), epoch_acc / len ( iterator ) We use a for loop train the model for N_EPOCHS . # Adopted from train.py optimizer = optim . Adam ( model . parameters ()) criterion = nn . BCELoss () N_EPOCHS = config . N_EPOCH best_valid_loss = float ( 'inf' ) # start training for epoch in range ( N_EPOCHS ): # train the model train_loss , train_acc = train ( model , train_iterator , optimizer , criterion ) # evaluate the model valid_loss , valid_acc = evaluate ( model , valid_iterator , criterion ) # save the best model if valid_loss < best_valid_loss : best_valid_loss = valid_loss # torch.save(model.state_dict(), './output/best_model.pt') torch . save ( model , './output/best_model.pt' ) print ( \"Epoch {} / {} :\" . format ( epoch + 1 , N_EPOCHS )) print ( f ' \\t Train Loss: { train_loss : .3f } | Train Acc: { train_acc * 100 : .2f } %' ) print ( f ' \\t Val. Loss: { valid_loss : .3f } | Val. Acc: { valid_acc * 100 : .2f } %' ) Demonstration After we reviewed the important code blocks above, it's time to see how to actually use the code. Train the model The first time run the code need to download the glove.6B.100d in order to build the TEXT vocabulary. This procedure may take a while according to your internet speed, since the pre-trained embedding is about 870 Mb. The best_model will be saved in the output folder together with TEXT which contains vocabulary information for our training data, and it will be also used for prediction. In [1]: % cd pytorch-nlp-classifier/ /content/pytorch-nlp-classifier In [2]: ! python3 train.py Size of TEXT vocabulary: 17119 classifer( (embedding): Embedding(17119, 100) (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True) (fc): Linear(in_features=64, out_features=1, bias=True) (act): Sigmoid() ) The model has 1,771,357 trainable parameters Epoch 1/5: Train Loss: 0.331 | Train Acc: 84.24% Val. Loss: 0.285 | Val. Acc: 88.86% Epoch 2/5: Train Loss: -0.329 | Train Acc: 89.81% Val. Loss: 0.337 | Val. Acc: 89.21% Epoch 3/5: Train Loss: -1.319 | Train Acc: 91.68% Val. Loss: 0.336 | Val. Acc: 88.88% Epoch 4/5: Train Loss: -1.484 | Train Acc: 92.73% Val. Loss: 0.385 | Val. Acc: 88.58% Epoch 5/5: Train Loss: -1.578 | Train Acc: 93.98% Val. Loss: 0.391 | Val. Acc: 88.23% /usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning) We obtained about 89% accuracy on the validation data in 5 epochs. Inference We'll see a few examples to check how our model performs on identifying sincere and insinsere questions. Example 1 : \"Why people vote for Donald Trump?\" In [3]: ! python3 predict.py --question \"Why people vote for Donald Trump?\" Results close to 1 represent insincere questions. Results close to 0 represent sincere questions. ------ The result for 'Why people vote for Donald Trump?' is 0.9592183232307434 The prediction result is very close to 1, and it indicates this is an insincere question. Wow, I will take this, as it is always not neutral when you talk something about Trump. LOL. Example 2 : \"What is Quora and why should you care?\" In [4]: ! python3 predict.py --question \"What is Quora and why should you care?\" Results close to 1 represent insincere questions. Results close to 0 represent sincere questions. ------ The result for 'What is Quora and why should you care?' is 0.23973408341407776 The prediction result is ~ 0.24 and it's close to zero, which means it is a sincere question. And yes, there is not much emotion involved in this question, and it's neutral. Example 3 : \" What is your biggest weakness?\" In [5]: ! python3 predict.py --question \" What is your biggest weakness?\" Results close to 1 represent insincere questions. Results close to 0 represent sincere questions. ------ The result for ' What is your biggest weakness?' is 0.02312156930565834 This is pretty common interview question. And the result shows that it is a sincere question. These examples indicate that our trained sincere detetor works pretty well. Summary In this post, we've used Pytorch to train a NLP Sincereness Detector which will detect whether a question is asked sincerely or not. We see the complete PyTorch workflow for training the model and performing inference.","tags":"NLP","url":"https://haochen23.github.io/2020/04/pytorch-sincereness-detector.html","loc":"https://haochen23.github.io/2020/04/pytorch-sincereness-detector.html"},{"title":"Object Detection using Faster-RCNN PyTorch","text":"Object Detection using Faster-RCNN Pytorch In this post, we will explore Faster-RCNN object detector with Pytorch. We will use the pretrained Faster-RCNN model with Resnet50 as the backbone. Understanding model inputs and outputs: The pretrained Faster-RCNN ResNet-50 model we are going to use expects the input image tensor to be in the form [n, c, h, w] where n is the number of images c is the number of channels , for RGB images its 3 h is the height of the image w is the widht of the image The model will return Bounding boxes [x0, y0, x1, y1] all all predicted classes of shape (N,4) where N is the number of classes predicted by the model to be present in the image. Labels of all predicted classes. Scores of each predicted label. Load model Now, we are loading the pretrained Faster-RCNN Resnet50 model, and also loading the COCO dataset category names. In [0]: # import necessary libraries % matplotlib inline import matplotlib.pyplot as plt from PIL import Image import torch import torchvision.transforms as T import torchvision import numpy as np import cv2 import warnings warnings . filterwarnings ( 'ignore' ) In [0]: # load model model = torchvision . models . detection . fasterrcnn_resnet50_fpn ( pretrained = True ) # set to evaluation mode model . eval () # load the COCO dataset category names # we will use the same list for this notebook COCO_INSTANCE_CATEGORY_NAMES = [ '__background__' , 'person' , 'bicycle' , 'car' , 'motorcycle' , 'airplane' , 'bus' , 'train' , 'truck' , 'boat' , 'traffic light' , 'fire hydrant' , 'N/A' , 'stop sign' , 'parking meter' , 'bench' , 'bird' , 'cat' , 'dog' , 'horse' , 'sheep' , 'cow' , 'elephant' , 'bear' , 'zebra' , 'giraffe' , 'N/A' , 'backpack' , 'umbrella' , 'N/A' , 'N/A' , 'handbag' , 'tie' , 'suitcase' , 'frisbee' , 'skis' , 'snowboard' , 'sports ball' , 'kite' , 'baseball bat' , 'baseball glove' , 'skateboard' , 'surfboard' , 'tennis racket' , 'bottle' , 'N/A' , 'wine glass' , 'cup' , 'fork' , 'knife' , 'spoon' , 'bowl' , 'banana' , 'apple' , 'sandwich' , 'orange' , 'broccoli' , 'carrot' , 'hot dog' , 'pizza' , 'donut' , 'cake' , 'chair' , 'couch' , 'potted plant' , 'bed' , 'N/A' , 'dining table' , 'N/A' , 'N/A' , 'toilet' , 'N/A' , 'tv' , 'laptop' , 'mouse' , 'remote' , 'keyboard' , 'cell phone' , 'microwave' , 'oven' , 'toaster' , 'sink' , 'refrigerator' , 'N/A' , 'book' , 'clock' , 'vase' , 'scissors' , 'teddy bear' , 'hair drier' , 'toothbrush' ] We can see some N/A's in the list, as a few classes were removed in the later papers. We will go with the list given by Pytorch. Object detection pipeline We define two functions used for model inference. get_prediction take the img_path, and confidence as input, and returns predicted bounding boxes and classes. detect_object uses the get_prediction function and gives the visualization result. In [0]: def get_prediction ( img_path , confidence ): \"\"\" get_prediction parameters: - img_path - path of the input image - confidence - threshold value for prediction score method: - Image is obtained from the image path - the image is converted to image tensor using PyTorch's Transforms - image is passed through the model to get the predictions - class, box coordinates are obtained, but only prediction score > threshold are chosen. \"\"\" img = Image . open ( img_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) pred = model ([ img ]) pred_class = [ COCO_INSTANCE_CATEGORY_NAMES [ i ] for i in list ( pred [ 0 ][ 'labels' ] . numpy ())] pred_boxes = [[( i [ 0 ], i [ 1 ]), ( i [ 2 ], i [ 3 ])] for i in list ( pred [ 0 ][ 'boxes' ] . detach () . numpy ())] pred_score = list ( pred [ 0 ][ 'scores' ] . detach () . numpy ()) pred_t = [ pred_score . index ( x ) for x in pred_score if x > confidence ][ - 1 ] pred_boxes = pred_boxes [: pred_t + 1 ] pred_class = pred_class [: pred_t + 1 ] return pred_boxes , pred_class In [0]: def detect_object ( img_path , confidence = 0.5 , rect_th = 2 , text_size = 2 , text_th = 2 ): \"\"\" object_detection_api parameters: - img_path - path of the input image - confidence - threshold value for prediction score - rect_th - thickness of bounding box - text_size - size of the class label text - text_th - thichness of the text method: - prediction is obtained from get_prediction method - for each prediction, bounding box is drawn and text is written with opencv - the final image is displayed \"\"\" boxes , pred_cls = get_prediction ( img_path , confidence ) img = cv2 . imread ( img_path ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # print(len(boxes)) for i in range ( len ( boxes )): cv2 . rectangle ( img , boxes [ i ][ 0 ], boxes [ i ][ 1 ], color = ( 0 , 255 , 0 ), thickness = rect_th ) cv2 . putText ( img , pred_cls [ i ], boxes [ i ][ 0 ], cv2 . FONT_HERSHEY_SIMPLEX , text_size , ( 0 , 255 , 0 ), thickness = text_th ) plt . figure ( figsize = ( 20 , 30 )) plt . imshow ( img ) plt . xticks ([]) plt . yticks ([]) plt . show () Making predictions Now we are ready to use the model to do inference. Let's look at a few examples. Example 1 In [30]: ! wget -nv https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me.jpg -O traffic.jpg detect_object ( './traffic.jpg' , confidence = 0.7 ) 2020-06-14 10:28:35 URL:https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me_800.jpg?cached=1522560655 [409997/409997] -> \"traffic.jpg\" [1] The result is a bit surprising. We not only detected the three cars in the picture, but also detect the person in the car which is very indistinct. Example 2 In [33]: ! wget -nv https://pixnio.com/free-images/2018/12/10/2018-12-10-18-38-14-1196x900.jpg -O traffic2.jpg detect_object ( './traffic2.jpg' , confidence = 0.7 ) 2020-06-14 10:35:30 URL:https://pixnio.com/free-images/2018/12/10/2018-12-10-18-38-14-1196x900.jpg [189333/189333] -> \"traffic2.jpg\" [1] It looks like we are getting quite accurate predictions with the model. Example 3 In [34]: ! wget -nv https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg -O pedestrian.jpg detect_object ( './pedestrian.jpg' , confidence = 0.7 ) 2020-06-14 10:37:56 URL:https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg [409534/409534] -> \"pedestrian.jpg\" [1] Comparing inference time for CPU and GPU Let's take a look at the inference time of the model for CPU and GPU. I am using Google Colab to do the experiment. In [0]: import time def check_inference_time ( image_path , gpu = False ): model = torchvision . models . detection . fasterrcnn_resnet50_fpn ( pretrained = True ) model . eval () img = Image . open ( image_path ) transform = T . Compose ([ T . ToTensor ()]) img = transform ( img ) if gpu : model . cuda () img = img . cuda () else : model . cpu () img = img . cpu () start_time = time . time () pred = model ([ img ]) end_time = time . time () return end_time - start_time In [36]: cpu_time = sum ([ check_inference_time ( './traffic.jpg' , gpu = False ) for _ in range ( 10 )]) / 10.0 gpu_time = sum ([ check_inference_time ( './traffic.jpg' , gpu = True ) for _ in range ( 10 )]) / 10.0 print ( ' \\n\\n Average Time take by the model with GPU = {} s \\n Average Time take by the model with CPU = {} s' . format ( gpu_time , cpu_time )) Average Time take by the model with GPU = 0.10062439441680908s Average Time take by the model with CPU = 4.527194166183472s In [37]: plt . bar ([ 0.1 , 0.2 ], [ cpu_time , gpu_time ], width = 0.08 ) plt . ylabel ( 'Time/s' ) plt . xticks ([ 0.1 , 0.2 ], [ 'CPU' , 'GPU' ]) plt . title ( 'Inference time of Faster-RCNN with Resnet-50 backbone on CPU and GPU' ) plt . show () Using Google Colab, the inference time of the Faster-RCNN model on GPU is approximately 45 times faster than on CPU.","tags":"Computer Vision","url":"https://haochen23.github.io/2020/04/object-detection-faster-rcnn.html","loc":"https://haochen23.github.io/2020/04/object-detection-faster-rcnn.html"},{"title":"Transfer Learning in NLP - BERT as Service for Text Classification","text":"Transfer Learning in NLP - BERT as Service for Text Classification BERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. BERT can be viewed as a pre-trained model in Computer Vision. (source: BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding https://arxiv.org/pdf/1810.04805v2.pdf ) In this post, I will show you how to use BERT as a feature extractor (embedding extractor) and perform text classification on the outputs of BERT. Setting Up the Environment Use the following code to install BERT server and client, downgrade tensorflow (in order to run BERT server), download and unzip the pretrained BERT model. We are using BERT-Base Uncased model. ! pip install bert - serving - server # server ! pip install bert - serving - client # client, independent of `bert-serving-server` ! pip install tensorflow - gpu == 1.15 . 0 # downgrade tensorflow to 1.15.0 # download and unzip the BERT Model ! wget https : // storage . googleapis . com / bert_models / 2018_10_18 / uncased_L - 12 _H - 768 _A - 12. zip ! unzip uncased_L - 12 _H - 768 _A - 12. zip Starting BERT Serve Run the following code to start the BERT server. If you are using Google Colab, run the second code line bert - serving - start - model_dir =./ uncased_L - 12 _H - 768 _A - 12 - num_worker = 4 - max_seq_len 50 #run this if use Colab ! nohup bert - serving - start - model_dir =./ uncased_L - 12 _H - 768 _A - 12 - num_worker = 4 - max_seq_len 50 > out . file 2 >& 1 & Now the BERT server should be runing, now we simply test whether it is working properly. In [1]: from bert_serving.client import BertClient bc = BertClient () print ( bc . encode ([ 'We are using BERT model' , 'BERT is amazing' ]) . shape ) (2, 768) Perfect! Both the BERT server and client is working properly. As we can see from the above output, BERT model converts a sentence into a 768-dimentional vector, we also call it embeddings. We can feed these embeddings to a classifer and then get the classification results. Data Preparation For this text classificaiton task, we are going to use the twitter sentiment dataset as in our previous post . This dataset contains 20000 labeled tweets data. And use the following code to download the dataset. ! wget https : // raw . githubusercontent . com / haochen23 / nlp - rnn - lstm - sentiment / master / training . 1600000. processed . noemoticon . csv Load and preview the dataset In this data table, 0th column represents the sentiment label, and 5th column contains the actual tweet text content. In [2]: # import necessary libraries import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report import re from keras.models import Sequential from keras.layers import Dense , Dropout data = pd . read_csv ( \"training.1600000.processed.noemoticon.csv\" , header = None , encoding = 'latin-1' ) print ( \"The shape of the original dataset is {} \" . format ( data . shape )) data . head () Using TensorFlow backend. The shape of the original dataset is (20000, 6) Out[2]: 0 1 2 3 4 5 0 0 1467810369 Mon Apr 06 22:19:45 PDT 2009 NO_QUERY _TheSpecialOne_ @switchfoot http://twitpic.com/2y1zl - Awww, t... 1 0 1467810672 Mon Apr 06 22:19:49 PDT 2009 NO_QUERY scotthamilton is upset that he can't update his Facebook by ... 2 0 1467810917 Mon Apr 06 22:19:53 PDT 2009 NO_QUERY mattycus @Kenichan I dived many times for the ball. Man... 3 0 1467811184 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY ElleCTF my whole body feels itchy and like its on fire 4 0 1467811193 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY Karoli @nationwideclass no, it's not behaving at all.... Clean the texts For simplicity, we use the clean_text function to briefly clean the text content by removing non-alphabets and unicode characters. In [3]: # clean text from noise def clean_text ( text ): # filter to allow only alphabets text = re . sub ( r '[&#94;a-zA-Z \\' ]' , ' ' , text ) # remove Unicode characters text = re . sub ( r '[&#94;\\x00-\\x7F]+' , '' , text ) # convert to lowercase to maintain consistency text = text . lower () return text data [ 'clean_text' ] = data [ data . columns [ 5 ]] . apply ( clean_text ) Split the dataset for training and testing In [4]: data_y = data [ data . columns [ 0 ]] . to_numpy () data_y = pd . get_dummies ( data_y ) . to_numpy () X_tr , X_test , y_tr , y_test = train_test_split ( data [ 'clean_text' ] . to_numpy (), data_y , test_size = 0.3 , random_state = 42 ) print ( 'X_tr shape:' , X_tr . shape ) print ( 'X_test shape:' , X_test . shape ) X_tr shape: (14000,) X_test shape: (6000,) Transform the text data into BERT embeddings using BERT server and client As we mentioned before, each tweet will be convert into a 768-length embeddings by BERT. After convert the tweets data to embeddings, we can then feed these embeddings to any type of classifier to do text classification. In [5]: from bert_serving.client import BertClient # make a connection with the BERT server using it's ip address bc = BertClient () # get the embedding for train and val sets X_tr_bert = bc . encode ( X_tr . tolist ()) X_test_bert = bc . encode ( X_test . tolist ()) Model Training (Fine-tuning BERT) Now, you will see how easy it is to fine-tune a BERT model. This time, we simply use a feed-forward network with only one hidden layer as the the classification head. And use Dropout to combat overfitting. You can play with the model architecture. Build and train the model In [6]: model = Sequential () model . add ( Dense ( 100 , activation = 'relu' , input_dim = 768 )) # model.add(Dense(100, activation='relu')) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 2 , activation = 'softmax' )) model . compile ( loss = \"categorical_crossentropy\" , optimizer = 'adam' , metrics = [ 'accuracy' ]) H = model . fit ( X_tr_bert , y_tr , epochs = 15 , validation_split = 0.2 ) WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. Train on 11200 samples, validate on 2800 samples Epoch 1/15 11200/11200 [==============================] - 1s 129us/step - loss: 0.5363 - accuracy: 0.7265 - val_loss: 0.4728 - val_accuracy: 0.7825 Epoch 2/15 11200/11200 [==============================] - 1s 99us/step - loss: 0.4786 - accuracy: 0.7733 - val_loss: 0.4629 - val_accuracy: 0.7821 Epoch 3/15 11200/11200 [==============================] - 1s 99us/step - loss: 0.4605 - accuracy: 0.7807 - val_loss: 0.4553 - val_accuracy: 0.7871 Epoch 4/15 11200/11200 [==============================] - 1s 99us/step - loss: 0.4493 - accuracy: 0.7871 - val_loss: 0.4580 - val_accuracy: 0.7893 Epoch 5/15 11200/11200 [==============================] - 1s 99us/step - loss: 0.4395 - accuracy: 0.7921 - val_loss: 0.4732 - val_accuracy: 0.7764 Epoch 6/15 11200/11200 [==============================] - 1s 98us/step - loss: 0.4282 - accuracy: 0.8012 - val_loss: 0.4554 - val_accuracy: 0.7954 Epoch 7/15 11200/11200 [==============================] - 1s 102us/step - loss: 0.4193 - accuracy: 0.8066 - val_loss: 0.4570 - val_accuracy: 0.7864 Epoch 8/15 11200/11200 [==============================] - 1s 100us/step - loss: 0.4111 - accuracy: 0.8135 - val_loss: 0.4736 - val_accuracy: 0.7732 Epoch 9/15 11200/11200 [==============================] - 1s 97us/step - loss: 0.3959 - accuracy: 0.8173 - val_loss: 0.4577 - val_accuracy: 0.7950 Epoch 10/15 11200/11200 [==============================] - 1s 100us/step - loss: 0.3922 - accuracy: 0.8210 - val_loss: 0.4593 - val_accuracy: 0.8004 Epoch 11/15 11200/11200 [==============================] - 1s 99us/step - loss: 0.3827 - accuracy: 0.8316 - val_loss: 0.4633 - val_accuracy: 0.7971 Epoch 12/15 11200/11200 [==============================] - 1s 96us/step - loss: 0.3679 - accuracy: 0.8387 - val_loss: 0.4573 - val_accuracy: 0.8021 Epoch 13/15 11200/11200 [==============================] - 1s 100us/step - loss: 0.3579 - accuracy: 0.8393 - val_loss: 0.4613 - val_accuracy: 0.7889 Epoch 14/15 11200/11200 [==============================] - 1s 97us/step - loss: 0.3574 - accuracy: 0.8442 - val_loss: 0.4714 - val_accuracy: 0.7989 Epoch 15/15 11200/11200 [==============================] - 1s 97us/step - loss: 0.3423 - accuracy: 0.8535 - val_loss: 0.4833 - val_accuracy: 0.7943 Model Evaluation Now, let's plot the training and validation loss, and traing and validationaccuracy during the training process. In [7]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( \"ggplot\" ) N = 15 plt . plot ( np . arange ( 0 , N ), H . history [ \"loss\" ], label = \"train_loss\" ) plt . plot ( np . arange ( 0 , N ), H . history [ \"val_loss\" ], label = \"val_loss\" ) plt . plot ( np . arange ( 0 , N ), H . history [ \"accuracy\" ], label = \"train_acc\" ) plt . plot ( np . arange ( 0 , N ), H . history [ \"val_accuracy\" ], label = \"val_acc\" ) plt . title ( \"Training Loss and Accuracy\" ) plt . xlabel ( \"Epoch #\" ) plt . ylabel ( \"Loss/Accuracy\" ) plt . legend ( loc = \"upper left\" ) plt . show () Using the BERT extract embeddings, we obtained a pretty similar results in our previous post where we tried using different types of Recurrent Neural Networks, namely SimpleRNN, LSTM, and GRU. You can refer to that post for comparison. Evaluate on test set In [8]: predictions = model . predict ( X_test_bert ) predictions = predictions . argmax ( axis = 1 ) print ( classification_report ( y_test . argmax ( axis = 1 ), predictions )) precision recall f1-score support 0 0.78 0.79 0.79 3016 1 0.79 0.78 0.78 2984 accuracy 0.79 6000 macro avg 0.79 0.78 0.78 6000 weighted avg 0.79 0.79 0.78 6000 We obtained an accuracy of 0.79 on the test set, which is consistent of the accuracy of training and validation. It seems our model can generalize pretty well. Although the accuracy is not very high, you can try different architecture to improve the model accuracy. Besides neural networks, you can also try machine learning algorithms on the extracted embeddings. For example, you can use a Logistic Regression Classifer, or an SVM Classifier, etc. Summary In this post, we've seen how to perform Transfer Learning in NLP tasks . We use the BERT-as-Service, and used the BERT_Base Model as a embeddings extractor. These embeddings can then be fed into a classification head to do text classification. The overall workflow is quite easy to follow.","tags":"NLP","url":"https://haochen23.github.io/2020/02/bert-fine-tune.html","loc":"https://haochen23.github.io/2020/02/bert-fine-tune.html"},{"title":"Semantic Segmentation using FCN and DeepLabV3","text":"Semantic Segmentation using FCN and DeepLabV3 Semantic Segmentation is an image analysis task in which we classify each pixel in the image into a class. In this post, we will perform semantic segmentation using pre-trained models built in Pytorch. They are FCN and DeepLabV3. Understanding model inputs and outputs Now before we get started, we need to know about the inputs and outputs of these semantic segmentation models. So, let's start! These models expect a 3-channled image which is normalized with the Imagenet mean and standard deviation, i.e., mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225] So, the input is [Ni x Ci x Hi x Wi] where, Ni -> the batch size Ci -> the number of channels (which is 3) Hi -> the height of the image Wi -> the width of the image And the output of the model is [No x Co x Ho x Wo] where, No -> is the batch size (same as Ni ) Co -> is the number of classes that the dataset have! So each class will have a map in the output. Ho -> the height of the image (which is the same as Hi in almost all cases) Wo -> the width of the image (which is the same as Wi in almost all cases) Note that torchvision models output is an OrderedDict and not a torch.Tensor And in .eval() mode it just has one key out and thus to get the output we need to get the value stored in that key . The out key of this OrderedDict is the key that holds the output. FCN with Resnet-101 backbone FCN - Fully Convolutional Netowrks, are among the most early invented Neural Networks for the task of Semantic Segmentation. Let's load one up! In [0]: from torchvision import models fcn = models . segmentation . fcn_resnet101 ( pretrained = True ) . eval () Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /root/.cache/torch/checkpoints/fcn_resnet101_coco-7ecb50ca.pth And that's it we have a pretrained model of FCN (which stands for Fully Convolutional Neural Networks) with a Resnet101 backbone :) Now, let's get an image! In [0]: from PIL import Image import matplotlib.pyplot as plt import torch ! wget -nv https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me.jpg -O car.png img = Image . open ( './car.png' ) plt . imshow ( img ) plt . axis ( 'off' ) plt . show () 2020-06-14 05:56:51 URL:https://www.goodfreephotos.com/cache/other-photos/car-and-traffic-on-the-road-coming-towards-me_800.jpg?cached=1522560655 [409997/409997] -> \"car.png\" [1] Now, that we have the image we need to preprocess it and normalize it! These preprocess is following the ImageNet training data. In [0]: # Apply the transformations needed import torchvision.transforms as T trf = T . Compose ([ T . Resize ( 256 ), T . CenterCrop ( 224 ), T . ToTensor (), T . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ])]) inp = trf ( img ) print ( inp . shape ) inp = inp . unsqueeze ( 0 ) print ( inp . shape ) torch.Size([3, 224, 224]) torch.Size([1, 3, 224, 224]) Forward pass the input tensor. The output of the model is a OrderedDict so, we need to take the out key from that to get the output of the model. In [0]: # Pass the input through the net out = fcn ( inp )[ 'out' ] print ( out . shape ) torch.Size([1, 21, 224, 224]) Alright! So, out is the final output of the model. And as we can see, its shape is [1 x 21 x H x W] as discussed earlier. So, the model was trained on 21 classes and thus our output have 21 channels! Since the pretrained FCN segmentation model is trained on PASCAL VOC dataset, which have 20 class labels + 1 background class. Therefore, there are 21 classes in total. Now we need to compress the 21 channels into a single channel which contains all the class labels for each pixel. In [0]: import numpy as np om = torch . argmax ( out . squeeze (), dim = 0 ) . detach () . cpu () . numpy () print ( om . shape ) print ( np . unique ( om )) (224, 224) [0 7] So, we as we can see now have a 2D image. Where each pixel corresponds to a class! The last thing is to take this 2D image where each pixel corresponds to a class label and convert this into a segmentation map where each class label is converted into a RGB color and thus helping in easy visualization. We will use the following function to convert this 2D image to an RGB image wheree each label is mapped to its corresponding color. In [0]: # Define the helper function def decode_segmap ( image , nc = 21 ): label_colors = np . array ([( 0 , 0 , 0 ), # 0=background # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle ( 128 , 0 , 0 ), ( 0 , 128 , 0 ), ( 128 , 128 , 0 ), ( 0 , 0 , 128 ), ( 128 , 0 , 128 ), # 6=bus, 7=car, 8=cat, 9=chair, 10=cow ( 0 , 128 , 128 ), ( 128 , 128 , 128 ), ( 64 , 0 , 0 ), ( 192 , 0 , 0 ), ( 64 , 128 , 0 ), # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person ( 192 , 128 , 0 ), ( 64 , 0 , 128 ), ( 192 , 0 , 128 ), ( 64 , 128 , 128 ), ( 192 , 128 , 128 ), # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor ( 0 , 64 , 0 ), ( 128 , 64 , 0 ), ( 0 , 192 , 0 ), ( 128 , 192 , 0 ), ( 0 , 64 , 128 )]) r = np . zeros_like ( image ) . astype ( np . uint8 ) g = np . zeros_like ( image ) . astype ( np . uint8 ) b = np . zeros_like ( image ) . astype ( np . uint8 ) for l in range ( 0 , nc ): idx = image == l r [ idx ] = label_colors [ l , 0 ] g [ idx ] = label_colors [ l , 1 ] b [ idx ] = label_colors [ l , 2 ] rgb = np . stack ([ r , g , b ], axis = 2 ) return rgb In [0]: rgb = decode_segmap ( om ) plt . imshow ( rgb ); plt . show () Because we are doing semantic segmentation. So the two cars in the middle are assigned to the same class. And that class is car Also, Do note that the image after segmentation is smaller than the original image as in the preprocessing step the image is resized and cropped. Next, let's move all this under one function and play with a few more images! In [0]: def segment ( net , path , show_orig = True , dev = 'cuda' ): img = Image . open ( path ) if show_orig : plt . imshow ( img ); plt . axis ( 'off' ); plt . show () # Comment the Resize and CenterCrop for better inference results trf = T . Compose ([ T . Resize ( 640 ), #T.CenterCrop(224), T . ToTensor (), T . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ])]) inp = trf ( img ) . unsqueeze ( 0 ) . to ( dev ) out = net . to ( dev )( inp )[ 'out' ] om = torch . argmax ( out . squeeze (), dim = 0 ) . detach () . cpu () . numpy () rgb = decode_segmap ( om ) plt . imshow ( rgb ); plt . axis ( 'off' ); plt . show () And let's get a new image! In [0]: ! wget -nv https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Sussex_cow_4.JPG/1200px-Sussex_cow_4.JPG -O cow.jpg segment ( fcn , './cow.jpg' ) 2020-06-14 06:01:04 URL:https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Sussex_cow_4.JPG/1200px-Sussex_cow_4.JPG [223575/223575] -> \"cow.jpg\" [1] In [0]: ! wget -nv https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg -O pedestrian.jpg segment ( fcn , './pedestrian.jpg' ) 2020-06-14 06:03:03 URL:https://storage.needpix.com/rsynced_images/pedestrian-zone-456909_1280.jpg [409534/409534] -> \"pedestrian.jpg\" [1] DeepLabv3 with Resnet-101 backbone In [0]: dlab = models . segmentation . deeplabv3_resnet101 ( pretrained = 1 ) . eval () Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth Alright! Now we have god-level segmentation model! Let's see how we perform with the same image on this model! In [0]: segment ( dlab , './car.png' ) Wow. It looks like DeepLabV3 is able to find the third car which is very dark in the original picture. Let's see how it performs on other pictures. In [0]: segment ( dlab , './cow.jpg' ) In [0]: segment ( dlab , './pedestrian.jpg' ) Comparision For, now we will see how these two models compare with each other in 3 metrics Inference time Size of the model GPU memory used by the model Inference Time In [0]: import time def infer_time ( net , path = './pedestrian.jpg' , dev = 'cuda' ): img = Image . open ( path ) trf = T . Compose ([ T . Resize ( 256 ), T . CenterCrop ( 224 ), T . ToTensor (), T . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ])]) inp = trf ( img ) . unsqueeze ( 0 ) . to ( dev ) st = time . time () out1 = net . to ( dev )( inp ) et = time . time () return et - st On CPU In [0]: avg_over = 100 fcn_infer_time_list_cpu = [ infer_time ( fcn , dev = 'cpu' ) for _ in range ( avg_over )] fcn_infer_time_avg_cpu = sum ( fcn_infer_time_list_cpu ) / avg_over dlab_infer_time_list_cpu = [ infer_time ( dlab , dev = 'cpu' ) for _ in range ( avg_over )] dlab_infer_time_avg_cpu = sum ( dlab_infer_time_list_cpu ) / avg_over print ( 'Inference time for first few calls for FCN : {} ' . format ( fcn_infer_time_list_cpu [: 10 ])) print ( 'Inference time for first few calls for DeepLabv3: {} ' . format ( dlab_infer_time_list_cpu [: 10 ])) print ( 'The Average Inference time on FCN is: {:.2f} s' . format ( fcn_infer_time_avg_cpu )) print ( 'The Average Inference time on DeepLab is: {:.2f} s' . format ( dlab_infer_time_avg_cpu )) Inference time for first few calls for FCN : [1.7313017845153809, 1.57572603225708, 1.5629489421844482, 1.5728049278259277, 1.5671417713165283, 1.5791828632354736, 1.5742602348327637, 1.5786330699920654, 1.5627250671386719, 1.578148365020752] Inference time for first few calls for DeepLabv3: [2.0515308380126953, 1.8824033737182617, 1.8790647983551025, 1.8520596027374268, 1.860480546951294, 1.8530280590057373, 1.837153434753418, 1.8442959785461426, 1.8274402618408203, 1.8295633792877197] The Average Inference time on FCN is: 1.57s The Average Inference time on DeepLab is: 1.85s On GPU In [0]: avg_over = 100 fcn_infer_time_list_gpu = [ infer_time ( fcn ) for _ in range ( avg_over )] fcn_infer_time_avg_gpu = sum ( fcn_infer_time_list_gpu ) / avg_over dlab_infer_time_list_gpu = [ infer_time ( dlab ) for _ in range ( avg_over )] dlab_infer_time_avg_gpu = sum ( dlab_infer_time_list_gpu ) / avg_over print ( 'Inference time for first few calls for FCN : {} ' . format ( fcn_infer_time_list_gpu [: 10 ])) print ( 'Inference time for first few calls for DeepLabv3: {} ' . format ( dlab_infer_time_list_gpu [: 10 ])) print ( 'The Average Inference time on FCN is: {:.3f} s' . format ( fcn_infer_time_avg_gpu )) print ( 'The Average Inference time on DeepLab is: {:.3f} s' . format ( dlab_infer_time_avg_gpu )) Inference time for first few calls for FCN : [0.1053011417388916, 0.017090559005737305, 0.01671576499938965, 0.01747894287109375, 0.018079280853271484, 0.01729726791381836, 0.01781606674194336, 0.024182796478271484, 0.017487287521362305, 0.017383575439453125] Inference time for first few calls for DeepLabv3: [0.12458348274230957, 0.019745349884033203, 0.01925039291381836, 0.018856048583984375, 0.020597457885742188, 0.01930522918701172, 0.01948070526123047, 0.018863677978515625, 0.018346786499023438, 0.019052743911743164] The Average Inference time on FCN is: 0.019s The Average Inference time on DeepLab is: 0.020s We can see that in both cases (for GPU and CPU) its taking longer for the DeepLabv3 model, as its a much deeper model as compared to FCN. In [0]: plt . bar ([ 0.1 , 0.2 ], [ fcn_infer_time_avg_cpu , dlab_infer_time_avg_cpu ], width = 0.08 ) plt . ylabel ( 'Time/s' ) plt . xticks ([ 0.1 , 0.2 ], [ 'FCN' , 'DeepLabv3' ]) plt . title ( 'Inference time of FCN and DeepLabv3 with Resnet-101 backbone on CPU' ) plt . show () In [0]: plt . bar ([ 0.1 , 0.2 ], [ fcn_infer_time_avg_gpu , dlab_infer_time_avg_gpu ], width = 0.08 ) plt . ylabel ( 'Time/s' ) plt . xticks ([ 0.1 , 0.2 ], [ 'FCN' , 'DeepLabv3' ]) plt . title ( 'Inference time of FCN and DeepLabv3 with Resnet-101 backbone on GPU' ) plt . show () Okay! Now, let's move on to the next comparison, where we will compare the model sizes for both the models. Model Size In [0]: import os resnet101_size = os . path . getsize ( '/root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth' ) fcn_size = os . path . getsize ( '/root/.cache/torch/checkpoints/fcn_resnet101_coco-7ecb50ca.pth' ) dlab_size = os . path . getsize ( '/root/.cache/torch/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth' ) fcn_total = fcn_size + resnet101_size dlab_total = dlab_size + resnet101_size print ( 'Size of the FCN model with Resnet101 backbone is: {:.2f} MB' . format ( fcn_total / ( 1024 * 1024 ))) print ( 'Size of the DeepLabv3 model with Resnet101 backbone is: {:.2f} MB' . format ( dlab_total / ( 1024 * 1024 ))) Size of the FCN model with Resnet101 backbone is: 378.16 MB Size of the DeepLabv3 model with Resnet101 backbone is: 403.67 MB In [0]: plt . bar ([ 0 , 1 ], [ fcn_total / ( 1024 * 1024 ), dlab_total / ( 1024 * 1024 )]) plt . ylabel ( 'Size of the model in MegaBytes' ) plt . xticks ([ 0 , 1 ], [ 'FCN' , 'DeepLabv3' ]) plt . title ( 'Comparison of the model size of FCN and DeepLabv3 with Resnet-101 backbone' ) plt . show ()","tags":"Computer Vision","url":"https://haochen23.github.io/2020/02/semantic-segmentation-pytorch.html","loc":"https://haochen23.github.io/2020/02/semantic-segmentation-pytorch.html"},{"title":"Sentiment Analysis using SimpleRNN, LSTM and GRU","text":"Sentiment Analysis using SimpleRNN, LSTM and GRU Intro Recurrent Neural Networks (RNN) are good at processing sequence data for predictions. Therefore, they are extremely useful for deep learning applications like speech recognition, speech synthesis, natural language understanding, etc. Three are three main types of RNNs: SimpleRNN, Long-Short Term Memories (LSTM), and Gated Recurrent Units (GRU). SimpleRNNs are good for processing sequence data for predictions but suffers from short-term memory. LSTM's and GRU's were created as a method to mitigate short-term memory using mechanisms called gates. In this post, I am not going to discuss the details of the theory behinds these RNNs. Instead, I am going to show you how you can actually apply this RNNs to your application. We'll take an example of twitter sentiment analysis. I'm also going to show how to use pre-trained word embeddings in these RNNs. The code I've uploaded to my repo , feel free to use it. Download Dependencies You can use the following code to download the twitter pre-trained embeddings from kaggle (you need install Kaggle API to use the code, otherwise you can use the link ), and download the tweets data. # download and unzip the glove model ! kaggle datasets download fullmetal26 / glovetwitter27b100dtxt ! unzip glovetwitter27b100dtxt . zip # download the tweets data ! wget https : // raw . githubusercontent . com / haochen23 / nlp - rnn - lstm - sentiment / master / training . 1600000. processed . noemoticon . csv In [4]: # import necessary libraries import tensorflow.keras from tensorflow.keras.models import Sequential from tensorflow.keras.layers import InputLayer , Dense , SimpleRNN , Activation , Dropout , Conv1D from tensorflow.keras.layers import Embedding , Flatten , LSTM , GRU from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.callbacks import EarlyStopping import pandas as pd import numpy as np import spacy from sklearn.metrics import classification_report Exploratory Data Analysis Fisrt, let's take a look at the whole dataset. There are 20000 tweets in this dataset, and 6 attributes for each tweet. The dataset is well prepared and there is no missing value in it. In [5]: data = pd . read_csv ( \"training.1600000.processed.noemoticon.csv\" , header = None , encoding = 'latin-1' ) print ( \"The shape of the original dataset is {} \" . format ( data . shape )) data . head () The shape of the original dataset is (20000, 6) Out[5]: 0 1 2 3 4 5 0 0 1467810369 Mon Apr 06 22:19:45 PDT 2009 NO_QUERY _TheSpecialOne_ @switchfoot http://twitpic.com/2y1zl - Awww, t... 1 0 1467810672 Mon Apr 06 22:19:49 PDT 2009 NO_QUERY scotthamilton is upset that he can't update his Facebook by ... 2 0 1467810917 Mon Apr 06 22:19:53 PDT 2009 NO_QUERY mattycus @Kenichan I dived many times for the ball. Man... 3 0 1467811184 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY ElleCTF my whole body feels itchy and like its on fire 4 0 1467811193 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY Karoli @nationwideclass no, it's not behaving at all.... In [8]: # Check missing values data . isnull () . any () Out[8]: 0 False 1 False 2 False 3 False 4 False 5 False dtype: bool Data Preparation Define Util Functions Before we start data preparation, we first define some util functions: load_glove_model load the twitter embeddings model we downloaded. This model is trained on 2 billion tweets, which contains 27 billion tokens, 1.2 million vocabs. remove_stopwords remove the stop words in a sentence lemmatize perform lemmatization on a sentence sent_vectorizer convert a sentence into a vector using the glove_model. This function may be used if we want a different type of input to the RNNs. In [9]: def load_glove_model ( glove_file ): print ( \"[INFO]Loading GloVe Model...\" ) model = {} with open ( glove_file , 'r' ) as f : for line in f : split_line = line . split () word = split_line [ 0 ] embeddings = [ float ( val ) for val in split_line [ 1 :]] model [ word ] = embeddings print ( \"[INFO] Done... {} words loaded!\" . format ( len ( model ))) return model # adopted from utils.py nlp = spacy . load ( \"en\" ) def remove_stopwords ( sentence ): ''' function to remove stopwords input: sentence - string of sentence ''' new = [] # tokenize sentence sentence = nlp ( sentence ) for tk in sentence : if ( tk . is_stop == False ) & ( tk . pos_ != \"PUNCT\" ): new . append ( tk . string . strip ()) # convert back to sentence string c = \" \" . join ( str ( x ) for x in new ) return c def lemmatize ( sentence ): ''' function to do lemmatization input: sentence - string of sentence ''' sentence = nlp ( sentence ) s = \"\" for w in sentence : s += \" \" + w . lemma_ return nlp ( s ) def sent_vectorizer ( sent , model ): ''' sentence vectorizer using the pretrained glove model ''' sent_vector = np . zeros ( 200 ) num_w = 0 for w in sent . split (): try : # add up all token vectors to a sent_vector sent_vector = np . add ( sent_vector , model [ str ( w )]) num_w += 1 except : pass return sent_vector Preparing Data We only care about the tweet text and tweet sentiment information, which stored in the 5th column and 0th column in the dataset. In the sentiment column, 0 represents negative, and 1 represents positive. We organize the data as data_X contains all the tweet text, data_y contains the labels. In [10]: data_X = data [ data . columns [ 5 ]] . to_numpy () data_y = data [ data . columns [ 0 ]] data_y = pd . get_dummies ( data_y ) . to_numpy () The following code cell will convert the tweet text data_X to sequence format that will be feed into RNNs. In [11]: # load the glove model glove_model = load_glove_model ( \"glove.twitter.27B.200d.txt\" ) # number of vocab to keep max_vocab = 18000 # length of sequence that will generate max_len = 15 tokenizer = Tokenizer ( num_words = max_vocab ) tokenizer . fit_on_texts ( data_X ) sequences = tokenizer . texts_to_sequences ( data_X ) word_index = tokenizer . word_index print ( 'Found %s unique tokens.' % len ( word_index )) data_keras = pad_sequences ( sequences , maxlen = max_len , padding = \"post\" ) [INFO]Loading GloVe Model... [INFO] Done...1193514 words loaded! Found 30256 unique tokens. Split Data into Training and Evaluation Sets In [12]: from sklearn.model_selection import train_test_split train_X , valid_X , train_y , valid_y = train_test_split ( data_keras , data_y , test_size = 0.3 , random_state = 42 ) Preparing Word Embeddings using the GloVe Model In [13]: # calcultaete number of words nb_words = len ( tokenizer . word_index ) + 1 # obtain the word embedding matrix embedding_matrix = np . zeros (( nb_words , 200 )) for word , i in word_index . items (): embedding_vector = glove_model . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector print ( 'Null word embeddings: %d ' % np . sum ( np . sum ( embedding_matrix , axis = 1 ) == 0 )) Null word embeddings: 12567 Build RNN Models In [14]: # adopted from sent_tran_eval.py def build_model ( nb_words , rnn_model = \"SimpleRNN\" , embedding_matrix = None ): ''' build_model function: inputs: rnn_model - which type of RNN layer to use, choose in (SimpleRNN, LSTM, GRU) embedding_matrix - whether to use pretrained embeddings or not ''' model = Sequential () # add an embedding layer if embedding_matrix is not None : model . add ( Embedding ( nb_words , 200 , weights = [ embedding_matrix ], input_length = max_len , trainable = False )) else : model . add ( Embedding ( nb_words , 200 , input_length = max_len , trainable = False )) # add an RNN layer according to rnn_model if rnn_model == \"SimpleRNN\" : model . add ( SimpleRNN ( 200 )) elif rnn_model == \"LSTM\" : model . add ( LSTM ( 200 )) else : model . add ( GRU ( 200 )) # model.add(Dense(500,activation='relu')) # model.add(Dense(500, activation='relu')) model . add ( Dense ( 2 , activation = 'softmax' )) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model Training and Evaluation Now we'll train and evaluate the SimpleRNN, LSTM, and GRU networks on our prepared dataset. We are using the pre-trained word embeddings from the glove.twitter.27B.200d.txt data. Using the pre-trained word embeddings as weights for the Embedding layer leads to better results and faster convergence. We set each models to run 20 epochs, but we also set EarlyStopping rules to prevent overfitting. The results of the SimpleRNN, LSTM, GRU models can be seen below. In [15]: model_rnn = build_model ( nb_words , \"SimpleRNN\" , embedding_matrix ) model_rnn . fit ( train_X , train_y , epochs = 20 , batch_size = 120 , validation_data = ( valid_X , valid_y ), callbacks = EarlyStopping ( monitor = 'val_accuracy' , mode = 'max' , patience = 3 )) predictions = model_rnn . predict ( valid_X ) predictions = predictions . argmax ( axis = 1 ) print ( classification_report ( valid_y . argmax ( axis = 1 ), predictions )) Epoch 1/20 117/117 [==============================] - 2s 17ms/step - loss: 0.5889 - accuracy: 0.6806 - val_loss: 0.5029 - val_accuracy: 0.7485 Epoch 2/20 117/117 [==============================] - 2s 13ms/step - loss: 0.4855 - accuracy: 0.7658 - val_loss: 0.4932 - val_accuracy: 0.7533 Epoch 3/20 117/117 [==============================] - 2s 15ms/step - loss: 0.4454 - accuracy: 0.7939 - val_loss: 0.4954 - val_accuracy: 0.7522 Epoch 4/20 117/117 [==============================] - 2s 14ms/step - loss: 0.4117 - accuracy: 0.8104 - val_loss: 0.5041 - val_accuracy: 0.7513 Epoch 5/20 117/117 [==============================] - 2s 14ms/step - loss: 0.3785 - accuracy: 0.8316 - val_loss: 0.5172 - val_accuracy: 0.7483 precision recall f1-score support 0 0.76 0.73 0.74 3016 1 0.74 0.77 0.75 2984 accuracy 0.75 6000 macro avg 0.75 0.75 0.75 6000 weighted avg 0.75 0.75 0.75 6000 In [16]: model_lstm = build_model ( nb_words , \"LSTM\" , embedding_matrix ) model_lstm . fit ( train_X , train_y , epochs = 20 , batch_size = 120 , validation_data = ( valid_X , valid_y ), callbacks = EarlyStopping ( monitor = 'val_accuracy' , mode = 'max' , patience = 3 )) predictions = model_lstm . predict ( valid_X ) predictions = predictions . argmax ( axis = 1 ) print ( classification_report ( valid_y . argmax ( axis = 1 ), predictions )) Epoch 1/20 117/117 [==============================] - 1s 10ms/step - loss: 0.5441 - accuracy: 0.7188 - val_loss: 0.4777 - val_accuracy: 0.7783 Epoch 2/20 117/117 [==============================] - 1s 7ms/step - loss: 0.4764 - accuracy: 0.7664 - val_loss: 0.4651 - val_accuracy: 0.7828 Epoch 3/20 117/117 [==============================] - 1s 6ms/step - loss: 0.4514 - accuracy: 0.7817 - val_loss: 0.4604 - val_accuracy: 0.7802 Epoch 4/20 117/117 [==============================] - 1s 6ms/step - loss: 0.4241 - accuracy: 0.8017 - val_loss: 0.4538 - val_accuracy: 0.7837 Epoch 5/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3923 - accuracy: 0.8206 - val_loss: 0.4442 - val_accuracy: 0.7925 Epoch 6/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3589 - accuracy: 0.8394 - val_loss: 0.4700 - val_accuracy: 0.7943 Epoch 7/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3275 - accuracy: 0.8539 - val_loss: 0.4635 - val_accuracy: 0.7855 Epoch 8/20 117/117 [==============================] - 1s 6ms/step - loss: 0.2867 - accuracy: 0.8772 - val_loss: 0.4842 - val_accuracy: 0.7903 Epoch 9/20 117/117 [==============================] - 1s 6ms/step - loss: 0.2450 - accuracy: 0.8966 - val_loss: 0.5190 - val_accuracy: 0.7822 precision recall f1-score support 0 0.79 0.78 0.78 3016 1 0.78 0.79 0.78 2984 accuracy 0.78 6000 macro avg 0.78 0.78 0.78 6000 weighted avg 0.78 0.78 0.78 6000 In [17]: model_gru = build_model ( nb_words , \"GRU\" , embedding_matrix ) model_gru . fit ( train_X , train_y , epochs = 20 , batch_size = 120 , validation_data = ( valid_X , valid_y ), callbacks = EarlyStopping ( monitor = 'val_accuracy' , mode = 'max' , patience = 3 )) predictions = model_gru . predict ( valid_X ) predictions = predictions . argmax ( axis = 1 ) print ( classification_report ( valid_y . argmax ( axis = 1 ), predictions )) Epoch 1/20 117/117 [==============================] - 1s 10ms/step - loss: 0.5824 - accuracy: 0.6819 - val_loss: 0.4902 - val_accuracy: 0.7640 Epoch 2/20 117/117 [==============================] - 1s 6ms/step - loss: 0.4761 - accuracy: 0.7703 - val_loss: 0.4661 - val_accuracy: 0.7800 Epoch 3/20 117/117 [==============================] - 1s 6ms/step - loss: 0.4483 - accuracy: 0.7876 - val_loss: 0.4689 - val_accuracy: 0.7748 Epoch 4/20 117/117 [==============================] - 1s 6ms/step - loss: 0.4231 - accuracy: 0.8029 - val_loss: 0.4518 - val_accuracy: 0.7877 Epoch 5/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3916 - accuracy: 0.8202 - val_loss: 0.4637 - val_accuracy: 0.7810 Epoch 6/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3650 - accuracy: 0.8358 - val_loss: 0.4784 - val_accuracy: 0.7748 Epoch 7/20 117/117 [==============================] - 1s 6ms/step - loss: 0.3193 - accuracy: 0.8609 - val_loss: 0.4802 - val_accuracy: 0.7863 precision recall f1-score support 0 0.80 0.76 0.78 3016 1 0.77 0.81 0.79 2984 accuracy 0.79 6000 macro avg 0.79 0.79 0.79 6000 weighted avg 0.79 0.79 0.79 6000 As we can see from the above results, even using this small RNN architecture, LSTM and GRU have much better performances than SimpleRNN. It is consistent to general practice. You can also try training the RNNs without the pre-trained word embeddings and see what results you'll get. Summary In this post, we've seen the use of RNNs for sentiment analysis task in NLP. SimpleRNNs are good for processing sequence data for predictions but suffers from short-term memory. LSTMs and GRUs were created as a method to mitigate short-term memory using mechanisms called gates. And they usually perform better than SimpleRNNs. Engineers or practitioners using experiment on both LSTM and GRU to see which one have better results.","tags":"NLP","url":"https://haochen23.github.io/2020/01/nlp-rnn-sentiment.html","loc":"https://haochen23.github.io/2020/01/nlp-rnn-sentiment.html"},{"title":"Twitter Sentiment Modeling on Detecting Racist or Sexist tweets","text":"Twitter Sentiment Modeling on Detecting Racist or Sexist tweets 1. Introduction Twitter is one of the most popular social media nowadays. It provides online news and social networking service on which users post and interact with messages known as \"tweets\". Registered users can post, like, and retweet tweets, but unregistered users can only read them. Sentiment Analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. (Source: Wikipedia) Sentiment analysis on tweets can be extremely useful. By analyzing the tweets, we can find the sentiment of people on certain affair, understand people's opinion. Also it can help us make right strategies/reactions. In this post, our main goal is to build a model to identify tweets with racist or sexist sentiment. 2. The data we use We have a training data of 31,962 tweets with labeled sentiments, the dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet. Label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist. We also have a test data of 17,197 tweets. The test data file contains only tweet ids and the tweet text with each tweet in a new line. The dataset we use can be download from here . 3. Tweets cleaning and preprocessing Load tweets In [1]: # importing libraries import pandas as pd import numpy as np import re import string import nltk import warnings warnings . filterwarnings ( \"ignore\" ) #import plotting libraries import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline # loading training and test tweets train = pd . read_csv ( 'data \\\\ train_E6oV3lV.csv' ) test = pd . read_csv ( 'data \\\\ test_tweets_anuFYb8.csv' ) print ( \"The size of the training tweets is {} \" . format ( train . shape )) print ( \"The size of the test set is {} \" . format ( test . shape )) The size of the training tweets is (31962, 3) The size of the test set is (17197, 2) Let's take a look at the first 5 rows of the training data. In [2]: train . head () Out[2]: id label tweet 0 1 0 @user when a father is dysfunctional and is s... 1 2 0 @user @user thanks for #lyft credit i can't us... 2 3 0 bihday your majesty 3 4 0 #model i love u take with u all the time in ... 4 5 0 factsguide: society now #motivation In [3]: train [ 'label' ] . value_counts () Out[3]: 0 29720 1 2242 Name: label, dtype: int64 We caclulated the number of tweets of each label in our training data. As can be seen, it is quite uneven. We have to take this into account when building our prediction model. The data has 3 columns id , label , and tweet . id is a unique id of each tweet. label is the binary target variable, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist. tweet contains the tweets that we will clean and preprocess. Tweets cleaning After taking a look at the first 5 records, we can have some initial thoughts regards to cleaning our training data: The Twitter handles are already masked as @user due to privacy concerns. So, these Twitter handles are hardly giving any information about the nature of the tweet and they need to be removed. The punctuations, numbers and special characters also need to be removed. Short words, like \"is\", \"are\", \"for\", \"and\" and so on, which do not have much value to the tweets also need to be cleaned. Tokenization. Tokenization is an essential step in any NLP task. Tokenization splits every tweets into individual tokens/words. Stemming/lemmatizing. Reduce similar words (different variations of words of the same root). For example, 'love', 'loving', 'loves' etc. are from the same root 'love'. We can reduce the total number of unique words in our data without losing a significant amount of information. 1. Removing handles (@user) For our convience, let's combine train and test set together to perform cleaning. In [4]: all_tweets = train . append ( test , ignore_index = True ) We define a function _remove pattern to remove unwanted patterns in our tweets data. In our case, the unwanted pattern is '@user'. In [5]: # function to remove unwanted patterns def remove_pattern ( input_txt , pattern ): matches = re . findall ( pattern , input_txt ) for match in matches : input_txt = re . sub ( match , '' , input_txt ) return input_txt Now let's create a new column tidy_tweet , it will contain the cleaned and processed tweets. Note that we have passed \"@[\\w]*\" as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with '@'. In [6]: # remove twitter handles (@username) all_tweets [ 'tidy_tweet' ] = np . vectorize ( remove_pattern )( all_tweets [ 'tweet' ], \"@[\\w]*\" ) 2. Romving punctuations, numbers, and special characters We also use regular expressions to remove punctuations, numbers, and special characters in this step. In [7]: # remove punctuations, numbers, and special characters all_tweets [ 'tidy_tweet' ] = all_tweets [ 'tidy_tweet' ] . str . replace ( \"[&#94;a-zA-Z#]\" , \" \" ) 3. Removing short words Some short words are of little useful information. So we need to remove them from our tweets data. Let's have a look at the data after the three cleaning steps. In [8]: all_tweets [ 'tidy_tweet' ] = all_tweets [ 'tidy_tweet' ] . apply ( lambda x : ' ' . join ([ w for w in x . split () if len ( w ) > 3 ])) all_tweets . head () Out[8]: id label tweet tidy_tweet 0 1 0.0 @user when a father is dysfunctional and is s... when father dysfunctional selfish drags kids i... 1 2 0.0 @user @user thanks for #lyft credit i can't us... thanks #lyft credit cause they offer wheelchai... 2 3 0.0 bihday your majesty bihday your majesty 3 4 0.0 #model i love u take with u all the time in ... #model love take with time 4 5 0.0 factsguide: society now #motivation factsguide society #motivation As can be clearly seen, the tweets in _tidy tweet column are much shorter than the original ones. They only contain important words and the noise(numbers, punctuations, and special characters) has been removed effectively. 4. Tokenization In [9]: # tokenization tokenized_tweet = all_tweets [ 'tidy_tweet' ] . apply ( lambda x : x . split ()) tokenized_tweet . head () Out[9]: 0 [when, father, dysfunctional, selfish, drags, ... 1 [thanks, #lyft, credit, cause, they, offer, wh... 2 [bihday, your, majesty] 3 [#model, love, take, with, time] 4 [factsguide, society, #motivation] Name: tidy_tweet, dtype: object 5. Stemming/Lemmatizing Stemming is a rule-based process of stripping the suffixes (\"ing\", \"ly\", \"es\", \"s\" etc) from a word. In [10]: from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer () tokenized_tweet = tokenized_tweet . apply ( lambda x : [ stemmer . stem ( i ) for i in x ]) tokenized_tweet . head () Out[10]: 0 [when, father, dysfunct, selfish, drag, kid, i... 1 [thank, #lyft, credit, caus, they, offer, whee... 2 [bihday, your, majesti] 3 [#model, love, take, with, time] 4 [factsguid, societi, #motiv] Name: tidy_tweet, dtype: object Now let's stitch these tokens back together. In [11]: for i in range ( len ( tokenized_tweet )): tokenized_tweet [ i ] = ' ' . join ( tokenized_tweet [ i ]) all_tweets [ 'tidy_tweet' ] = tokenized_tweet all_tweets . head () Out[11]: id label tweet tidy_tweet 0 1 0.0 @user when a father is dysfunctional and is s... when father dysfunct selfish drag kid into dys... 1 2 0.0 @user @user thanks for #lyft credit i can't us... thank #lyft credit caus they offer wheelchair ... 2 3 0.0 bihday your majesty bihday your majesti 3 4 0.0 #model i love u take with u all the time in ... #model love take with time 4 5 0.0 factsguide: society now #motivation factsguid societi #motiv 4. Insights and visualization from cleaned tweets A few questions we are gonna ask are: What are the most common words in the entire dataset? What are the most common words in the dataset for negative and positive tweets, respectively? How many hashtags are there in a tweet? Which trends are associated with my dataset? Which trends are associated with either of the sentiments? Are they compatible with the sentiments? 1. Most common words: WordCloud Wordcloud is an useful tool in finding the most common words appreared in the dataset. The most frequent words apprear in larger sizes and the less common words appear in smaller sizes. In [12]: all_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ]]) from wordcloud import WordCloud wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( all_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () We can see most of the words are positive or neutral. With happy , thank , today and love being the most frequent ones. t doesn't give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes(racist/sexist or not) in our train data. 2. Most common words in normal/positive tweets In [13]: normal_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ][ all_tweets [ 'label' ] == 0 ]]) wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( normal_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( 'off' ) plt . show () As expected, the most common words in normal/positive tweets are either positive or neutral. 3. Racist/sexist tweets In [14]: negative_words = ' ' . join ([ text for text in all_tweets [ 'tidy_tweet' ][ all_tweets [ 'label' ] == 1 ]]) wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( negative_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data. 4. Exploring hashtags on tweets sentiment Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. The hashtags may have values in helping us finding negative tweets. We will first store all the hashtags into two separate lists - one for normal/positive tweets and the other for tweets contain racist/sexist. In [15]: # function to collect hashtags def hashtag_extract ( x ): hashtags = [] # loop over the words in the tweet for i in x : ht = re . findall ( r \"#(\\w+)\" , i ) hashtags . append ( ht ) return hashtags In [16]: #extracting hashtags from non racist/sexist tweets HT_regular = hashtag_extract ( all_tweets [ 'tidy_tweet' ] [ all_tweets [ 'label' ] == 0 ]) # extracting hashtags from racist/sexist tweets HT_negative = hashtag_extract ( all_tweets [ 'tidy_tweet' ] [ all_tweets [ 'label' ] == 1 ]) # unnesting list HT_regular = sum ( HT_regular , []) HT_negative = sum ( HT_negative , []) Top hashtags in non-racist/sexist tweets In [17]: a = nltk . FreqDist ( HT_regular ) d = pd . DataFrame ({ 'Hashtag' : list ( a . keys ()), 'Count' : list ( a . values ())}) # selecting top 10 most frequent hashtags d = d . nlargest ( columns = 'Count' , n = 10 ) plt . figure ( figsize = ( 16 , 5 )) ax = sns . barplot ( data = d , x = \"Hashtag\" , y = \"Count\" ) ax . set ( ylabel = 'Count' ) plt . show () Top hastags in racist/sexist tweets In [18]: b = nltk . FreqDist ( HT_negative ) e = pd . DataFrame ({ 'Hashtag' : list ( b . keys ()), 'Count' : list ( b . values ())}) # selecting top 10 most frequent hashtags e = e . nlargest ( columns = \"Count\" , n = 10 ) plt . figure ( figsize = ( 16 , 5 )) ax = sns . barplot ( data = e , x = \"Hashtag\" , y = \"Count\" ) ax . set ( ylabel = 'Count' ) plt . show () As can be seen, the positive/regular hashtags are positive or neutral, and the negative hashtags are really negative. 5. Feature extraction from processed tweets Depending upon the usage, text features can be constructed using assorted techniques – Bag-of-Words, term frequency–inverse document frequency (TF-IDF), and Word Embeddings. In this post, we will be covering only Bag-of-Words and TF-IDF. Bag-of-words features Bag-of-Words is a method to convert text into numerical features. Bag-of-Words features can be easily created using sklearn's CountVectorizer function. We will set the parameter max_features = 1000 to select only top 1000 terms ordered by term frequency across the corpus. In [19]: from sklearn.feature_extraction.text import CountVectorizer bow_vectorizer = CountVectorizer ( max_df = 0.9 , min_df = 2 , max_features = 1000 , stop_words = 'english' ) # bag-of-words feature matrix bow = bow_vectorizer . fit_transform ( all_tweets [ 'tidy_tweet' ]) TF-IDF features Term frequency–inverse document frequency (TF-IDF), takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus. TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents. TF = (number of appearance of a term t)/(number of total terms in the document) IDF = log(N/n), where N is the number of documents and n is the number of documents a term t has appeared in. TF-IDF = TF * IDF In [20]: from sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer = TfidfVectorizer ( max_df = 0.9 , min_df = 2 , max_features = 1000 , stop_words = 'english' ) # TF-IDF feature matrix tfidf = tfidf_vectorizer . fit_transform ( all_tweets [ 'tidy_tweet' ]) 6. Sentiment modeling Since this is an binary prediction, we will use Logistic Regression to build our prediction model. It predicts the probability of occurrence of fitting data into logit function. As mentioned before, the distribution in our training data is quite uneven. So simply using prediction accuracy/precision is not able to correctly evaluate our model's performance. Here, we use F1_score which can be more useful in this situation. This metric can be understood as: Precision = TP/TP+FP Recall = TP/TP+FN F1 Score = 2 (Recall Precision) / (Recall + Precision) Where TP - true positive, FP - false positive, TN - true negative, FN - false negative. Building a model using bag-of-words features In [21]: from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score train_bow = bow [: 31962 , :] test_bow = bow [ 31962 :, :] # splitting the train_bow into training and validation set xtrain_bow , xvalid_bow , ytrain , yvalid = train_test_split ( train_bow , train [ 'label' ], random_state = 42 , test_size = 0.3 ) lreg = LogisticRegression () lreg . fit ( xtrain_bow , ytrain ) # make prediction on validation set prediction = lreg . predict_proba ( xvalid_bow ) prediction_int = prediction [:, 1 ] >= 0.3 #if prediction is greater than or equal to 0.3 than 1 else 0 prediction_int = prediction_int . astype ( np . int ) f1 = f1_score ( yvalid , prediction_int ) #calculate f1_score print ( \"The f1-score for using only the bag-of-words features is : {} \" . format ( f1 )) The f1-score for using only the bag-of-words features is : 0.5307820299500832 In [22]: test_pred = lreg . predict_proba ( test_bow ) test_pred_int = test_pred >= 0.3 test_pred_int = test_pred_int . astype ( np . int ) # test['label'] = test_pred_int # submission = test[['id','label']] # submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file Building a model using TF-IDF features In [23]: train_tfidf = tfidf [: 31962 , :] test_tfidf = tfidf [ 31962 :, :] xtrain_tfidf = train_tfidf [ ytrain . index ] xvalid_tfidf = train_tfidf [ yvalid . index ] lreg . fit ( xtrain_tfidf , ytrain ) prediction = lreg . predict_proba ( xvalid_tfidf ) prediction_int = prediction [:, 1 ] >= 0.3 prediction_int = prediction_int . astype ( np . int ) f1 = f1_score ( yvalid , prediction_int ) print ( \"The f1-score for using only the TF-IDF features is : {} \" . format ( f1 )) The f1-score for using only the TF-IDF features is : 0.5446507515473032 The output is about 0.545 by using the TF-IDF features which is pretty good. The public leader board F1 score is 0.564.","tags":"NLP","url":"https://haochen23.github.io/2019/09/NLP-twitter-sentiment.html","loc":"https://haochen23.github.io/2019/09/NLP-twitter-sentiment.html"},{"title":"Modeling on Amazon Phone and Accessories Reviews using NLP","text":"Topic Modeling on Amazon Phone and Accessories Reviews using NLP Introduction Online shopping now makes our life much easier than it used to be. Without the need of going out and visting a shopping mall or a grocery store, we can buy anything we want through e-shopping. But online shopping comes with its own caveats. One of the biggest challenges, and I guess almost every would face, is verifying the authenticity of a product. Is it as good as advertised on the e-commerce site? Will the product has a good quality? Are the reviews given by other customers really true or are they false advertising, or are they fake or bots' reviews? These are important questions customers need to ask before splurging their money. Importance of online reviews I believe there are a bunch of people like me, who believe in customers' reviews much much more than the fancy product description of an item. In making our decision in buying a product, if most customers' reviews talking about how good it is, then it is probably a really good product. If every customer talks bad about a product, I am afraid I will not buy it no matter how good it shows me in the product description. And in some funny cases, which is quite often from my perspective, some users like to leave a high rating or score no matter how bad they think the product is. Oh god, this is tricky! Identifying the problem Here, an interesting problem comes up. What if the number of reviews is in the hundreds or thousands? It's just not feasible to go through all those reviews, right? And this is where natural language processing (NLP) comes up trumps. But how? Online reviews can be really overwhelming in terms of numbers and information, an intelligent system, capable of finding key insights (topics) from these reviews, will be of great help for the consumers to get an real impression on products. To solve this task, we will use the concept of Topic Modeling (LDA) on Amazon Phone and Accessories Review data. The dataset can be downloaded here . The details of the dataset will be given in the following section during exploratory analysis. Topic modeling Topic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including: Document clustering Organizing large blocks of textual data Text feature selection information retrieval from unstructured text In the case of online product reviews, we intend to extract a certain number of groups of important words from the reviews. These groups of words are basically the topics which would help in ascertaining what the consumers are actually talking about in the reviews. Analyzing the review dataset First, let's import the necessary libraries. In [1]: # import libraries import nltk from nltk import FreqDist import numpy as np import pandas as pd import re import spacy import gensim from gensim import corpora import warnings warnings . filterwarnings ( \"ignore\" ) #libraries for visualization import matplotlib.pyplot as plt # plt.style.use('classic') import seaborn as sns import pyLDAvis import pyLDAvis.gensim % matplotlib inline C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\") Loading data The online review data is stored in a json file. Let's read in the file, and take a look at what's in the dataset. In [2]: # load the dataset df = pd . read_json ( 'data \\\\ Cell_Phones_and_Accessories_5.json' , lines = True ) print ( df . shape ) df . head () (194439, 9) Out[2]: asin helpful overall reviewText reviewTime reviewerID reviewerName summary unixReviewTime 0 120401325X [0, 0] 4 They look good and stick good! I just don't li... 05 21, 2014 A30TL5EWN6DFXT christina Looks Good 1400630400 1 120401325X [0, 0] 5 These stickers work like the review says they ... 01 14, 2014 ASY55RVNIL0UD emily l. Really great product. 1389657600 2 120401325X [0, 0] 5 These are awesome and make my phone look so st... 06 26, 2014 A2TMXE2AFO7ONB Erica LOVE LOVE LOVE 1403740800 3 120401325X [4, 4] 4 Item arrived in great time and was in perfect ... 10 21, 2013 AWJ0WZQYMYFQ4 JM Cute! 1382313600 4 120401325X [2, 3] 5 awesome! stays on, and looks great. can be use... 02 3, 2013 ATX7CZYFXI1KW patrice m rogoza leopard home button sticker for iphone 4s 1359849600 In the Phone and Accessories review dataset, we have nearly 200,000 total reviews and 9 columns. The columns contain the following information: asin - product ID helpful - helpfulness rating of the review, e.g. [19, 19] means 19 out of 19 people think this review is helpful overall - rating of the product (1 - 5 stars) reviewText - the text content of the review reviewTime - tiem of the review (raw) reviewerID - reviewer ID reviewerName - reviewer name summary - summary of the review unixReviewTime - time of the review (unix time) For the scope of our analysis and making it simple, we will be using only the reviewText column. Data cleansing Data cleansing is a important step before NLP analysis. The original text in the reviews data is very complicated. It contains many punctuations, stopwords and useful information etc. In this cleaning and preprocessing step, we should remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, let's define a function that would plot a bar graph of n most frequent words in the data. In [3]: # function to plot most frequent words def freq_words ( x , words = 30 ): all_words = ' ' . join ([ text for text in x ]) all_words = all_words . split () #frequency distribution fdist = FreqDist ( all_words ) words_df = pd . DataFrame ({ 'word' : list ( fdist . keys ()), 'count' : list ( fdist . values ())}) # select the top 30 (default) most frequent words d = words_df . nlargest ( columns = 'count' , n = words ) plt . figure ( figsize = ( 20 , 5 )) ax = sns . barplot ( data = d , x = \"word\" , y = \"count\" ) ax . set ( ylabel = 'Count' ) plt . show () Let's find out the most frequent words in the reviewText column without any preprocessing. In [4]: freq_words ( df [ 'reviewText' ]) As can be easily find, the most common words without any preprocessing are 'the', 'and', 'to', 'a' etc. These words contain little information and are not so important for us to find the key topics. We have to remove these types of words. Before that we also need to remove the punctuations and numbers from the data. In [5]: # remove unwanted characters, numbers and symbols df [ 'reviewText' ] = df [ 'reviewText' ] . str . replace ( \"[&#94;a-zA-Z#]\" , \" \" ) In [6]: # import stopwords library from nltk.corpus import stopwords stop_words = stopwords . words ( 'english' ) # function toremove stopwords and very short words (< 2 letters) # from the review text def remove_stopwords ( rev ): rev_new = \" \" . join ([ i for i in rev if i not in stop_words ]) return rev_new # remove short words df [ 'reviewText' ] = df [ 'reviewText' ] . apply ( lambda x : ' ' . join ([ w for w in x . split () if len ( w ) > 2 ])) # remove stopwords reviews = [ remove_stopwords ( r . split ()) for r in df [ 'reviewText' ]] # lowercase the reviews reviews = [ r . lower () for r in reviews ] After removing the numbers, special characters and the stopwords, let's now look again at the 30 most frequent words. In [7]: freq_words ( reviews ) We observe some improvement in the result. We can find some more useful words in the chart, like \"battery', 'price', 'good', 'product', 'oil' and so on. However, some neutral terms such 'the', 'this', 'they' etc. still stay at the top positions. To further remove noise from the text we can use lemmatization from the spaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word. In [8]: nlp = spacy . load ( 'en' , disable = [ 'parser' , 'ner' ]) # function to filter noun and adjective def lemmatization ( texts , tags = [ 'NOUN' , 'ADJ' ]): output = [] for sent in texts : doc = nlp ( \" \" . join ( sent )) output . append ([ token . lemma_ for token in doc if token . pos_ in tags ]) return output Tokenize and lemmatize the reviews In [9]: # tokenization tokenized_reviews = pd . Series ( reviews ) . apply ( lambda x : x . split ()) # an example after tokenization print ( tokenized_reviews [ 2 ]) ['these', 'awesome', 'make', 'phone', 'look', 'stylish', 'used', 'one', 'far', 'almost', 'year', 'can', 'you', 'believe', 'that', 'one', 'year', 'great', 'quality'] In [10]: # lemmatization reviews_2 = lemmatization ( tokenized_reviews ) #the above result followed by lemmatization print ( reviews_2 [ 2 ]) ['awesome', 'phone', 'stylish', 'year', 'year', 'great', 'quality'] As we can see, we have not only lemmatized the words in the reivews but also filtered out nouns and adjectives. Let's de-tokenize the lemmatized reviews and plot the most common words. In [11]: reviews_3 = [] for i in range ( len ( reviews_2 )): reviews_3 . append ( ' ' . join ( reviews_2 [ i ])) df [ 'reviews' ] = reviews_3 freq_words ( df [ 'reviews' ]) From the above chart of the top 30 words, we can see they all contain relevant information we are interested in. Now, let's move on and start building our topic model. Let's then visualize the frequent words using WordCloud. A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes. In [12]: all_words = ' ' . join ([ text for text in df [ 'reviews' ]]) from wordcloud import WordCloud wordcloud = WordCloud ( width = 800 , height = 500 , random_state = 21 , max_font_size = 110 ) . generate ( all_words ) plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( 'off' ) plt . show () Create the model Building the term dictionary of our corpus Then we will convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above. In [13]: dictionary = corpora . Dictionary ( reviews_2 ) doc_term_matrix = [ dictionary . doc2bow ( rev ) for rev in reviews_2 ] Creating a Latent Dirichlet Allocation (LDA) model We specify the model to find the num_topics = 5 topics. In [14]: # Creating the LDA model using gensim LDA = gensim . models . ldamodel . LdaModel lda_model = LDA ( corpus = doc_term_matrix , id2word = dictionary , num_topics = 5 , random_state = 100 , chunksize = 1000 , passes = 50 ) Let's print out our 5 topics that the LDA model has learned. In [15]: lda_model . print_topics () Out[15]: [(0, '0.056*\"phone\" + 0.014*\"good\" + 0.011*\"app\" + 0.010*\"day\" + 0.010*\"time\" + 0.009*\"screen\" + 0.008*\"camera\" + 0.008*\"battery\" + 0.007*\"new\" + 0.007*\"great\"'), (1, '0.095*\"case\" + 0.046*\"phone\" + 0.027*\"screen\" + 0.019*\"good\" + 0.018*\"protector\" + 0.016*\"great\" + 0.012*\"protection\" + 0.011*\"color\" + 0.010*\"nice\" + 0.010*\"iphone\"'), (2, '0.022*\"phone\" + 0.013*\"use\" + 0.010*\"place\" + 0.009*\"car\" + 0.009*\"mount\" + 0.009*\"pad\" + 0.009*\"note\" + 0.008*\"tip\" + 0.008*\"small\" + 0.008*\"holder\"'), (3, '0.037*\"charger\" + 0.036*\"battery\" + 0.036*\"charge\" + 0.030*\"device\" + 0.027*\"cable\" + 0.025*\"usb\" + 0.022*\"power\" + 0.021*\"port\" + 0.013*\"phone\" + 0.012*\"time\"'), (4, '0.039*\"speaker\" + 0.026*\"bluetooth\" + 0.024*\"sound\" + 0.023*\"good\" + 0.016*\"quality\" + 0.015*\"headphone\" + 0.015*\"music\" + 0.014*\"button\" + 0.013*\"volume\" + 0.012*\"ear\"')] From the first topic, topic index number 0, we can see terms like \"phone\", \"good\", \"app\", \"screen\", \"camera\" and so on, indciating that the topic is very much related to hardware quality of cellphones. From the second topic, which is Topic 1 , it might be raleted to accessories description of cellphones. Topics visulaization To visualize our topics in a 2-dimensional space we will use the pyLDAvis library. This visualization is interactive in nature and displays topics along with the most relevant words. In [16]: # Visualize the topics pyLDAvis . enable_notebook () vis = pyLDAvis . gensim . prepare ( lda_model , doc_term_matrix , dictionary ) vis Out[16]: Summary From the above visualization, we can clearly see the most relevant of words in each topic learned by our model. By assigning a lager number to the _num topics in our model training, we can find much more specific topics on the online reviews. Topic modeling help us finding the most important topics in a large text review dataset. We can easily get an overview of all the reviews and find useful information for us to make purchasing desicions.","tags":"NLP","url":"https://haochen23.github.io/2019/08/NLP-topic-modeling.html","loc":"https://haochen23.github.io/2019/08/NLP-topic-modeling.html"},{"title":"Transfer Learning with Pytorch","text":"Transfer Learning with Pytorch In this post, we'll explore how to perform transfer learning using Pytorch. We will use a subset of Food-11k that contains 11 different kinds of food categories. We will go over the dataset preparation, data augmentation and then steps to build the classifier. We use transfer learning to use the low level image features like edges, textures etc. learnt by a pretrained model, ResNet50, and then train our classifier to learn the higher level details in our dataset images. ResNet50 has already been trained on ImageNet with millions of images. The original Food-11k dataset constains about 11k images of 11 categories of foods. Training on the whole dataset will take hours. Hence, we are going to use a subset of this dataset. The 10 food categories include Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles, Rice, Seafood, Soup, Vegetable . I've prepared the sub-dataset into train, valid, test set. In the train set, there are 10 folders for the 11 kinds of food, and each folder contains 100 images for a particular kind of food. valid and test set follow the same structure, but with 20 and 40 images per category respectively. So finally, we have 1100 training images, 220 validation images, and 440 test images in 10 classes of foods. In [0]: # import libraries import torch from torchvision import models , datasets from torchvision import transforms from torch import nn , optim from torch.utils.data.dataloader import DataLoader import time import numpy as np import matplotlib.pyplot as plt import os from PIL import Image Data Augmentations The images in the available training set can be modified in a number of ways to incorporate more variations in the training process, so that the trained model gets more generalized and performs well on different kinds of test data. Also the input data can come in a variety of sizes. They need to be normalized to a fixed size and format before batches of data are used together for training. Let us go over the transformations we used for our data augmentation. The transform RandomResizedCrop crops the input image by a random size(within a scale range of 0.8 to 1.0 of the original size and a random aspect ratio in the default range of 0.75 to 1.33 ). The crop is then resized to 256×256. RandomRotation rotates the image by an angle randomly chosen between -15 to 15 degrees. RandomHorizontalFlip randomly flips the image horizontally with a default probability of 50%. CenterCrop crops an 224×224 image from the center. ToTensor converts the PIL Image which has values in the range of 0-255 to a floating point Tensor and normalizes them to a range of 0-1, by dividing it by 255. Normalize takes in a 3 channel Tensor and normalizes each channel by the input mean and standard deviation for the channel. Mean and standard deviation vectors are input as 3 element vectors. Each channel in the tensor is normalized as T = (T – mean)/(standard deviation) In [0]: # applying transforms to the data image_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( size = 256 , scale = ( 0.8 , 1.0 )), transforms . RandomRotation ( degrees = 15 ), transforms . RandomHorizontalFlip (), transforms . CenterCrop ( size = 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'valid' : transforms . Compose ([ transforms . Resize ( size = 256 ), transforms . CenterCrop ( size = 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'test' : transforms . Compose ([ transforms . Resize ( size = 256 ), transforms . CenterCrop ( size = 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) } Note that for the validation and test data, we do not do the RandomResizedCrop , RandomRotation and RandomHorizontalFlip transformations. Because they are used for testing model performance. In [53]: # Load data # Set train, valid, and test directory train_directory = 'food-11k-sub/train' valid_directory = 'food-11k-sub/valid' test_directory = 'food-11k-sub/test' # batch size bs = 32 # number of epochs epochs = 20 # number of classes num_classes = 11 # device device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Load data from directory data = { 'train' : datasets . ImageFolder ( root = train_directory , transform = image_transforms [ 'train' ]), 'valid' : datasets . ImageFolder ( root = valid_directory , transform = image_transforms [ 'valid' ]), 'test' : datasets . ImageFolder ( root = test_directory , transform = image_transforms [ 'test' ]) } # Get a mapping of the indices to the class names, in order to see the output classes of the test images. idx_to_class = { v : k for k , v in data [ 'train' ] . class_to_idx . items ()} print ( idx_to_class ) # size of data, to be used for calculating Averge Loss and Accuracy train_data_size = len ( data [ 'train' ]) valid_data_size = len ( data [ 'valid' ]) test_data_size = len ( data [ 'test' ]) # Create iterators for the Data loaded using DataLoader module train_data = DataLoader ( data [ 'train' ], batch_size = bs , shuffle = True ) valid_data = DataLoader ( data [ 'valid' ], batch_size = bs , shuffle = True ) test_data = DataLoader ( data [ 'test' ], batch_size = bs , shuffle = True ) train_data_size , valid_data_size , test_data_size {0: 'Bread', 1: 'Dairy product', 2: 'Dessert', 3: 'Egg', 4: 'Fried food', 5: 'Meat', 6: 'Noodles', 7: 'Rice', 8: 'Seafood', 9: 'Soup', 10: 'Vegetable'} Out[53]: (1100, 220, 440) The torchvision.transforms package and the DataLoader are very important PyTorch features that makes the data augmentation and loading process very easy. Transfer Learning We are going to use the Resnet50 as the base model. It is one of the best performant models in terms of model size, inference speed, and prediction accuracy. First we load the pretrained Resnet50. Then we freeze the model parameters of the convolutional layers (as a feature extractor). Because we are doing transfer learning. In [0]: # load pretrained resnet50 resnet_50 = models . resnet50 ( pretrained = True ) # Freeze model parameters, coz we are fine-tuning for param in resnet_50 . parameters (): param . requires_grad = False Then we replace the final layer of the ResNet50 model by a small set of Sequential layers. The inputs to the last fully connected layer of ResNet50 is fed to a Linear layer (Dense) which has 256 outputs, which are then fed into ReLU and Dropout layers. It is then followed by a 256×11 Linear Layer which has 11 outputs corresponding to the 11 classes. In [0]: # change the final layer of Resnet50 Model for fine-tuning fc_inputs = resnet_50 . fc . in_features resnet_50 . fc = nn . Sequential ( nn . Linear ( fc_inputs , 256 ), nn . ReLU (), nn . Dropout ( 0.4 ), nn . Linear ( 256 , 11 ), nn . LogSoftmax ( dim = 1 ) # for using NLLLoss() ) # convert model to GPU resnet_50 = resnet_50 . to ( device ) # define optimizer and loss function loss_func = nn . NLLLoss () optimizer = optim . Adam ( resnet_50 . parameters ()) Start training Now the entire model has been set up, let's start training. First, take a look at the summary of the model. Since we freezed the layers of the base model, Trainable parameters are only 527,371 for the added layers. In [56]: from torchsummary import summary summary ( resnet_50 , input_size = ( 3 , 224 , 224 )) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 112, 112] 9,408 BatchNorm2d-2 [-1, 64, 112, 112] 128 ReLU-3 [-1, 64, 112, 112] 0 MaxPool2d-4 [-1, 64, 56, 56] 0 Conv2d-5 [-1, 64, 56, 56] 4,096 BatchNorm2d-6 [-1, 64, 56, 56] 128 ReLU-7 [-1, 64, 56, 56] 0 Conv2d-8 [-1, 64, 56, 56] 36,864 BatchNorm2d-9 [-1, 64, 56, 56] 128 ReLU-10 [-1, 64, 56, 56] 0 Conv2d-11 [-1, 256, 56, 56] 16,384 BatchNorm2d-12 [-1, 256, 56, 56] 512 Conv2d-13 [-1, 256, 56, 56] 16,384 BatchNorm2d-14 [-1, 256, 56, 56] 512 ReLU-15 [-1, 256, 56, 56] 0 Bottleneck-16 [-1, 256, 56, 56] 0 Conv2d-17 [-1, 64, 56, 56] 16,384 BatchNorm2d-18 [-1, 64, 56, 56] 128 ReLU-19 [-1, 64, 56, 56] 0 Conv2d-20 [-1, 64, 56, 56] 36,864 BatchNorm2d-21 [-1, 64, 56, 56] 128 ReLU-22 [-1, 64, 56, 56] 0 Conv2d-23 [-1, 256, 56, 56] 16,384 BatchNorm2d-24 [-1, 256, 56, 56] 512 ReLU-25 [-1, 256, 56, 56] 0 Bottleneck-26 [-1, 256, 56, 56] 0 Conv2d-27 [-1, 64, 56, 56] 16,384 BatchNorm2d-28 [-1, 64, 56, 56] 128 ReLU-29 [-1, 64, 56, 56] 0 Conv2d-30 [-1, 64, 56, 56] 36,864 BatchNorm2d-31 [-1, 64, 56, 56] 128 ReLU-32 [-1, 64, 56, 56] 0 Conv2d-33 [-1, 256, 56, 56] 16,384 BatchNorm2d-34 [-1, 256, 56, 56] 512 ReLU-35 [-1, 256, 56, 56] 0 Bottleneck-36 [-1, 256, 56, 56] 0 Conv2d-37 [-1, 128, 56, 56] 32,768 BatchNorm2d-38 [-1, 128, 56, 56] 256 ReLU-39 [-1, 128, 56, 56] 0 Conv2d-40 [-1, 128, 28, 28] 147,456 BatchNorm2d-41 [-1, 128, 28, 28] 256 ReLU-42 [-1, 128, 28, 28] 0 Conv2d-43 [-1, 512, 28, 28] 65,536 BatchNorm2d-44 [-1, 512, 28, 28] 1,024 Conv2d-45 [-1, 512, 28, 28] 131,072 BatchNorm2d-46 [-1, 512, 28, 28] 1,024 ReLU-47 [-1, 512, 28, 28] 0 Bottleneck-48 [-1, 512, 28, 28] 0 Conv2d-49 [-1, 128, 28, 28] 65,536 BatchNorm2d-50 [-1, 128, 28, 28] 256 ReLU-51 [-1, 128, 28, 28] 0 Conv2d-52 [-1, 128, 28, 28] 147,456 BatchNorm2d-53 [-1, 128, 28, 28] 256 ReLU-54 [-1, 128, 28, 28] 0 Conv2d-55 [-1, 512, 28, 28] 65,536 BatchNorm2d-56 [-1, 512, 28, 28] 1,024 ReLU-57 [-1, 512, 28, 28] 0 Bottleneck-58 [-1, 512, 28, 28] 0 Conv2d-59 [-1, 128, 28, 28] 65,536 BatchNorm2d-60 [-1, 128, 28, 28] 256 ReLU-61 [-1, 128, 28, 28] 0 Conv2d-62 [-1, 128, 28, 28] 147,456 BatchNorm2d-63 [-1, 128, 28, 28] 256 ReLU-64 [-1, 128, 28, 28] 0 Conv2d-65 [-1, 512, 28, 28] 65,536 BatchNorm2d-66 [-1, 512, 28, 28] 1,024 ReLU-67 [-1, 512, 28, 28] 0 Bottleneck-68 [-1, 512, 28, 28] 0 Conv2d-69 [-1, 128, 28, 28] 65,536 BatchNorm2d-70 [-1, 128, 28, 28] 256 ReLU-71 [-1, 128, 28, 28] 0 Conv2d-72 [-1, 128, 28, 28] 147,456 BatchNorm2d-73 [-1, 128, 28, 28] 256 ReLU-74 [-1, 128, 28, 28] 0 Conv2d-75 [-1, 512, 28, 28] 65,536 BatchNorm2d-76 [-1, 512, 28, 28] 1,024 ReLU-77 [-1, 512, 28, 28] 0 Bottleneck-78 [-1, 512, 28, 28] 0 Conv2d-79 [-1, 256, 28, 28] 131,072 BatchNorm2d-80 [-1, 256, 28, 28] 512 ReLU-81 [-1, 256, 28, 28] 0 Conv2d-82 [-1, 256, 14, 14] 589,824 BatchNorm2d-83 [-1, 256, 14, 14] 512 ReLU-84 [-1, 256, 14, 14] 0 Conv2d-85 [-1, 1024, 14, 14] 262,144 BatchNorm2d-86 [-1, 1024, 14, 14] 2,048 Conv2d-87 [-1, 1024, 14, 14] 524,288 BatchNorm2d-88 [-1, 1024, 14, 14] 2,048 ReLU-89 [-1, 1024, 14, 14] 0 Bottleneck-90 [-1, 1024, 14, 14] 0 Conv2d-91 [-1, 256, 14, 14] 262,144 BatchNorm2d-92 [-1, 256, 14, 14] 512 ReLU-93 [-1, 256, 14, 14] 0 Conv2d-94 [-1, 256, 14, 14] 589,824 BatchNorm2d-95 [-1, 256, 14, 14] 512 ReLU-96 [-1, 256, 14, 14] 0 Conv2d-97 [-1, 1024, 14, 14] 262,144 BatchNorm2d-98 [-1, 1024, 14, 14] 2,048 ReLU-99 [-1, 1024, 14, 14] 0 Bottleneck-100 [-1, 1024, 14, 14] 0 Conv2d-101 [-1, 256, 14, 14] 262,144 BatchNorm2d-102 [-1, 256, 14, 14] 512 ReLU-103 [-1, 256, 14, 14] 0 Conv2d-104 [-1, 256, 14, 14] 589,824 BatchNorm2d-105 [-1, 256, 14, 14] 512 ReLU-106 [-1, 256, 14, 14] 0 Conv2d-107 [-1, 1024, 14, 14] 262,144 BatchNorm2d-108 [-1, 1024, 14, 14] 2,048 ReLU-109 [-1, 1024, 14, 14] 0 Bottleneck-110 [-1, 1024, 14, 14] 0 Conv2d-111 [-1, 256, 14, 14] 262,144 BatchNorm2d-112 [-1, 256, 14, 14] 512 ReLU-113 [-1, 256, 14, 14] 0 Conv2d-114 [-1, 256, 14, 14] 589,824 BatchNorm2d-115 [-1, 256, 14, 14] 512 ReLU-116 [-1, 256, 14, 14] 0 Conv2d-117 [-1, 1024, 14, 14] 262,144 BatchNorm2d-118 [-1, 1024, 14, 14] 2,048 ReLU-119 [-1, 1024, 14, 14] 0 Bottleneck-120 [-1, 1024, 14, 14] 0 Conv2d-121 [-1, 256, 14, 14] 262,144 BatchNorm2d-122 [-1, 256, 14, 14] 512 ReLU-123 [-1, 256, 14, 14] 0 Conv2d-124 [-1, 256, 14, 14] 589,824 BatchNorm2d-125 [-1, 256, 14, 14] 512 ReLU-126 [-1, 256, 14, 14] 0 Conv2d-127 [-1, 1024, 14, 14] 262,144 BatchNorm2d-128 [-1, 1024, 14, 14] 2,048 ReLU-129 [-1, 1024, 14, 14] 0 Bottleneck-130 [-1, 1024, 14, 14] 0 Conv2d-131 [-1, 256, 14, 14] 262,144 BatchNorm2d-132 [-1, 256, 14, 14] 512 ReLU-133 [-1, 256, 14, 14] 0 Conv2d-134 [-1, 256, 14, 14] 589,824 BatchNorm2d-135 [-1, 256, 14, 14] 512 ReLU-136 [-1, 256, 14, 14] 0 Conv2d-137 [-1, 1024, 14, 14] 262,144 BatchNorm2d-138 [-1, 1024, 14, 14] 2,048 ReLU-139 [-1, 1024, 14, 14] 0 Bottleneck-140 [-1, 1024, 14, 14] 0 Conv2d-141 [-1, 512, 14, 14] 524,288 BatchNorm2d-142 [-1, 512, 14, 14] 1,024 ReLU-143 [-1, 512, 14, 14] 0 Conv2d-144 [-1, 512, 7, 7] 2,359,296 BatchNorm2d-145 [-1, 512, 7, 7] 1,024 ReLU-146 [-1, 512, 7, 7] 0 Conv2d-147 [-1, 2048, 7, 7] 1,048,576 BatchNorm2d-148 [-1, 2048, 7, 7] 4,096 Conv2d-149 [-1, 2048, 7, 7] 2,097,152 BatchNorm2d-150 [-1, 2048, 7, 7] 4,096 ReLU-151 [-1, 2048, 7, 7] 0 Bottleneck-152 [-1, 2048, 7, 7] 0 Conv2d-153 [-1, 512, 7, 7] 1,048,576 BatchNorm2d-154 [-1, 512, 7, 7] 1,024 ReLU-155 [-1, 512, 7, 7] 0 Conv2d-156 [-1, 512, 7, 7] 2,359,296 BatchNorm2d-157 [-1, 512, 7, 7] 1,024 ReLU-158 [-1, 512, 7, 7] 0 Conv2d-159 [-1, 2048, 7, 7] 1,048,576 BatchNorm2d-160 [-1, 2048, 7, 7] 4,096 ReLU-161 [-1, 2048, 7, 7] 0 Bottleneck-162 [-1, 2048, 7, 7] 0 Conv2d-163 [-1, 512, 7, 7] 1,048,576 BatchNorm2d-164 [-1, 512, 7, 7] 1,024 ReLU-165 [-1, 512, 7, 7] 0 Conv2d-166 [-1, 512, 7, 7] 2,359,296 BatchNorm2d-167 [-1, 512, 7, 7] 1,024 ReLU-168 [-1, 512, 7, 7] 0 Conv2d-169 [-1, 2048, 7, 7] 1,048,576 BatchNorm2d-170 [-1, 2048, 7, 7] 4,096 ReLU-171 [-1, 2048, 7, 7] 0 Bottleneck-172 [-1, 2048, 7, 7] 0 AdaptiveAvgPool2d-173 [-1, 2048, 1, 1] 0 Linear-174 [-1, 256] 524,544 ReLU-175 [-1, 256] 0 Dropout-176 [-1, 256] 0 Linear-177 [-1, 11] 2,827 LogSoftmax-178 [-1, 11] 0 ================================================================ Total params: 24,035,403 Trainable params: 527,371 Non-trainable params: 23,508,032 ---------------------------------------------------------------- Input size (MB): 0.57 Forward/backward pass size (MB): 286.56 Params size (MB): 91.69 Estimated Total Size (MB): 378.82 ---------------------------------------------------------------- In [0]: def train_and_validate ( model , loss_criterion , optimizer , epochs = 25 ): ''' Function to train and validate Parameters :param model: Model to train and validate :param loss_criterion: Loss Criterion to minimize :param optimizer: Optimizer for computing gradients :param epochs: Number of epochs (default=25) Returns model: Trained Model with best validation accuracy history: (dict object): Having training loss, accuracy and validation loss, accuracy ''' start = time . time () history = [] best_acc = 0.0 for epoch in range ( epochs ): epoch_start = time . time () print ( \"Epoch: {} / {} \" . format ( epoch + 1 , epochs )) # Set to training mode model . train () # Loss and Accuracy within the epoch train_loss = 0.0 train_acc = 0.0 valid_loss = 0.0 valid_acc = 0.0 for i , ( inputs , labels ) in enumerate ( train_data ): inputs = inputs . to ( device ) labels = labels . to ( device ) # Clean existing gradients optimizer . zero_grad () # Forward pass - compute outputs on input data using the model outputs = model ( inputs ) # Compute loss loss = loss_criterion ( outputs , labels ) # Backpropagate the gradients loss . backward () # Update the parameters optimizer . step () # Compute the total loss for the batch and add it to train_loss train_loss += loss . item () * inputs . size ( 0 ) # Compute the accuracy ret , predictions = torch . max ( outputs . data , 1 ) correct_counts = predictions . eq ( labels . data . view_as ( predictions )) # Convert correct_counts to float and then compute the mean acc = torch . mean ( correct_counts . type ( torch . FloatTensor )) # Compute total accuracy in the whole batch and add to train_acc train_acc += acc . item () * inputs . size ( 0 ) #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item())) # Validation - No gradient tracking needed with torch . no_grad (): # Set to evaluation mode model . eval () # Validation loop for j , ( inputs , labels ) in enumerate ( valid_data ): inputs = inputs . to ( device ) labels = labels . to ( device ) # Forward pass - compute outputs on input data using the model outputs = model ( inputs ) # Compute loss loss = loss_criterion ( outputs , labels ) # Compute the total loss for the batch and add it to valid_loss valid_loss += loss . item () * inputs . size ( 0 ) # Calculate validation accuracy ret , predictions = torch . max ( outputs . data , 1 ) correct_counts = predictions . eq ( labels . data . view_as ( predictions )) # Convert correct_counts to float and then compute the mean acc = torch . mean ( correct_counts . type ( torch . FloatTensor )) # Compute total accuracy in the whole batch and add to valid_acc valid_acc += acc . item () * inputs . size ( 0 ) #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item())) # Find average training loss and training accuracy avg_train_loss = train_loss / train_data_size avg_train_acc = train_acc / train_data_size # Find average training loss and training accuracy avg_valid_loss = valid_loss / valid_data_size avg_valid_acc = valid_acc / valid_data_size history . append ([ avg_train_loss , avg_valid_loss , avg_train_acc , avg_valid_acc ]) epoch_end = time . time () print ( \"Epoch : {:03d} , Training: Loss: {:.4f} , Accuracy: {:.4f} %, \\n\\t\\t Validation : Loss : {:.4f} , Accuracy: {:.4f} %, Time: {:.4f} s\" . format ( epoch + 1 , avg_train_loss , avg_train_acc * 100 , avg_valid_loss , avg_valid_acc * 100 , epoch_end - epoch_start )) # Save if the model has best accuracy till now # torch.save(model, 'model_'+str(epoch)+'.pt') return model , history In [58]: num_epochs = 25 trained_model , history = train_and_validate ( resnet_50 , loss_func , optimizer , num_epochs ) torch . save ( history , 'history.pt' ) Epoch: 1/25 Epoch : 001, Training: Loss: 2.0360, Accuracy: 32.7273%, Validation : Loss : 1.3534, Accuracy: 65.0000%, Time: 15.9276s Epoch: 2/25 Epoch : 002, Training: Loss: 1.3181, Accuracy: 59.7273%, Validation : Loss : 0.9446, Accuracy: 73.6364%, Time: 15.8867s Epoch: 3/25 Epoch : 003, Training: Loss: 1.0782, Accuracy: 65.2727%, Validation : Loss : 0.8406, Accuracy: 76.8182%, Time: 15.8777s Epoch: 4/25 Epoch : 004, Training: Loss: 0.9117, Accuracy: 72.0909%, Validation : Loss : 0.7171, Accuracy: 79.0909%, Time: 16.2870s Epoch: 5/25 Epoch : 005, Training: Loss: 0.8877, Accuracy: 72.6364%, Validation : Loss : 0.7464, Accuracy: 77.7273%, Time: 15.8715s Epoch: 6/25 Epoch : 006, Training: Loss: 0.8385, Accuracy: 72.9091%, Validation : Loss : 0.7248, Accuracy: 75.9091%, Time: 15.8559s Epoch: 7/25 Epoch : 007, Training: Loss: 0.7604, Accuracy: 74.7273%, Validation : Loss : 0.6933, Accuracy: 79.5455%, Time: 15.8478s Epoch: 8/25 Epoch : 008, Training: Loss: 0.7379, Accuracy: 76.8182%, Validation : Loss : 0.5714, Accuracy: 85.0000%, Time: 15.9050s Epoch: 9/25 Epoch : 009, Training: Loss: 0.6839, Accuracy: 78.4545%, Validation : Loss : 0.5900, Accuracy: 83.1818%, Time: 15.9014s Epoch: 10/25 Epoch : 010, Training: Loss: 0.6969, Accuracy: 76.5455%, Validation : Loss : 0.6262, Accuracy: 80.0000%, Time: 15.9232s Epoch: 11/25 Epoch : 011, Training: Loss: 0.5962, Accuracy: 80.0909%, Validation : Loss : 0.5857, Accuracy: 79.5455%, Time: 15.8562s Epoch: 12/25 Epoch : 012, Training: Loss: 0.5672, Accuracy: 81.7273%, Validation : Loss : 0.5673, Accuracy: 80.4545%, Time: 15.8696s Epoch: 13/25 Epoch : 013, Training: Loss: 0.6247, Accuracy: 80.5455%, Validation : Loss : 0.5577, Accuracy: 81.3636%, Time: 15.8772s Epoch: 14/25 Epoch : 014, Training: Loss: 0.5686, Accuracy: 81.8182%, Validation : Loss : 0.5743, Accuracy: 80.9091%, Time: 15.8630s Epoch: 15/25 Epoch : 015, Training: Loss: 0.5647, Accuracy: 80.7273%, Validation : Loss : 0.5236, Accuracy: 84.0909%, Time: 15.9140s Epoch: 16/25 Epoch : 016, Training: Loss: 0.5239, Accuracy: 82.0000%, Validation : Loss : 0.6071, Accuracy: 79.5455%, Time: 15.9032s Epoch: 17/25 Epoch : 017, Training: Loss: 0.5532, Accuracy: 81.3636%, Validation : Loss : 0.5373, Accuracy: 84.0909%, Time: 15.8482s Epoch: 18/25 Epoch : 018, Training: Loss: 0.5236, Accuracy: 82.9091%, Validation : Loss : 0.5534, Accuracy: 82.7273%, Time: 15.8996s Epoch: 19/25 Epoch : 019, Training: Loss: 0.4936, Accuracy: 83.0000%, Validation : Loss : 0.5223, Accuracy: 82.2727%, Time: 16.0922s Epoch: 20/25 Epoch : 020, Training: Loss: 0.5198, Accuracy: 82.2727%, Validation : Loss : 0.4858, Accuracy: 84.5455%, Time: 15.8731s Epoch: 21/25 Epoch : 021, Training: Loss: 0.4850, Accuracy: 82.6364%, Validation : Loss : 0.6114, Accuracy: 78.6364%, Time: 15.8514s Epoch: 22/25 Epoch : 022, Training: Loss: 0.5058, Accuracy: 83.3636%, Validation : Loss : 0.5244, Accuracy: 82.7273%, Time: 15.9262s Epoch: 23/25 Epoch : 023, Training: Loss: 0.4296, Accuracy: 86.1818%, Validation : Loss : 0.6065, Accuracy: 78.1818%, Time: 15.9853s Epoch: 24/25 Epoch : 024, Training: Loss: 0.4808, Accuracy: 83.4545%, Validation : Loss : 0.5916, Accuracy: 81.8182%, Time: 16.1919s Epoch: 25/25 Epoch : 025, Training: Loss: 0.5062, Accuracy: 82.8182%, Validation : Loss : 0.5311, Accuracy: 80.0000%, Time: 15.8669s In [0]: torch . save ( trained_model , 'trained_model.pt' ) In [61]: history = np . array ( history ) plt . plot ( history [:, 0 : 2 ]) plt . legend ([ 'Tr Loss' , 'Val Loss' ]) plt . xlabel ( 'Epoch Number' ) plt . ylabel ( 'Loss' ) plt . ylim ( 0 , 1 ) plt . savefig ( 'loss_curve.png' ) plt . show () In [62]: plt . plot ( history [:, 2 : 4 ]) plt . legend ([ 'Tr Accuracy' , 'Val Accuracy' ]) plt . xlabel ( 'Epoch Number' ) plt . ylabel ( 'Accuracy' ) plt . ylim ( 0 , 1 ) plt . savefig ( '_accuracy_curve.png' ) plt . show () We achieved over 80% accuracy on the validation score. The result is acceptable for this small dataset. If we trained on the entire dataset, the accuracy would be much better. In the next post, I will try fine-tuning the same model, on the same dataset with Keras. Let's see the difference in implementation. So stay tuned ~. Test set accuracy In [0]: def computeTestSetAccuracy ( model , loss_criterion ): ''' Function to compute the accuracy on the test set Parameters :param model: Model to test :param loss_criterion: Loss Criterion to minimize ''' device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) test_acc = 0.0 test_loss = 0.0 # Validation - No gradient tracking needed with torch . no_grad (): # Set to evaluation mode model . eval () # Validation loop for j , ( inputs , labels ) in enumerate ( test_data ): inputs = inputs . to ( device ) labels = labels . to ( device ) # Forward pass - compute outputs on input data using the model outputs = model ( inputs ) # Compute loss loss = loss_criterion ( outputs , labels ) # Compute the total loss for the batch and add it to valid_loss test_loss += loss . item () * inputs . size ( 0 ) # Calculate validation accuracy ret , predictions = torch . max ( outputs . data , 1 ) correct_counts = predictions . eq ( labels . data . view_as ( predictions )) # Convert correct_counts to float and then compute the mean acc = torch . mean ( correct_counts . type ( torch . FloatTensor )) # Compute total accuracy in the whole batch and add to valid_acc test_acc += acc . item () * inputs . size ( 0 ) print ( \"Test Batch number: {:03d} , Test: Loss: {:.4f} , Accuracy: {:.4f} \" . format ( j , loss . item (), acc . item ())) # Find average test loss and test accuracy avg_test_loss = test_loss / test_data_size avg_test_acc = test_acc / test_data_size print ( \"Test accuracy : \" + str ( avg_test_acc )) In [65]: computeTestSetAccuracy ( trained_model , loss_func ) Test Batch number: 000, Test: Loss: 0.5162, Accuracy: 0.7812 Test Batch number: 001, Test: Loss: 0.5739, Accuracy: 0.7812 Test Batch number: 002, Test: Loss: 0.7250, Accuracy: 0.8125 Test Batch number: 003, Test: Loss: 0.7017, Accuracy: 0.7812 Test Batch number: 004, Test: Loss: 0.6244, Accuracy: 0.8125 Test Batch number: 005, Test: Loss: 0.7207, Accuracy: 0.7812 Test Batch number: 006, Test: Loss: 0.7719, Accuracy: 0.8125 Test Batch number: 007, Test: Loss: 0.8399, Accuracy: 0.6562 Test Batch number: 008, Test: Loss: 0.3801, Accuracy: 0.8750 Test Batch number: 009, Test: Loss: 0.9372, Accuracy: 0.7812 Test Batch number: 010, Test: Loss: 0.9332, Accuracy: 0.6875 Test Batch number: 011, Test: Loss: 0.4517, Accuracy: 0.8438 Test Batch number: 012, Test: Loss: 0.7051, Accuracy: 0.7812 Test Batch number: 013, Test: Loss: 0.6603, Accuracy: 0.8333 Test accuracy : 0.7863636352799156 Predict on test images In [0]: def predict ( model , test_image_name ): ''' Function to predict the class of a single test image Parameters :param model: Model to test :param test_image_name: Test image ''' transform = image_transforms [ 'test' ] test_image = Image . open ( test_image_name ) plt . imshow ( test_image ) test_image_tensor = transform ( test_image ) if torch . cuda . is_available (): test_image_tensor = test_image_tensor . view ( 1 , 3 , 224 , 224 ) . cuda () else : test_image_tensor = test_image_tensor . view ( 1 , 3 , 224 , 224 ) with torch . no_grad (): model . eval () # Model outputs log probabilities out = model ( test_image_tensor ) ps = torch . exp ( out ) topk , topclass = ps . topk ( 3 , dim = 1 ) for i in range ( 3 ): print ( \"Predcition\" , i + 1 , \":\" , idx_to_class [ topclass . cpu () . numpy ()[ 0 ][ i ]], \", Score: \" , topk . cpu () . numpy ()[ 0 ][ i ]) In [67]: model = torch . load ( 'trained_model.pt' ) predict ( model , 'food-11k-sub/test/Egg/3_22.jpg' ) Predcition 1 : Egg , Score: 0.947523 Predcition 2 : Bread , Score: 0.04919945 Predcition 3 : Meat , Score: 0.0022846945 Summary The above results and test accuracy is pretty good I would say, considering we are using a fairly small dataset. In this post, we've tried transfer learning on a small dataset using Pytorch with the pre-trained ResNet50 model. In a future post, I will try to do the same thing using Keras. Stay tuned.","tags":"Computer Vision","url":"https://haochen23.github.io/2019/05/transfer-learning-pytorch.html","loc":"https://haochen23.github.io/2019/05/transfer-learning-pytorch.html"},{"title":"Using Pre-trained Models with Pytorch and Keras","text":"Using Pre-trained Models: PyTorch and Keras In this post, we will try to use pre-trained models to do image classification. We will use two popular deep learning frameworks, PyTorch and Keras. Let's find out the workflow of using pre-trained models in these two frameworks. PyTorch pre-trained models Let's first look at the pre-trained models in PyTorch. We can find all of them in torchvision.models . In [0]: from torchvision import models import torch dir ( models ) Out[0]: ['AlexNet', 'DenseNet', 'GoogLeNet', 'GoogLeNetOutputs', 'Inception3', 'InceptionOutputs', 'MNASNet', 'MobileNetV2', 'ResNet', 'ShuffleNetV2', 'SqueezeNet', 'VGG', '_GoogLeNetOutputs', '_InceptionOutputs', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_utils', 'alexnet', 'densenet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'detection', 'googlenet', 'inception', 'inception_v3', 'mnasnet', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet', 'mobilenet_v2', 'quantization', 'resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'segmentation', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'shufflenetv2', 'squeezenet', 'squeezenet1_0', 'squeezenet1_1', 'utils', 'vgg', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'video', 'wide_resnet101_2', 'wide_resnet50_2'] Step 1: load pre-trained model In [0]: # load the pretrained alexnet alexnet = models . alexnet ( pretrained = True ) # view the alexnet print ( alexnet ) Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth AlexNet( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (4): ReLU(inplace=True) (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (7): ReLU(inplace=True) (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (9): ReLU(inplace=True) (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(6, 6)) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=9216, out_features=4096, bias=True) (2): ReLU(inplace=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=4096, out_features=4096, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) Step 2: Specify transformations of images Once we have the model with us, the next step is to transform the input image so that they have the right shape and other characteristics like mean and standard deviation. These values should be similar to the ones which were used while training the model. In [0]: from torchvision import transforms transform = transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ] )]) Step 3: load and transform input image In [0]: % matplotlib inline In [0]: from PIL import Image import matplotlib.pyplot as plt img = Image . open ( \"cat1.jpg\" ) plt . imshow ( img ) Out[0]: <matplotlib.image.AxesImage at 0x7fa0438e37b8> In [0]: # transform the image and prepare a batch to be passed to the alexnet img_t = transform ( img ) batch_t = torch . unsqueeze ( img_t , 0 ) Step 4: Model Inference In [0]: # put our model to inference mode alexnet . eval () # forward pass out = alexnet ( batch_t ) print ( out . shape ) torch.Size([1, 1000]) For this, we will first read and store the labels from a text file having a list of all the 1000 labels. In [0]: # read in imagenet class labels with open ( \"imagenet_classes.txt\" ) as f : classes = [ line . strip () for line in f . readlines ()] In [0]: _ , index = torch . max ( out , 1 ) percentage = torch . nn . functional . softmax ( out , dim = 1 )[ 0 ] * 100 print ( classes [ index [ 0 ]], percentage [ index [ 0 ]] . item ()) Siamese cat, Siamese 99.61538696289062 We got the 'Siamese cat' class with an over 99% confidence. Let's see what other labels it might be. In [0]: _ , indices = torch . sort ( out , descending = True ) [( classes [ idx ], percentage [ idx ] . item ()) for idx in indices [ 0 ][: 5 ]] Out[0]: [('Siamese cat, Siamese', 99.61538696289062), ('Chihuahua', 0.0938020721077919), ('Egyptian cat', 0.0931343361735344), ('wallaby, brush kangaroo', 0.03188218176364899), ('toy terrier', 0.027880175039172173)] Let's try out resnet50 and resnet101. In [0]: # resnet 50 # first load model resnet_50 = models . resnet50 ( pretrained = True ) # then put the model in eval mode resnet_50 . eval () # forward pass out = resnet_50 ( batch_t ) # Forth, print the top 5 classes predicted by the model _ , indices = torch . sort ( out , descending = True ) percentage = torch . nn . functional . softmax ( out , dim = 1 )[ 0 ] * 100 [( classes [ idx ], percentage [ idx ] . item ()) for idx in indices [ 0 ][: 5 ]] Out[0]: [('Siamese cat, Siamese', 99.51265716552734), ('Egyptian cat', 0.22618000209331512), ('lynx, catamount', 0.16791078448295593), ('paper towel', 0.010585855692625046), ('mouse, computer mouse', 0.010341660119593143)] In [0]: # resnet 101 # first load model resnet_101 = models . resnet101 ( pretrained = True ) # then put the model in eval mode resnet_101 . eval () # forward pass out = resnet_101 ( batch_t ) # Forth, print the top 5 classes predicted by the model _ , indices = torch . sort ( out , descending = True ) percentage = torch . nn . functional . softmax ( out , dim = 1 )[ 0 ] * 100 [( classes [ idx ], percentage [ idx ] . item ()) for idx in indices [ 0 ][: 5 ]] Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth Out[0]: [('Siamese cat, Siamese', 99.88835144042969), ('lynx, catamount', 0.03424644097685814), ('Egyptian cat', 0.015446522273123264), ('paper towel', 0.0066812834702432156), ('jay', 0.005932020954787731)] Keras work flow on pre-trained models Let's try out an example using keras. First, let's find out the pretrained models in keras. In [0]: import keras dir ( keras . applications ) Out[0]: ['DenseNet121', 'DenseNet169', 'DenseNet201', 'InceptionResNetV2', 'InceptionV3', 'MobileNet', 'MobileNetV2', 'NASNetLarge', 'NASNetMobile', 'ResNet101', 'ResNet101V2', 'ResNet152', 'ResNet152V2', 'ResNet50', 'ResNet50V2', 'VGG16', 'VGG19', 'Xception', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'absolute_import', 'backend', 'densenet', 'division', 'inception_resnet_v2', 'inception_v3', 'keras_applications', 'keras_modules_injection', 'layers', 'mobilenet', 'mobilenet_v2', 'models', 'nasnet', 'print_function', 'resnet', 'resnet50', 'resnet_v2', 'utils', 'vgg16', 'vgg19', 'xception'] In this case, we will only try out mobilenetV2. Other models work in a similar way. In [0]: from keras.preprocessing.image import load_img , img_to_array from keras.applications.imagenet_utils import decode_predictions from keras.applications import mobilenet_v2 from keras.applications.mobilenet_v2 import preprocess_input import numpy as np In [0]: # first, load image , to 224*224 imagenet image size original_image = load_img ( \"cat1.jpg\" , target_size = ( 224 , 224 )) # second, convert the PIL image to numpy array numpy_image = img_to_array ( original_image ) # third, convert the image into 4D tensor (samples, height, width, channels) input_image = np . expand_dims ( numpy_image , axis = 0 ) print ( 'PIL image size = ' , original_image . size ) print ( 'NumPy image size = ' , numpy_image . shape ) print ( 'Input image size = ' , input_image . shape ) plt . imshow ( np . uint8 ( input_image [ 0 ])) PIL image size = (224, 224) NumPy image size = (224, 224, 3) Input image size = (1, 224, 224, 3) Out[0]: <matplotlib.image.AxesImage at 0x7fa03fbf7b70> In [0]: # fourth, Normalize the image processed_image = preprocess_input ( input_image . copy ()) Now, we are ready to make predictions. In [0]: mobilenet_model = mobilenet_v2 . MobileNetV2 ( weights = \"imagenet\" ) prediction = mobilenet_model . predict ( processed_image ) label = decode_predictions ( prediction ) print ( 'label = ' , label [ 0 ][: 5 ]) label = [('n02123597', 'Siamese_cat', 0.7589597), ('n02124075', 'Egyptian_cat', 0.01455726), ('n04141975', 'scale', 0.010692967), ('n15075141', 'toilet_tissue', 0.0065661813), ('n04493381', 'tub', 0.0037253292)] Now, we have seen the workflows of using pre-trained models in PyTorch and Tensorflow. Using these pre-trained models is very convenient, but in most cases, they may not satisfy the specifications of our applications. We may want a more specific model. It opens up another topic Transfer Learning , or Fine Tuning these pre-trained models to meet our demands. In a flowing post, we will focus on Tranfer Learning using these models.","tags":"Computer Vision","url":"https://haochen23.github.io/2019/04/pre-trained-models-pytorch-keras.html","loc":"https://haochen23.github.io/2019/04/pre-trained-models-pytorch-keras.html"},{"title":"Time Series Analysis on Superstore Sales Data","text":"Time Series Analysis Project on Superstore Sales Data Time Series Analysis (TSA) is an important part in the field of data science. TSA uses methods for analyzing time series data in order to identify useful patterns and extract meaningful statistics of the data. There are two major goals of TSA: 1) identifing patterns or features represented by the data; and 2) forecasting (using a model to predict future values based on previous data). TSA is widely used for analysing non-stationary data, like weather data, stock price prediction, economic trend prediction and store sales prediction in this post. Different techniques in TSA will be demonstrated in this post. Let's get into it. The data we will use is the superstore sales data, and it can be download here . A Brief Exploratory Data Analysis (EDA) In [1]: # Import libraries import itertools import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . style . use ( 'fivethirtyeight' ) import statsmodels.api as sm import matplotlib matplotlib . rcParams [ 'axes.labelsize' ] = 14 matplotlib . rcParams [ 'xtick.labelsize' ] = 12 matplotlib . rcParams [ 'ytick.labelsize' ] = 12 matplotlib . rcParams [ 'text.color' ] = 'k' import warnings warnings . filterwarnings ( 'ignore' ) Read in the data. We do a brief EDA to see what's in the data. In [2]: # Import data df = pd . read_excel ( 'data\\Superstore.xls' ) df . head () Out[2]: Row ID Order ID Order Date Ship Date Ship Mode Customer ID Customer Name Segment Country City ... Postal Code Region Product ID Category Sub-Category Product Name Sales Quantity Discount Profit 0 1 CA-2016-152156 2016-11-08 2016-11-11 Second Class CG-12520 Claire Gute Consumer United States Henderson ... 42420 South FUR-BO-10001798 Furniture Bookcases Bush Somerset Collection Bookcase 261.9600 2 0.00 41.9136 1 2 CA-2016-152156 2016-11-08 2016-11-11 Second Class CG-12520 Claire Gute Consumer United States Henderson ... 42420 South FUR-CH-10000454 Furniture Chairs Hon Deluxe Fabric Upholstered Stacking Chairs,... 731.9400 3 0.00 219.5820 2 3 CA-2016-138688 2016-06-12 2016-06-16 Second Class DV-13045 Darrin Van Huff Corporate United States Los Angeles ... 90036 West OFF-LA-10000240 Office Supplies Labels Self-Adhesive Address Labels for Typewriters b... 14.6200 2 0.00 6.8714 3 4 US-2015-108966 2015-10-11 2015-10-18 Standard Class SO-20335 Sean O'Donnell Consumer United States Fort Lauderdale ... 33311 South FUR-TA-10000577 Furniture Tables Bretford CR4500 Series Slim Rectangular Table 957.5775 5 0.45 -383.0310 4 5 US-2015-108966 2015-10-11 2015-10-18 Standard Class SO-20335 Sean O'Donnell Consumer United States Fort Lauderdale ... 33311 South OFF-ST-10000760 Office Supplies Storage Eldon Fold 'N Roll Cart System 22.3680 2 0.20 2.5164 5 rows × 21 columns In [3]: df . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 9994 entries, 0 to 9993 Data columns (total 21 columns): Row ID 9994 non-null int64 Order ID 9994 non-null object Order Date 9994 non-null datetime64[ns] Ship Date 9994 non-null datetime64[ns] Ship Mode 9994 non-null object Customer ID 9994 non-null object Customer Name 9994 non-null object Segment 9994 non-null object Country 9994 non-null object City 9994 non-null object State 9994 non-null object Postal Code 9994 non-null int64 Region 9994 non-null object Product ID 9994 non-null object Category 9994 non-null object Sub-Category 9994 non-null object Product Name 9994 non-null object Sales 9994 non-null float64 Quantity 9994 non-null int64 Discount 9994 non-null float64 Profit 9994 non-null float64 dtypes: datetime64[ns](2), float64(3), int64(3), object(13) memory usage: 1.6+ MB We have 21 columns in the dataset. Let's take a look what are they. In [4]: df . columns . values Out[4]: array(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit'], dtype=object) Check if there is any missing values in the data. It turns out there's no missing values in the data. In [5]: df . isnull () . any () Out[5]: Row ID False Order ID False Order Date False Ship Date False Ship Mode False Customer ID False Customer Name False Segment False Country False City False State False Postal Code False Region False Product ID False Category False Sub-Category False Product Name False Sales False Quantity False Discount False Profit False dtype: bool Let's take a look about the statistics of numeric values in the data. In [6]: df . describe () Out[6]: Row ID Postal Code Sales Quantity Discount Profit count 9994.000000 9994.000000 9994.000000 9994.000000 9994.000000 9994.000000 mean 4997.500000 55190.379428 229.858001 3.789574 0.156203 28.656896 std 2885.163629 32063.693350 623.245101 2.225110 0.206452 234.260108 min 1.000000 1040.000000 0.444000 1.000000 0.000000 -6599.978000 25% 2499.250000 23223.000000 17.280000 2.000000 0.000000 1.728750 50% 4997.500000 56430.500000 54.490000 3.000000 0.200000 8.666500 75% 7495.750000 90008.000000 209.940000 5.000000 0.200000 29.364000 max 9994.000000 99301.000000 22638.480000 14.000000 0.800000 8399.976000 We found that there is an interesting column Category which can divided the data into several subsets according to the product categories. Let's see how many categories of products we have in the data. In [7]: df . Category . unique () Out[7]: array(['Furniture', 'Office Supplies', 'Technology'], dtype=object) In [8]: df . Category . value_counts () Out[8]: Office Supplies 6026 Furniture 2121 Technology 1847 Name: Category, dtype: int64 Now we can analyse different categories of products accordingly. In this post, let's take the Office Supplies data as an example. We will start our time series analysis on this type of products. You can also explore other categories in a similar way. In [9]: # Extract the data of Office Supplies office_supplies = df . loc [ df [ 'Category' ] == 'Office Supplies' ] We have a good 4-year office supplies sales data. We will try to find some meaningful patterns in this data. In [10]: office_supplies [ 'Order Date' ] . min (), office_supplies [ 'Order Date' ] . max () Out[10]: (Timestamp('2014-01-03 00:00:00'), Timestamp('2017-12-30 00:00:00')) Data Processing In this post, we only care about the sales data of office supplies. We'll start by droping other columns. First, let us set the index of our data using the 'Order Data' column. Note that, the sales data on the same date should be integrated together. In [11]: office_supplies = office_supplies [[ 'Order Date' , 'Sales' ]] office_supplies . head () Out[11]: Order Date Sales 2 2016-06-12 14.620 4 2015-10-11 22.368 6 2014-06-09 7.280 8 2014-06-09 18.504 9 2014-06-09 114.900 In [12]: office_supplies = office_supplies . sort_values ( 'Order Date' ) office_supplies = office_supplies . groupby ( 'Order Date' )[ 'Sales' ] . sum () . reset_index () office_supplies . head () Out[12]: Order Date Sales 0 2014-01-03 16.448 1 2014-01-04 288.060 2 2014-01-05 19.536 3 2014-01-06 685.340 4 2014-01-07 10.430 Indexing with the time series data In [13]: office_supplies = office_supplies . set_index ( 'Order Date' ) office_supplies . head () Out[13]: Sales Order Date 2014-01-03 16.448 2014-01-04 288.060 2014-01-05 19.536 2014-01-06 685.340 2014-01-07 10.430 In [14]: office_supplies . index Out[14]: DatetimeIndex(['2014-01-03', '2014-01-04', '2014-01-05', '2014-01-06', '2014-01-07', '2014-01-09', '2014-01-10', '2014-01-13', '2014-01-16', '2014-01-18', ... '2017-12-21', '2017-12-22', '2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30'], dtype='datetime64[ns]', name='Order Date', length=1148, freq=None) As can be seen from the index, our current datetime data is not continuous and can be tricky to work with. Therefore, we will use the average daily sales value for that month instead, and we are using the start of each month as the timestamp. This requires us to resample the data. In [15]: monthly = office_supplies [ 'Sales' ] . resample ( 'MS' ) . mean () In [16]: monthly Out[16]: Order Date 2014-01-01 285.357647 2014-02-01 63.042588 2014-03-01 391.176318 2014-04-01 464.794750 2014-05-01 324.346545 2014-06-01 588.774409 2014-07-01 756.060400 2014-08-01 541.879143 2014-09-01 1015.677704 2014-10-01 267.078815 2014-11-01 959.372714 2014-12-01 692.556231 2015-01-01 129.198571 2015-02-01 335.504187 2015-03-01 690.545522 2015-04-01 502.342320 2015-05-01 364.549440 2015-06-01 560.407737 2015-07-01 205.214739 2015-08-01 558.814667 2015-09-01 772.230680 2015-10-01 361.392083 2015-11-01 757.790357 2015-12-01 540.055800 2016-01-01 331.230125 2016-02-01 357.597368 2016-03-01 693.877240 2016-04-01 462.932478 2016-05-01 449.489724 2016-06-01 436.072400 2016-07-01 587.474727 2016-08-01 344.605385 2016-09-01 830.847786 2016-10-01 678.408083 2016-11-01 787.972231 2016-12-01 1357.055929 2017-01-01 967.013136 2017-02-01 389.882737 2017-03-01 538.899481 2017-04-01 558.229296 2017-05-01 508.776444 2017-06-01 650.463038 2017-07-01 393.902615 2017-08-01 1156.148154 2017-09-01 1139.137250 2017-10-01 886.045846 2017-11-01 1124.012036 2017-12-01 1049.549724 Freq: MS, Name: Sales, dtype: float64 Visualizing the Sales Time Series Data In [17]: monthly . plot ( figsize = ( 16 , 7 )) plt . show () Some distinguishable patterns appear when we plot the data. The time-series has seasonality pattern, such as sales are always low at the beginning of the year and high at the end of the year. There is always an upward trend within any single year with a couple of low months in the mid of the year. We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise. In [18]: from pylab import rcParams rcParams [ 'figure.figsize' ] = 16 , 7 decomposition = sm . tsa . seasonal_decompose ( monthly , model = 'additive' ) fig = decomposition . plot () plt . show () The figure above clearly shows the seasonality in our data, and the trend is gradually increasing through the years. Time Series Forecasting with ARIMA Autoregressive Integrated Moving Average (ARIMA) is a commonly used method for time-series prediction. ARIMA models are denoted with the notation ARIMA(p, d, q) . These three parameters account for seasonality, trend, and noise in data: In [19]: p = d = q = range ( 0 , 2 ) pdq = list ( itertools . product ( p , d , q )) seasonal_pdq = [( x [ 0 ], x [ 1 ], x [ 2 ], 12 ) for x in list ( itertools . product ( p , d , q ))] print ( 'Examples of parameter combinations for Seasonal ARIMA...' ) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 1 ], seasonal_pdq [ 1 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 1 ], seasonal_pdq [ 2 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 2 ], seasonal_pdq [ 3 ])) print ( 'SARIMAX: {} x {} ' . format ( pdq [ 2 ], seasonal_pdq [ 4 ])) Examples of parameter combinations for Seasonal ARIMA... SARIMAX: (0, 0, 1) x (0, 0, 1, 12) SARIMAX: (0, 0, 1) x (0, 1, 0, 12) SARIMAX: (0, 1, 0) x (0, 1, 1, 12) SARIMAX: (0, 1, 0) x (1, 0, 0, 12) This step is parameter selection for our Office Supplies sales ARIMA Time Series Model. Our goal here is to use a \"grid search\" to find the optimal set of parameters that yields the best performance for our model. In [20]: for param in pdq : for param_seasonal in seasonal_pdq : try : mod = sm . tsa . statespace . SARIMAX ( monthly , order = param , seasonal_order = param_seasonal , enforce_stationarity = False , enforce_invertibility = False ) results = mod . fit () print ( 'ARIMA {} x {} 12 - AIC: {} ' . format ( param , param_seasonal , results . aic )) except : continue ARIMA(0, 0, 0)x(0, 0, 0, 12)12 - AIC:747.1990404227043 ARIMA(0, 0, 0)x(0, 0, 1, 12)12 - AIC:1464.4967660059162 ARIMA(0, 0, 0)x(0, 1, 0, 12)12 - AIC:500.7335398750499 ARIMA(0, 0, 0)x(1, 0, 0, 12)12 - AIC:516.0876543936834 ARIMA(0, 0, 0)x(1, 0, 1, 12)12 - AIC:1459.9493195280877 ARIMA(0, 0, 0)x(1, 1, 0, 12)12 - AIC:347.44488563489716 ARIMA(0, 0, 1)x(0, 0, 0, 12)12 - AIC:702.6891395292475 ARIMA(0, 0, 1)x(0, 0, 1, 12)12 - AIC:nan ARIMA(0, 0, 1)x(0, 1, 0, 12)12 - AIC:487.76014158147206 ARIMA(0, 0, 1)x(1, 0, 0, 12)12 - AIC:517.4952646585557 ARIMA(0, 0, 1)x(1, 0, 1, 12)12 - AIC:3022.312250877466 ARIMA(0, 0, 1)x(1, 1, 0, 12)12 - AIC:345.69220944601045 ARIMA(0, 1, 0)x(0, 0, 0, 12)12 - AIC:666.4572045007284 ARIMA(0, 1, 0)x(0, 0, 1, 12)12 - AIC:1360.057691075371 ARIMA(0, 1, 0)x(0, 1, 0, 12)12 - AIC:503.0840747609876 ARIMA(0, 1, 0)x(1, 0, 0, 12)12 - AIC:500.0109385290892 ARIMA(0, 1, 0)x(1, 0, 1, 12)12 - AIC:1483.149298292236 ARIMA(0, 1, 0)x(1, 1, 0, 12)12 - AIC:335.7148959418817 ARIMA(0, 1, 1)x(0, 0, 0, 12)12 - AIC:637.3530008828178 ARIMA(0, 1, 1)x(0, 0, 1, 12)12 - AIC:3075.7023343964047 ARIMA(0, 1, 1)x(0, 1, 0, 12)12 - AIC:473.71539674554265 ARIMA(0, 1, 1)x(1, 0, 0, 12)12 - AIC:489.9419970027669 ARIMA(0, 1, 1)x(1, 0, 1, 12)12 - AIC:2578.319436918695 ARIMA(0, 1, 1)x(1, 1, 0, 12)12 - AIC:330.7061513093243 ARIMA(1, 0, 0)x(0, 0, 0, 12)12 - AIC:680.4032716562347 ARIMA(1, 0, 0)x(0, 0, 1, 12)12 - AIC:1588.3750282900137 ARIMA(1, 0, 0)x(0, 1, 0, 12)12 - AIC:501.7031226672368 ARIMA(1, 0, 0)x(1, 0, 0, 12)12 - AIC:500.2190534421442 ARIMA(1, 0, 0)x(1, 0, 1, 12)12 - AIC:1455.6307367581146 ARIMA(1, 0, 0)x(1, 1, 0, 12)12 - AIC:331.0719973254865 ARIMA(1, 0, 1)x(0, 0, 0, 12)12 - AIC:651.1768264308408 ARIMA(1, 0, 1)x(0, 0, 1, 12)12 - AIC:28112.535708773736 ARIMA(1, 0, 1)x(0, 1, 0, 12)12 - AIC:488.4314196132838 ARIMA(1, 0, 1)x(1, 0, 0, 12)12 - AIC:489.254640043546 ARIMA(1, 0, 1)x(1, 0, 1, 12)12 - AIC:2661.0163451942744 ARIMA(1, 0, 1)x(1, 1, 0, 12)12 - AIC:331.7136802273554 ARIMA(1, 1, 0)x(0, 0, 0, 12)12 - AIC:658.0713305703066 ARIMA(1, 1, 0)x(0, 0, 1, 12)12 - AIC:1417.437544166757 ARIMA(1, 1, 0)x(0, 1, 0, 12)12 - AIC:494.907679737394 ARIMA(1, 1, 0)x(1, 0, 0, 12)12 - AIC:482.80837530249664 ARIMA(1, 1, 0)x(1, 0, 1, 12)12 - AIC:1329.3989860683482 ARIMA(1, 1, 0)x(1, 1, 0, 12)12 - AIC:321.2325469884074 ARIMA(1, 1, 1)x(0, 0, 0, 12)12 - AIC:638.7962401173565 ARIMA(1, 1, 1)x(0, 0, 1, 12)12 - AIC:41587.6558850662 ARIMA(1, 1, 1)x(0, 1, 0, 12)12 - AIC:475.71388519565295 ARIMA(1, 1, 1)x(1, 0, 0, 12)12 - AIC:478.6987301743289 ARIMA(1, 1, 1)x(1, 0, 1, 12)12 - AIC:2580.1698313906977 ARIMA(1, 1, 1)x(1, 1, 0, 12)12 - AIC:318.20664790842784 Fitting the ARIMA Model using the Optimal parameters In [21]: mod = sm . tsa . statespace . SARIMAX ( monthly , order = ( 1 , 1 , 1 ), seasonal_order = ( 1 , 1 , 0 , 12 ), enforce_stationarity = False , enforce_invertibility = False ) results = mod . fit () print ( results . summary () . tables [ 1 ]) ============================================================================== coef std err z P>|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.2479 0.328 0.755 0.450 -0.395 0.891 ma.L1 -0.9389 0.462 -2.030 0.042 -1.845 -0.032 ar.S.L12 -0.6135 0.348 -1.762 0.078 -1.296 0.069 sigma2 7.421e+04 2.68e+04 2.768 0.006 2.17e+04 1.27e+05 ============================================================================== Let's see the model diagnostics to investigate any unusual behavior. In [22]: results . plot_diagnostics ( figsize = ( 16 , 7 )) plt . show () It is not a perfect model. However, our model diagnostics suggests that the model residuals are nearly normally distributed. Validating Forecasts To help us understand the accuracy of our forecasts, we compare predicted sales to real sales of the time series, and we set forecasts to start at 2017–01–01 to the end of the data. In [23]: pred = results . get_prediction ( start = pd . to_datetime ( '2017-01-01' ), dynamic = False ) pred_ci = pred . conf_int () ax = monthly [ '2014' :] . plot ( label = 'observed' ) pred . predicted_mean . plot ( ax = ax , label = 'One-step ahead Forecast' , alpha = . 7 , figsize = ( 14 , 6 )) ax . fill_between ( pred_ci . index , pred_ci . iloc [:, 0 ], pred_ci . iloc [:, 1 ], color = 'k' , alpha = . 2 ) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Office Supplies Sales' ) plt . legend () plt . show () The line plot is showing the observed values compared to the rolling forecast predictions. Overall, our forecasts align with the true values very well, showing an upward trend starts from the beginning of the year and captured the seasonality toward the end of the year. The grey area shows the confidence interval. In [24]: monthly_forecasted = pred . predicted_mean monthly_truth = monthly [ '2017-01-01' :] mse = (( monthly_forecasted - monthly_truth ) ** 2 ) . mean () print ( 'The Mean Squared Error of our forecasts is {} ' . format ( round ( mse , 2 ))) The Mean Squared Error of our forecasts is 65844.6 In [25]: print ( 'The Root Mean Squared Error of our forecasts is {} ' . format ( round ( np . sqrt ( mse ), 2 ))) The Root Mean Squared Error of our forecasts is 256.6 The Root Mean Squared Error (RMSE) shows that our model was able to predict the average daily Office Supplies Sales in the test set within 256.6 of the real sales. Our Office Supplies daily sales range from around 50 to 1350. So, our model works pretty well so far. Producting and Visualizing Forecasts In [26]: pred_uc = results . get_forecast ( steps = 100 ) pred_ci = pred_uc . conf_int () ax = monthly . plot ( label = 'observed' , figsize = ( 14 , 7 )) pred_uc . predicted_mean . plot ( ax = ax , label = 'Forecast' ) ax . fill_between ( pred_ci . index , pred_ci . iloc [:, 0 ], pred_ci . iloc [:, 1 ], color = 'k' , alpha = . 2 ) ax . set_xlabel ( 'Date' ) ax . set_ylabel ( 'Office Supplies Sales' ) plt . legend () plt . show () Our ARIMA prediction model clearly captured the office supplies sales seasonality. As we forecast further out into the future, it is natural for us to become less confident in our predicted values (the expansion of the grey area). The above time series analysis for Office Supplies makes me curious about other categories, and how do they compare with each other over time. Therefore, we are going to compare time series of office supplies and furniture. Time Series of Office Supplies vs. Furniture According to our data, the size of Office Supplies is much larger than the size of Furniture. In [27]: furniture = df . loc [ df [ 'Category' ] == 'Furniture' ] furniture = furniture [[ 'Order Date' , 'Sales' ]] furniture = furniture . sort_values ( 'Order Date' ) furniture = furniture . groupby ( 'Order Date' )[ 'Sales' ] . sum () . reset_index () furniture = furniture . set_index ( 'Order Date' ) furniture . head () Out[27]: Sales Order Date 2014-01-06 2573.820 2014-01-07 76.728 2014-01-10 51.940 2014-01-11 9.940 2014-01-13 879.939 In [28]: monthly_office = monthly monthly_furniture = furniture [ 'Sales' ] . resample ( 'MS' ) . mean () In [29]: furniture = pd . DataFrame ({ 'Order Date' : monthly_furniture . index , 'Sales' : monthly_furniture . values }) office = pd . DataFrame ({ 'Order Date' : monthly_office . index , 'Sales' : monthly_office . values }) store = furniture . merge ( office , how = 'inner' , on = 'Order Date' ) store . rename ( columns = { 'Sales_x' : 'furniture_sales' , 'Sales_y' : 'office_sales' }, inplace = True ) store . head () Out[29]: Order Date furniture_sales office_sales 0 2014-01-01 480.194231 285.357647 1 2014-02-01 367.931600 63.042588 2 2014-03-01 857.291529 391.176318 3 2014-04-01 567.488357 464.794750 4 2014-05-01 432.049188 324.346545 Visualizing the Sales Data In [30]: plt . figure ( figsize = ( 16 , 7 )) plt . plot ( store [ 'Order Date' ], store [ 'furniture_sales' ], 'b-' , label = 'Furniture' ) plt . plot ( store [ 'Order Date' ], store [ 'office_sales' ], 'r-' , label = 'Office Supplies' ) plt . xlabel ( 'Date' ) plt . ylabel ( 'Sales' ) plt . title ( 'Sales Furniture vs. Office Supplies' ) plt . legend () plt . show () We observe that sales of furniture and office supplies shared a similar seasonal pattern. Early of the year is the off season for both of the two categories. It seems summer time is quiet for office supplies too. in addition, average daily sales for furniture are higher than those of office supplies in most of the months. It is understandable, as the value of furniture should be much higher than those of office supplies. Occasionally, office supplies passed furniture on average daily sales. Let's find out when was the first time office supplies' sales surpassed those of furniture's. In [31]: first_date = store . ix [ np . min ( list ( np . where ( store [ 'office_sales' ] > store [ 'furniture_sales' ])[ 0 ])), 'Order Date' ] print ( \"Office supplies first time produced higher sales than furniture is {} .\" . format ( first_date . date ())) Office supplies first time produced higher sales than furniture is 2014-07-01. Time Series Modeling with Prophet Prophet is designed for analyzing time-series that display patterns on different time scales such as yearly, weekly and daily. It also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints. Therefore, we are using Prophet to get a model up and running. Let's take look at the Prophet forcasting models for Furniture and Office Supplies Sales. In [32]: from fbprophet import Prophet furniture = furniture . rename ( columns = { 'Order Date' : 'ds' , 'Sales' : 'y' }) furniture_model = Prophet ( interval_width = 0.95 ) furniture_model . fit ( furniture ) office = office . rename ( columns = { 'Order Date' : 'ds' , 'Sales' : 'y' }) office_model = Prophet ( interval_width = 0.95 ) office_model . fit ( office ) furniture_forecast = furniture_model . make_future_dataframe ( periods = 36 , freq = 'MS' ) furniture_forecast = furniture_model . predict ( furniture_forecast ) office_forecast = office_model . make_future_dataframe ( periods = 36 , freq = 'MS' ) office_forecast = office_model . predict ( office_forecast ) plt . figure ( figsize = ( 16 , 7 )) furniture_model . plot ( furniture_forecast , xlabel = 'Date' , ylabel = 'Sales' ) plt . title ( 'Furniture Sales' ) plt . show () INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. <Figure size 1152x504 with 0 Axes> In [33]: plt . figure ( figsize = ( 16 , 7 )) office_model . plot ( office_forecast , xlabel = 'Date' , ylabel = 'Sales' ) plt . title ( 'Office Supplies Sales' ) plt . show () <Figure size 1152x504 with 0 Axes> Compare the Two Forecasts We already have the forecasts for three years for these two categories into the future. We will now join them together to compare their future forecasts. In [34]: furniture_names = [ 'furniture_ %s ' % column for column in furniture_forecast . columns ] office_names = [ 'office_ %s ' % column for column in office_forecast . columns ] merge_furniture_forecast = furniture_forecast . copy () merge_office_forecast = office_forecast . copy () merge_furniture_forecast . columns = furniture_names merge_office_forecast . columns = office_names forecast = pd . merge ( merge_furniture_forecast , merge_office_forecast , how = 'inner' , left_on = 'furniture_ds' , right_on = 'office_ds' ) forecast = forecast . rename ( columns = { 'furniture_ds' : 'Date' }) . drop ( 'office_ds' , axis = 1 ) forecast . head () Out[34]: Date furniture_trend furniture_yhat_lower furniture_yhat_upper furniture_trend_lower furniture_trend_upper furniture_additive_terms furniture_additive_terms_lower furniture_additive_terms_upper furniture_yearly ... office_additive_terms office_additive_terms_lower office_additive_terms_upper office_yearly office_yearly_lower office_yearly_upper office_multiplicative_terms office_multiplicative_terms_lower office_multiplicative_terms_upper office_yhat 0 2014-01-01 731.079361 266.970810 823.529872 731.079361 731.079361 -178.836100 -178.836100 -178.836100 -178.836100 ... -132.483942 -132.483942 -132.483942 -132.483942 -132.483942 -132.483942 0.0 0.0 0.0 297.865749 1 2014-02-01 733.206972 133.362486 678.590785 733.206972 733.206972 -324.048145 -324.048145 -324.048145 -324.048145 ... -288.226070 -288.226070 -288.226070 -288.226070 -288.226070 -288.226070 0.0 0.0 0.0 149.595672 2 2014-03-01 735.128684 395.356700 921.716107 735.128684 735.128684 -69.406915 -69.406915 -69.406915 -69.406915 ... 0.829065 0.829065 0.829065 0.829065 0.829065 0.829065 0.0 0.0 0.0 445.399757 3 2014-04-01 737.256294 330.392503 863.616976 737.256294 737.256294 -140.477169 -140.477169 -140.477169 -140.477169 ... -89.156127 -89.156127 -89.156127 -89.156127 -89.156127 -89.156127 0.0 0.0 0.0 362.886617 4 2014-05-01 739.315271 273.750063 839.110706 739.315271 739.315271 -172.355011 -172.355011 -172.355011 -172.355011 ... -183.195734 -183.195734 -183.195734 -183.195734 -183.195734 -183.195734 0.0 0.0 0.0 276.078026 5 rows × 31 columns Trend and Forecast Visualization In [35]: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( forecast [ 'Date' ], forecast [ 'furniture_trend' ], 'b-' ) plt . plot ( forecast [ 'Date' ], forecast [ 'office_trend' ], 'r-' ) plt . legend (); plt . xlabel ( 'Date' ); plt . ylabel ( 'Sales' ) plt . title ( 'Furniture vs. Office Supplies Sales Trend' ); plt . show () In [36]: plt . figure ( figsize = ( 10 , 7 )) plt . plot ( forecast [ 'Date' ], forecast [ 'furniture_yhat' ], 'b-' ) plt . plot ( forecast [ 'Date' ], forecast [ 'office_yhat' ], 'r-' ) plt . legend (); plt . xlabel ( 'Date' ); plt . ylabel ( 'Sales' ) plt . title ( 'Furniture vs. Office Supplies Estimate' ); Trends and Patterns Now, we can use the Prophet Models to inspect different trends of these two categories in the data. In [37]: furniture_model . plot_components ( furniture_forecast ) plt . show () In [38]: office_model . plot_components ( office_forecast ) plt . show () Good to see that the sales for both furniture and office supplies have been linearly increasing over time and will be keep growing, although office supplies' growth seems slightly stronger.","tags":"Data Science","url":"https://haochen23.github.io/2019/02/time-series-analysis-superstore-sales.html","loc":"https://haochen23.github.io/2019/02/time-series-analysis-superstore-sales.html"},{"title":"Car Crash Prediction in NZ - Machine Learning Pipeline","text":"Car Crash Prediction in NZ - Machine Learning Pipeline 1. Introduction The dataset we use for this post is New Zealand Crash Analysis Dataset which is updated on a quarterly basis by the Transport Agency. The dataset was last updated on October 2018 (from January 2000). It contains all traffic crashes as reported to the Transport Agency by the NZ police. However, not all crashes are reported NZ police. A big portion of minor car crashes are settled on site by the parties without reporting to the police. The level of reporting increases with the severity of the crash. Due to the nature of non-fatal crashes it is believed that these are under-reported. The dataset is available in many different formats and APIs. It is easy to grab them through API interfaces, instead of downloading to your local machine. In this way, we can have the newest version of the dataset every time we run it. If you want to download the dataset to your local machine, it can be found here . The crashes shown in the dataset are strongly related to geography, so we will use the geojson format. So, we can also perform geographic data analysis without creating geometries from latitude and longtitude and deal with coordinate reference systems and projections. And we will also use different machine learning models to predict future car crashes based on the available data. Now, let's get into it. This post is a self-learning project, and the original content can be found here 2. Exploratory Data Analysis (EDA) Import Libraries In [1]: # Import libraries import pandas as pd import numpy as np import geopandas as gpd import matplotlib.pyplot as plt import folium import requests import seaborn as sns Import Data Import the dataset through API. This process can take a while. In [2]: url = 'https://opendata.arcgis.com/datasets/a163c5addf2c4b7f9079f08751bd2e1a_0.geojson' geojson = requests . get ( url ) . json () crs = { 'init' : 'epsg:3851' } #Coordinate reference system for New Zealand gdf = gpd . GeoDataFrame . from_features ( geojson [ 'features' ], crs = crs ) print ( gdf . shape ) gdf . head () (665847, 88) Out[2]: OBJECTID Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 3001 1 0 0 514101 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 3002 1 0 0 514102 0 0 0 0 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 3003 1 0 0 521802 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 3004 0 0 0 508801 0 0 0 0 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 3005 1 0 0 508411 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 88 columns In [3]: gdf . columns Out[3]: Index(['OBJECTID', 'Pedestrian', 'advisorySpeed', 'animals', 'areaUnitID', 'bicycle', 'bridge', 'bus', 'carStationWagon', 'cliffBank', 'cornerRoadSideRoad', 'crashDirectionDescription', 'crashDistance', 'crashFinancialYear', 'crashLocation1', 'crashLocation2', 'crashRPDirectionDescription', 'crashRPDisplacement', 'crashRPNewsDescription', 'crashRPRS', 'crashRPSH', 'crashSHDescription', 'crashSeverity', 'crashYear', 'darkLight', 'debris', 'directionRoleDescription', 'ditch', 'easting', 'fatalCount', 'fence', 'flatHill', 'geometry', 'guardRail', 'holiday', 'houseBuilding', 'intersection', 'intersectionMidblock', 'junctionType', 'kerb', 'light', 'meshblockID', 'minorInjuryCount', 'moped', 'motorcycle', 'multiVehicle', 'northing', 'numberOfLanes', 'objectThrownOrDropped', 'other', 'otherVehicleType', 'outdatedLocationDescription', 'overBank', 'parkedVehicle', 'phoneBoxEtc', 'postOrPole', 'regionDesc', 'roadCharacter', 'roadCurvature', 'roadLane', 'roadMarkings', 'roadSurface', 'roadWet', 'roadworks', 'schoolBus', 'seriousInjuryCount', 'slipFlood', 'speedLimit', 'strayAnimal', 'streetLight', 'suv', 'taxi', 'temporarySpeedLimit', 'tlaID', 'tlaName', 'trafficControl', 'trafficIsland', 'trafficSign', 'train', 'tree', 'truck', 'unknownVehicleType', 'urban', 'vanOrUtility', 'vehicle', 'waterRiver', 'weatherA', 'weatherB'], dtype='object') Important Crash Information Crash information on death and injuries are very important to us. We will first explore the dataset on this part of information. crashSeverity : the severity of a car crash. Possible values are 'F'-fatal, 'S'-serious, 'M'-minor, 'N'-non-injury. This is determined by the worst injury sustained in the crash at the time of entry. seriousInjuryCount : the number of serious injuries associated with a crash. minorInjuryCount : the number of minor injuries associated with a crash. fatalCount : the number of fatal casualties associated with a crash. In [4]: start_year = gdf . crashYear . min () end_year = gdf . crashYear . max () fatal = gdf [ 'fatalCount' ] . sum () serious = gdf [ 'seriousInjuryCount' ] . sum () minor = gdf [ 'minorInjuryCount' ] . sum () print ( \"The total death in car crash accident in New Zealand from {} to {} is {} .\" . format ( start_year , end_year , fatal )) print ( \"While the total number of serious injuries and minor injuries in car crashes is {} , and {} , respectively.\" . format ( serious , minor )) The total death in car crash accident in New Zealand from 2000 to 2018 is 6922. While the total number of serious injuries and minor injuries in car crashes is 45044, and 205895, respectively. In [5]: import warnings warnings . simplefilter ( 'ignore' ) fig , ax = plt . subplots ( 1 , 3 , figsize = ( 25 , 5 )) sns . barplot ( x = 'crashYear' , y = 'fatalCount' , data = gdf , ax = ax [ 0 ], color = 'k' ) ax [ 0 ] . set_title ( \"The number of total death from year 2000 to 2018\" , fontsize = 14 ) sns . barplot ( x = 'crashYear' , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], color = 'r' ) ax [ 1 ] . set_title ( \"The number of total serious injuries from year 2000 to 2018\" , fontsize = 14 ) sns . barplot ( x = 'crashYear' , y = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ], color = 'g' ) ax [ 2 ] . set_title ( \"The number of total minor injuries from year 2000 to 2018\" , fontsize = 14 ) [ ax [ i ] . set_xlabel ( 'Crash Year' ) for i in range ( 3 )] plt . tight_layout () plt . show () In [6]: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) sns . countplot ( x = 'fatalCount' , data = gdf , ax = ax [ 0 ], color = 'k' ) sns . countplot ( x = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], color = 'r' ) sns . countplot ( x = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ], color = 'g' ) plt . tight_layout () plt . show () In [7]: fig , ax = plt . subplots ( figsize = ( 10 , 4 )) sns . countplot ( y = \"crashSeverity\" , data = gdf , order = 'NMSF' , ax = ax ) # Non-injury, Minor, Seriours, Fatal plt . show () In [8]: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 25 , 4 )) sns . lineplot ( x = 'crashYear' , y = 'fatalCount' , data = gdf , ax = ax [ 0 ]) ax [ 0 ] . set_title ( \"The number of total death from year 2000 to 2018\" , fontsize = 14 ) sns . lineplot ( x = 'crashYear' , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ]) ax [ 1 ] . set_title ( \"The number of total serious injuries from year 2000 to 2018\" , fontsize = 14 ) sns . lineplot ( x = 'crashYear' , y = 'minorInjuryCount' , data = gdf , ax = ax [ 2 ]) ax [ 2 ] . set_title ( \"The number of total minor injuries from year 2000 to 2018\" , fontsize = 14 ) [ ax [ i ] . set_xlabel ( 'Crash Year' ) for i in range ( 3 )] plt . tight_layout () plt . show () Road Information These variables, including roadworks, junctionType, roadCharacter, roadCurvature, roadLane, roadMarkings, roadSurface, roadWet, numberOfLanes, intersection, intersectionMidblock, flatHill, darkLight , provide information about the road condition when an accident occurs. Let's plot some of them out to see their patterns. In [9]: fig , ax = plt . subplots ( 3 , 3 , figsize = ( 20 , 10 )) sns . countplot ( x = 'roadCurvature' , data = gdf , ax = ax [ 0 , 0 ]) sns . countplot ( x = \"junctionType\" , data = gdf , ax = ax [ 0 , 1 ]) sns . countplot ( x = 'roadLane' , data = gdf , ax = ax [ 0 , 2 ]) sns . countplot ( x = 'numberOfLanes' , data = gdf , ax = ax [ 1 , 0 ]) sns . countplot ( x = 'roadWet' , data = gdf , ax = ax [ 1 , 1 ]) sns . countplot ( x = 'roadMarkings' , data = gdf , ax = ax [ 1 , 2 ]) sns . countplot ( x = 'roadSurface' , data = gdf , ax = ax [ 2 , 0 ]) sns . countplot ( x = 'darkLight' , data = gdf , ax = ax [ 2 , 1 ]) sns . countplot ( x = 'intersection' , data = gdf , ax = ax [ 2 , 2 ]) #intersectionMidblock plt . tight_layout () NumberOfLanes influence on fatal and serious injuries. In [10]: plt . figure ( figsize = ( 12 , 5 )) sns . barplot ( x = 'numberOfLanes' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' ) plt . figure ( figsize = ( 12 , 5 )) sns . barplot ( x = 'numberOfLanes' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' ) plt . show () JunctionType influence on fatal and serious injuries. In [11]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) sns . barplot ( x = 'junctionType' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'junctionType' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () RoadWet influence on fatal and serious injuries. In [12]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'roadWet' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'roadWet' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () DarkLight influence. Dark Light condition also has a significant influence on fatal and serious injury accident, observed from the figure shown below. In [13]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'darkLight' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'darkLight' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () RoadSurface influence. From the figure shown below, it can be easily observed that unsealed road surface has a much higher fatal or serious injury rate than sealed roads. In [14]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) sns . barplot ( x = 'roadSurface' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'roadSurface' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () Traffic Conditions SpeedLimit with fatal and serious injuries. In [15]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 4 )) sns . barplot ( x = 'speedLimit' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'speedLimit' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () StreetLight with fatal and serious injuries. In [16]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 4 )) sns . barplot ( x = 'streetLight' , y = 'fatalCount' , data = gdf , palette = 'Reds_d' , ax = ax [ 0 ]) sns . barplot ( x = 'streetLight' , y = 'seriousInjuryCount' , data = gdf , palette = 'Blues_d' , ax = ax [ 1 ]) plt . tight_layout () plt . show () AdvisorySpeed with fatal and serious injuries. In [17]: sns . catplot ( x = \"advisorySpeed\" , y = \"crashSeverity\" , data = gdf , kind = 'bar' ); plt . show () Weather Conditions Total Weather Counts , weatherA and weatherB . And their influence on fatal and serious injuries. In [18]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . countplot ( x = \"weatherA\" , data = gdf , ax = ax [ 0 ], palette = \"Greens_d\" ); sns . countplot ( x = \"weatherB\" , data = gdf , ax = ax [ 1 ], palette = \"Greens_d\" ); plt . tight_layout () plt . show () In [19]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . barplot ( x = \"weatherA\" , y = 'fatalCount' , data = gdf , ax = ax [ 0 ], palette = \"Reds_d\" ); sns . barplot ( x = \"weatherB\" , y = 'seriousInjuryCount' , data = gdf , ax = ax [ 1 ], palette = \"Blues_d\" ); plt . tight_layout () plt . show () Geographic Data Exploration In [20]: gdf . plot ( markersize = 0.01 , edgecolor = 'r' , figsize = ( 12 , 12 )) plt . axis ( 'off' ) Out[20]: (166.67652523873306, 179.10957030210642, -47.52866483091503, -33.806398085455236) In [21]: from folium.plugins import MarkerCluster gdf_sample = gdf . sample ( 5000 ) lons = gdf_sample . geometry . x lats = gdf_sample . geometry . y m = folium . Map ( location = [ np . mean ( lats ), np . mean ( lons )], zoom_start = 5.7 ) #FastMarkerCluster(data=list(zip(lats, lons))).add_to(m) MarkerCluster ( list ( zip ( lats , lons ))) . add_to ( m ) folium . LayerControl () . add_to ( m ) m Out[21]: In [22]: gdf_sample = gdf . sample ( 5000 ) lons = gdf_sample . geometry . x lats = gdf_sample . geometry . y heat_cols = list ( zip ( lats , lons )) from folium.plugins import HeatMap m = folium . Map ([ np . mean ( lats ), np . mean ( lons )], tiles = 'CartoDB dark_matter' , zoom_start = 7 ) HeatMap ( heat_cols ) . add_to ( m ) m Out[22]: Data Modelling We can approach the machine learning part in different ways. We can consider it as a regression problem and predict the number of death or serious injuries. We can also take it as a classification problem and classify the severity of a certain car crash. In this post, we will first approach it as a regression problem. We will try to predict the the fatalCount based on all other variables. Random Forest will be used the the modelling algorithm. It needs only minimal data preprocessing and does well in many cases. In [23]: # import libraries from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split # first tranfer our data from geoDataFrame to pandas DataFrame, so that we can use sklearn to work on it. df = pd . DataFrame ( gdf . drop ([ 'geometry' , 'OBJECTID' ], axis = 1 )) df . head () Out[23]: Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank cornerRoadSideRoad ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 1 0 0 514101 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 1 0 0 514102 0 0 0 0 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 1 0 0 521802 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 0 0 0 508801 0 0 0 0 1 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 1 0 0 508411 0 0 0 1 0 1 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 86 columns In [24]: gdf . head () Out[24]: OBJECTID Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 3001 1 0 0 514101 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 1 3002 1 0 0 514102 0 0 0 0 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 2 3003 1 0 0 521802 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 3 3004 0 0 0 508801 0 0 0 0 1 ... 0 0 0 0 Urban 1 0 0 Fine Unknown 4 3005 1 0 0 508411 0 0 0 1 0 ... 0 0 0 0 Urban 0 0 0 Fine Unknown 5 rows × 88 columns Regression Modeling with Random forest We first need to convert categorical variables into numerical variables, using LabelEncoder and OneHotEncoder. In [25]: from sklearn.preprocessing import LabelEncoder LE = LabelEncoder () for i in df : if df [ i ] . dtype == 'object' : LE . fit ( df [ i ]) df [ i ] = LE . transform ( df [ i ]) df = pd . get_dummies ( df ) df . head () Out[25]: Pedestrian advisorySpeed animals areaUnitID bicycle bridge bus carStationWagon cliffBank cornerRoadSideRoad ... train tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver weatherA weatherB 0 1 0 0 514101 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 1 1 0 0 514102 0 0 0 0 0 1 ... 0 0 0 0 2 0 0 0 0 2 2 1 0 0 521802 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 3 0 0 0 508801 0 0 0 0 1 1 ... 0 0 0 0 2 1 0 0 0 2 4 1 0 0 508411 0 0 0 1 0 1 ... 0 0 0 0 2 0 0 0 0 2 5 rows × 86 columns In [26]: # split data into training and validation sets X_train , X_test , y_train , y_test = train_test_split ( df . drop ( 'fatalCount' , axis = 1 ), df . fatalCount , test_size = 0.2 , random_state = 42 ) print (( X_train . shape , X_test . shape , y_train . shape , y_test . shape )) ((532677, 85), (133170, 85), (532677,), (133170,)) In [36]: def rmse ( x , y ): return np . sqrt ((( x - y ) ** 2 ) . mean ()) def print_score ( m ): res = [ rmse ( m . predict ( X_train ), y_train ), rmse ( m . predict ( X_test ), y_test ), m . score ( X_train , y_train ), m . score ( X_test , y_test )] if hasattr ( m , 'oob_score_' ): res . append ( m . oob_score_ ) print ( res ) In [28]: # Initialize the regressor m = RandomForestRegressor ( n_estimators = 50 ) m . fit ( X_train , Y_train ) print_score ( m ) [0.017907117808386587, 0.04273625304671058, 0.9766110961503995, 0.8620394418555027] Feature Importance In [29]: f_imp = pd . DataFrame ( data = { 'importance' : m . feature_importances_ , 'features' : X_train . columns }) . set_index ( 'features' ) f_imp = f_imp . sort_values ( 'importance' , ascending = False ) f_imp . head ( 10 ) Out[29]: importance features crashSeverity 0.855630 crashLocation2 0.010599 easting 0.009086 crashDistance 0.007509 crashRPDisplacement 0.007109 crashRPRS 0.006428 crashLocation1 0.006199 seriousInjuryCount 0.005648 northing 0.005389 meshblockID 0.004877 Clearly, crashSeverity is the most important in our model, because it is derived from the number of deaths. In [30]: f_imp [: 20 ] . plot ( kind = 'bar' , figsize = ( 12 , 10 )) plt . show () Select the variables with importance. In [31]: f_imp_sel = f_imp [ f_imp [ 'importance' ] > 0.0001 ] In [32]: len ( f_imp_sel . index ), len ( f_imp . index ) Out[32]: (73, 85) We have 73 out of 85 variables with importance value over 0.0001. In [33]: y = df . fatalCount df_imp = df [ f_imp_sel . index ] df_imp . shape Out[33]: (665847, 73) Retrain the RandomForestRegressor with only the variables with significance. In [34]: # Let us split our data into training and validation sets X_train , X_test , y_train , y_test = train_test_split ( df_imp , y , test_size = 0.33 , random_state = 42 ) m = RandomForestRegressor ( n_estimators = 50 , oob_score = True ) m . fit ( X_train , y_train ) Out[34]: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None, oob_score=True, random_state=None, verbose=0, warm_start=False) In [37]: print_score ( m ) [0.017239725389319133, 0.04320178481356111, 0.978469926234861, 0.8589594053214539, 0.8404163772494878] Adjusting the parameters of the RF regressor to see if we can get better results. Of course, you can also use a Grid Search Cross-validation to find more suitable parameters. Here, to make it easy, I will not go through this procedure. Because it may take a long time. In [38]: m = RandomForestRegressor ( n_estimators = 100 , min_samples_leaf = 10 , oob_score = True ) m . fit ( X_train , y_train ) print_score ( m ) [0.03777296689755251, 0.04108996727501145, 0.8966413028101987, 0.8724112618047403, 0.8542367058273785]","tags":"Data Science","url":"https://haochen23.github.io/2019/01/car-crash-prediction-nz.html","loc":"https://haochen23.github.io/2019/01/car-crash-prediction-nz.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part III","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II Introduction This is the third post of the valuable Matplotlib & Seaborn Visualization Handbook. This post will focus on the last three categories discussed in the last post, as follows: Composition Waffle chart Pie chart Treemap Bar chart Change Time series plot Time series with peaks and troughs annotated Autocorrelation plot Cross correlation plot Time series decomposition plot Multiple time series Plotting with different scales using secondary Y axis Time series with error bands Stacked area chart Unstacked area chart Calender heat map Seasonal plot Groups Dendrogram Cluster plot Andrews curve Parallel coordinates First, import neccessary libraries. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 5. Composition Waffle chart Waffle charts can be used to show the compositional information of groups in a larger population. It can be done using the pywaffle packege. In [2]: from pywaffle import Waffle import warnings warnings . simplefilter ( 'ignore' ) # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data # By Class Data df_class = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts_class' ) n_categories = df_class . shape [ 0 ] colors_class = [ plt . cm . Set3 ( i / float ( n_categories )) for i in range ( n_categories )] # By Cylinders Data df_cyl = df_raw . groupby ( 'cyl' ) . size () . reset_index ( name = 'counts_cyl' ) n_categories = df_cyl . shape [ 0 ] colors_cyl = [ plt . cm . Spectral ( i / float ( n_categories )) for i in range ( n_categories )] # By Make Data df_make = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts_make' ) n_categories = df_make . shape [ 0 ] colors_make = [ plt . cm . tab20b ( i / float ( n_categories )) for i in range ( n_categories )] # Draw Plot and Decorate fig = plt . figure ( FigureClass = Waffle , plots = { '311' : { 'values' : df_class [ 'counts_class' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_class [[ 'class' , 'counts_class' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Class' }, 'title' : { 'label' : '# Vehicles by Class' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_class }, '312' : { 'values' : df_cyl [ 'counts_cyl' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_cyl [[ 'cyl' , 'counts_cyl' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Cyl' }, 'title' : { 'label' : '# Vehicles by Cyl' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_cyl }, '313' : { 'values' : df_make [ 'counts_make' ], 'labels' : [ \" {1} \" . format ( n [ 0 ], n [ 1 ]) for n in df_make [[ 'manufacturer' , 'counts_make' ]] . itertuples ()], 'legend' : { 'loc' : 'upper left' , 'bbox_to_anchor' : ( 1.05 , 1 ), 'fontsize' : 12 , 'title' : 'Manufacturer' }, 'title' : { 'label' : '# Vehicles by Make' , 'loc' : 'center' , 'fontsize' : 18 }, 'colors' : colors_make } }, rows = 9 , figsize = ( 16 , 14 ) ) Pie chart Pie chart is a classic way to show the composition of groups. However, its not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading, especially for 3D pie chart. So, if you are to use pie chart, its highly recommended to explicitly write down the percentage or numbers for each portion of the pie. Here is a bad example without showing the percentage information of each part. In [3]: # Import data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () # Make the plot with pandas df . plot ( kind = 'pie' , subplots = True , figsize = ( 8 , 8 )) plt . title ( \"Pie Chart of Vehicle Class - Bad\" ) plt . ylabel ( \"\" ) plt . show () Now let's see a good example of using pie chart. It shows the exact compositions of each part. In [4]: # Import df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) # Draw Plot fig , ax = plt . subplots ( figsize = ( 12 , 7 ), subplot_kw = dict ( aspect = \"equal\" ), dpi = 80 ) data = df [ 'counts' ] categories = df [ 'class' ] explode = [ 0 , 0.1 , 0 , 0 , 0 , 0 , 0 ] def func ( pct , allvals ): absolute = int ( pct / 100. * np . sum ( allvals )) return \" {:.1f} % ( {:d} )\" . format ( pct , absolute ) wedges , texts , autotexts = ax . pie ( data , autopct = lambda pct : func ( pct , data ), textprops = dict ( color = \"w\" ), colors = plt . cm . Dark2 . colors , labels = categories . values . tolist (), shadow = False , startangle = 140 , explode = explode ) # Decoration ax . legend ( wedges , categories , title = \"Vehicle Class\" , loc = \"center left\" , bbox_to_anchor = ( 1 , 0 , 0.5 , 1 )) plt . setp ( autotexts , size = 10 , weight = 700 ) ax . set_title ( \"Class of Vehicles: Pie Chart\" ) plt . show () Treemap Treemap is another compistional chart. In my opinion, it is less used than pie chart. In [5]: import squarify # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'class' ) . size () . reset_index ( name = 'counts' ) labels = df . apply ( lambda x : str ( x [ 0 ]) + \" \\n (\" + str ( x [ 1 ]) + \")\" , axis = 1 ) sizes = df [ 'counts' ] . values . tolist () colors = [ plt . cm . Spectral ( i / float ( len ( labels ))) for i in range ( len ( labels ))] # Draw Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) squarify . plot ( sizes = sizes , label = labels , color = colors , alpha =. 8 ) # Decorate plt . title ( 'Treemap of Vechile Class' ) plt . axis ( 'off' ) plt . show () Bar chart Bar chart, both verticle and horizontal, can give a good presentation of the compositional information. In [6]: import random # Import Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare Data df = df_raw . groupby ( 'manufacturer' ) . size () . reset_index ( name = 'counts' ) n = df [ 'manufacturer' ] . unique () . __len__ () + 1 all_colors = list ( plt . cm . colors . cnames . keys ()) random . seed ( 100 ) c = random . choices ( all_colors , k = n ) # Plot Bars plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . bar ( df [ 'manufacturer' ], df [ 'counts' ], color = c , width =. 5 ) for i , val in enumerate ( df [ 'counts' ] . values ): plt . text ( i , val , float ( val ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontdict = { 'fontweight' : 500 , 'size' : 12 }) # Decoration plt . gca () . set_xticklabels ( df [ 'manufacturer' ], rotation = 60 , horizontalalignment = 'right' ) plt . title ( \"Number of Vehicles by Manaufacturers\" , fontsize = 22 ) plt . ylabel ( '# Vehicles' ) plt . ylim ( 0 , 45 ) plt . show () 6. Change Time series plot In [7]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:red' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 12 ] xtick_labels = [ x [ - 4 :] for x in df . date . tolist ()[:: 12 ]] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 0 , fontsize = 12 , horizontalalignment = 'center' , alpha =. 7 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'both' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.3 ) plt . show () Time series with peaks and troughs annotated In [8]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Get the Peaks and Troughs data = df [ 'traffic' ] . values doublediff = np . diff ( np . sign ( np . diff ( data ))) peak_locations = np . where ( doublediff == - 2 )[ 0 ] + 1 doublediff2 = np . diff ( np . sign ( np . diff ( - 1 * data ))) trough_locations = np . where ( doublediff2 == - 2 )[ 0 ] + 1 # Draw Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . plot ( 'date' , 'traffic' , data = df , color = 'tab:blue' , label = 'Air Traffic' ) plt . scatter ( df . date [ peak_locations ], df . traffic [ peak_locations ], marker = mpl . markers . CARETUPBASE , color = 'tab:green' , s = 100 , label = 'Peaks' ) plt . scatter ( df . date [ trough_locations ], df . traffic [ trough_locations ], marker = mpl . markers . CARETDOWNBASE , color = 'tab:red' , s = 100 , label = 'Troughs' ) # Annotate for t , p in zip ( trough_locations [ 1 :: 5 ], peak_locations [:: 3 ]): plt . text ( df . date [ p ], df . traffic [ p ] + 15 , df . date [ p ], horizontalalignment = 'center' , color = 'darkgreen' ) plt . text ( df . date [ t ], df . traffic [ t ] - 35 , df . date [ t ], horizontalalignment = 'center' , color = 'darkred' ) # Decoration plt . ylim ( 50 , 750 ) xtick_location = df . index . tolist ()[:: 6 ] xtick_labels = df . date . tolist ()[:: 6 ] plt . xticks ( ticks = xtick_location , labels = xtick_labels , rotation = 45 , fontsize = 12 , alpha =. 7 ) plt . title ( \"Peak and Troughs of Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . yticks ( fontsize = 12 , alpha =. 7 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . legend ( loc = 'upper left' ) plt . grid ( axis = 'y' , alpha =. 3 ) plt . show () Autocorrelation and partial autocorrelation plot The Autocorrelation plot shows the correlation of the time series with its own lags. Each vertical line (on the autocorrelation plot) represents the correlation between the series and its lag starting from lag 0. The blue shaded region in the plot is the significance level. Those lags that lie above the blue line are the significant lags. Partial autocorrelation plot on the other hand, shows the autocorrelation of any given lag (of time series) against the current series, but with the contributions of the lags-inbetween removed. In [9]: from statsmodels.graphics.tsaplots import plot_acf , plot_pacf # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Draw Plot fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 ), dpi = 80 ) plot_acf ( df . traffic . tolist (), ax = ax1 , lags = 50 ) plot_pacf ( df . traffic . tolist (), ax = ax2 , lags = 20 ) # Decorate # lighten the borders ax1 . spines [ \"top\" ] . set_alpha ( . 3 ); ax2 . spines [ \"top\" ] . set_alpha ( . 3 ) ax1 . spines [ \"bottom\" ] . set_alpha ( . 3 ); ax2 . spines [ \"bottom\" ] . set_alpha ( . 3 ) ax1 . spines [ \"right\" ] . set_alpha ( . 3 ); ax2 . spines [ \"right\" ] . set_alpha ( . 3 ) ax1 . spines [ \"left\" ] . set_alpha ( . 3 ); ax2 . spines [ \"left\" ] . set_alpha ( . 3 ) # font size of tick labels ax1 . tick_params ( axis = 'both' , labelsize = 12 ) ax2 . tick_params ( axis = 'both' , labelsize = 12 ) plt . show () Cross correlation plot Cross correlation plot shows the lags of two time series with each other. In [10]: import statsmodels.tsa.stattools as stattools # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) x = df [ 'mdeaths' ] y = df [ 'fdeaths' ] # Compute Cross Correlations ccs = stattools . ccf ( x , y )[: 100 ] nlags = len ( ccs ) # Compute the Significance level # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-significance-in-r/3128#3128 conf_level = 2 / np . sqrt ( nlags ) # Draw Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . hlines ( 0 , xmin = 0 , xmax = 100 , color = 'gray' ) # 0 axis plt . hlines ( conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . hlines ( - conf_level , xmin = 0 , xmax = 100 , color = 'gray' ) plt . bar ( x = np . arange ( len ( ccs )), height = ccs , width =. 3 ) # Decoration plt . title ( '$Cross\\; Correlation\\; Plot:\\; mdeaths\\; vs\\; fdeaths$' , fontsize = 22 ) plt . xlim ( 0 , len ( ccs )) plt . show () Time series decomposition plot Time series decomposition plot shows the break down of the time series into trend, seasonal and residual components. In [11]: from statsmodels.tsa.seasonal import seasonal_decompose from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) dates = pd . DatetimeIndex ([ parse ( d ) . strftime ( '%Y-%m-01' ) for d in df [ 'date' ]]) df . set_index ( dates , inplace = True ) # Decompose result = seasonal_decompose ( df [ 'traffic' ], model = 'multiplicative' ) # Plot plt . rcParams . update ({ 'figure.figsize' : ( 10 , 10 )}) result . plot () . suptitle ( 'Time Series Decomposition of Air Passengers' ) plt . show () Multiple time series In [12]: # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/mortality.csv' ) # Define the upper limit, lower limit, interval of Y axis and colors y_LL = 100 y_UL = int ( df . iloc [:, 1 :] . max () . max () * 1.1 ) y_interval = 400 mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] for i , column in enumerate ( columns ): plt . plot ( df . date . values , df [ column ], lw = 1.5 , color = mycolors [ i ]) plt . text ( df . shape [ 0 ] + 1 , df [ column ] . values [ - 1 ], column , fontsize = 14 , color = mycolors [ i ]) # Draw Tick lines for y in range ( y_LL , y_UL , y_interval ): plt . hlines ( y , xmin = 0 , xmax = 71 , colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Decorations plt . tick_params ( axis = \"both\" , which = \"both\" , bottom = False , top = False , labelbottom = True , left = False , right = False , labelleft = True ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Number of Deaths from Lung Diseases in the UK (1974-1979)' , fontsize = 22 ) plt . yticks ( range ( y_LL , y_UL , y_interval ), [ str ( y ) for y in range ( y_LL , y_UL , y_interval )], fontsize = 12 ) plt . xticks ( range ( 0 , df . shape [ 0 ], 12 ), df . date . values [:: 12 ], horizontalalignment = 'left' , fontsize = 12 ) plt . ylim ( y_LL , y_UL ) plt . xlim ( - 2 , 80 ) plt . show () Plotting with different scales using secondary Y axis In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) x = df [ 'date' ] y1 = df [ 'psavert' ] y2 = df [ 'unemploy' ] # Plot Line1 (Left Y Axis) fig , ax1 = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) ax1 . plot ( x , y1 , color = 'tab:blue' ) # Plot Line2 (Right Y Axis) ax2 = ax1 . twinx () # instantiate a second axes that shares the same x-axis ax2 . plot ( x , y2 , color = 'tab:red' ) # Decorations # ax1 (left Y axis) ax1 . set_xlabel ( 'Year' , fontsize = 20 ) ax1 . tick_params ( axis = 'x' , rotation = 0 , labelsize = 12 ) ax1 . set_ylabel ( 'Personal Savings Rate' , color = 'tab:blue' , fontsize = 20 ) ax1 . tick_params ( axis = 'y' , rotation = 0 , labelcolor = 'tab:blue' ) ax1 . grid ( alpha =. 4 ) # ax2 (right Y axis) ax2 . set_ylabel ( \"# Unemployed (1000's)\" , color = 'tab:red' , fontsize = 20 ) ax2 . tick_params ( axis = 'y' , labelcolor = 'tab:red' ) ax2 . set_xticks ( np . arange ( 0 , len ( x ), 60 )) ax2 . set_xticklabels ( x [:: 60 ], rotation = 90 , fontdict = { 'fontsize' : 10 }) ax2 . set_title ( \"Personal Savings Rate vs Unemployed: Plotting in Secondary Y Axis\" , fontsize = 22 ) fig . tight_layout () plt . show () Time series with error bands In [14]: from scipy.stats import sem # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\" ) df_mean = df . groupby ( 'order_hour_of_day' ) . quantity . mean () df_se = df . groupby ( 'order_hour_of_day' ) . quantity . apply ( sem ) . mul ( 1.96 ) # Plot plt . figure ( figsize = ( 12 , 7 ), dpi = 80 ) plt . ylabel ( \"# Orders\" , fontsize = 16 ) x = df_mean . index plt . plot ( x , df_mean , color = \"white\" , lw = 2 ) plt . fill_between ( x , df_mean - df_se , df_mean + df_se , color = \"#3F5D7D\" ) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 1 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 1 ) plt . xticks ( x [:: 2 ], [ str ( d ) for d in x [:: 2 ]] , fontsize = 12 ) plt . title ( \"User Orders by Hour of Day (95 % c onfidence)\" , fontsize = 22 ) plt . xlabel ( \"Hour of Day\" ) s , e = plt . gca () . get_xlim () plt . xlim ( s , e ) # Draw Horizontal Tick lines for y in range ( 8 , 20 , 2 ): plt . hlines ( y , xmin = s , xmax = e , colors = 'black' , alpha = 0.5 , linestyles = \"--\" , lw = 0.5 ) plt . show () Stacked area chart Stacked area chart gives an visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other. In [15]: # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/nightvisitors.csv' ) # Decide Colors mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] # Draw Plot and Annotate fig , ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 9 ), dpi = 80 ) columns = df . columns [ 1 :] labs = columns . values . tolist () # Prepare data x = df [ 'yearmon' ] . values . tolist () y0 = df [ columns [ 0 ]] . values . tolist () y1 = df [ columns [ 1 ]] . values . tolist () y2 = df [ columns [ 2 ]] . values . tolist () y3 = df [ columns [ 3 ]] . values . tolist () y4 = df [ columns [ 4 ]] . values . tolist () y5 = df [ columns [ 5 ]] . values . tolist () y6 = df [ columns [ 6 ]] . values . tolist () y7 = df [ columns [ 7 ]] . values . tolist () y = np . vstack ([ y0 , y2 , y4 , y6 , y7 , y5 , y1 , y3 ]) # Plot for each column labs = columns . values . tolist () ax = plt . gca () ax . stackplot ( x , y , labels = labs , colors = mycolors , alpha = 0.8 ) # Decorations ax . set_title ( 'Night Visitors in Australian Regions' , fontsize = 18 ) ax . set ( ylim = [ 0 , 100000 ]) ax . legend ( fontsize = 10 , ncol = 4 ) plt . xticks ( x [:: 5 ], fontsize = 10 , rotation = 45 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 10000 , 100000 , 20000 ), fontsize = 10 ) plt . xlim ( x [ 0 ], x [ - 1 ]) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Area chart unstacked An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In [16]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" ) # Prepare Data x = df [ 'date' ] . values . tolist () y1 = df [ 'psavert' ] . values . tolist () y2 = df [ 'uempmed' ] . values . tolist () mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' ] columns = [ 'psavert' , 'uempmed' ] # Draw Plot fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 7 ), dpi = 80 ) ax . fill_between ( x , y1 = y1 , y2 = 0 , label = columns [ 1 ], alpha = 0.5 , color = mycolors [ 1 ], linewidth = 2 ) ax . fill_between ( x , y1 = y2 , y2 = 0 , label = columns [ 0 ], alpha = 0.5 , color = mycolors [ 0 ], linewidth = 2 ) # Decorations ax . set_title ( 'Personal Savings Rate vs Median Duration of Unemployment' , fontsize = 18 ) ax . set ( ylim = [ 0 , 30 ]) ax . legend ( loc = 'best' , fontsize = 12 ) plt . xticks ( x [:: 50 ], fontsize = 10 , horizontalalignment = 'center' ) plt . yticks ( np . arange ( 2.5 , 30.0 , 2.5 ), fontsize = 10 ) plt . xlim ( - 10 , x [ - 1 ]) # Draw Tick lines for y in np . arange ( 2.5 , 30.0 , 2.5 ): plt . hlines ( y , xmin = 0 , xmax = len ( x ), colors = 'black' , alpha = 0.3 , linestyles = \"--\" , lw = 0.5 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . show () Calender heat map Calendar map is an alternate and a less preferred option to visualise time based data compared to a time series. Though can be visually appealing, the numeric values are not quite evident. It is however effective in picturising the extreme values and holiday effects nicely. In [17]: import matplotlib as mpl import calmap # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\" , parse_dates = [ 'date' ]) df . set_index ( 'date' , inplace = True ) # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) calmap . calendarplot ( df [ '2014' ][ 'VIX.Close' ], fig_kws = { 'figsize' : ( 16 , 10 )}, yearlabel_kws = { 'color' : 'black' , 'fontsize' : 14 }, subplot_kws = { 'title' : 'Yahoo Stock Prices' }) plt . show () <Figure size 1280x800 with 0 Axes> Seasonal plot The seasonal plot can be used to compare how the time series performed at same day in the previous season (year / month / week etc). In [18]: from dateutil.parser import parse # Import Data df = pd . read_csv ( 'https://github.com/selva86/datasets/raw/master/AirPassengers.csv' ) # Prepare data df [ 'year' ] = [ parse ( d ) . year for d in df . date ] df [ 'month' ] = [ parse ( d ) . strftime ( '%b' ) for d in df . date ] years = df [ 'year' ] . unique () # Draw Plot mycolors = [ 'tab:red' , 'tab:blue' , 'tab:green' , 'tab:orange' , 'tab:brown' , 'tab:grey' , 'tab:pink' , 'tab:olive' , 'deeppink' , 'steelblue' , 'firebrick' , 'mediumseagreen' ] plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) for i , y in enumerate ( years ): plt . plot ( 'month' , 'traffic' , data = df . loc [ df . year == y , :], color = mycolors [ i ], label = y ) plt . text ( df . loc [ df . year == y , :] . shape [ 0 ] -. 9 , df . loc [ df . year == y , 'traffic' ][ - 1 :] . values [ 0 ], y , fontsize = 12 , color = mycolors [ i ]) # Decoration plt . ylim ( 50 , 750 ) plt . xlim ( - 0.3 , 11 ) plt . ylabel ( '$Air Traffic$' ) plt . yticks ( fontsize = 12 , alpha =. 7 ) plt . title ( \"Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)\" , fontsize = 22 ) plt . grid ( axis = 'y' , alpha =. 3 ) # Remove borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( 0.5 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0.0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( 0.5 ) # plt.legend(loc='upper right', ncol=2, fontsize=12) plt . show () 7. Groups Dendrogram A Dendrogram groups similar points together based on a given distance metric and organizes them in tree like links based on the point's similarity. In [19]: import scipy.cluster.hierarchy as shc # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) plt . title ( \"USArrests Dendograms\" , fontsize = 22 ) dend = shc . dendrogram ( shc . linkage ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]], method = 'ward' ), labels = df . State . values , color_threshold = 100 ) plt . xticks ( fontsize = 12 ) plt . show () Cluster plot In [20]: from sklearn.cluster import AgglomerativeClustering from scipy.spatial import ConvexHull # Import Data df = pd . read_csv ( 'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv' ) # Agglomerative Clustering cluster = AgglomerativeClustering ( n_clusters = 5 , affinity = 'euclidean' , linkage = 'ward' ) cluster . fit_predict ( df [[ 'Murder' , 'Assault' , 'UrbanPop' , 'Rape' ]]) # Plot plt . figure ( figsize = ( 13 , 8 ), dpi = 80 ) plt . scatter ( df . iloc [:, 0 ], df . iloc [:, 1 ], c = cluster . labels_ , cmap = 'tab10' ) # Encircle def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices ,:], ** kw ) ax . add_patch ( poly ) # Draw polygon surrounding vertices encircle ( df . loc [ cluster . labels_ == 0 , 'Murder' ], df . loc [ cluster . labels_ == 0 , 'Assault' ], ec = \"k\" , fc = \"gold\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 1 , 'Murder' ], df . loc [ cluster . labels_ == 1 , 'Assault' ], ec = \"k\" , fc = \"tab:blue\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 2 , 'Murder' ], df . loc [ cluster . labels_ == 2 , 'Assault' ], ec = \"k\" , fc = \"tab:red\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 3 , 'Murder' ], df . loc [ cluster . labels_ == 3 , 'Assault' ], ec = \"k\" , fc = \"tab:green\" , alpha = 0.2 , linewidth = 0 ) encircle ( df . loc [ cluster . labels_ == 4 , 'Murder' ], df . loc [ cluster . labels_ == 4 , 'Assault' ], ec = \"k\" , fc = \"tab:orange\" , alpha = 0.2 , linewidth = 0 ) # Decorations plt . xlabel ( 'Murder' ); plt . xticks ( fontsize = 12 ) plt . ylabel ( 'Assault' ); plt . yticks ( fontsize = 12 ) plt . title ( 'Agglomerative Clustering of USArrests (5 Groups)' , fontsize = 22 ) plt . show () Andrews curve Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) doesn't help discriminate the group (cyl), then the lines will not be well segregated as you see below. In [21]: from pandas.plotting import andrews_curves # Import df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) df . drop ([ 'cars' , 'carname' ], axis = 1 , inplace = True ) # Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) andrews_curves ( df , 'cyl' , colormap = 'Set1' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Andrews Curves of mtcars' , fontsize = 22 ) plt . xlim ( - 3 , 3 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Parallel coordinates Parallel coordinates helps to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group. In [22]: from pandas.plotting import parallel_coordinates # Import Data df_final = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\" ) # Plot plt . figure ( figsize = ( 12 , 8 ), dpi = 80 ) parallel_coordinates ( df_final , 'cut' , colormap = 'Dark2' ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . title ( 'Parallel Coordinated of Diamonds' , fontsize = 18 ) plt . grid ( alpha = 0.3 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show ()","tags":"Data Science","url":"https://haochen23.github.io/2018/12/50-visualization-part3.html","loc":"https://haochen23.github.io/2018/12/50-visualization-part3.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part II Introduction This is the second post of the valuable Matplotlib & Seaborn Visualization Handbook. This post will focus on the third and forth categories discussed in the last post, as follows: Ranking Ordered bar chart Lollipop chart Dot plot Slope chart Dumbbell plot Distribution Histogram for continuous variables Histogram for categorical variables Density plot Density curves with histogram Joy plot Distributed dot plot Box plot Dot+box plot Violin plot Population pyramid Categorical plots Before getting into it, let's first import neccessary libraries and plot settings. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 3. Ranking Ordered bar chart Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself. In [2]: import matplotlib.patches as patches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) fig , ax = plt . subplots ( figsize = ( 16 , 10 ), facecolor = 'white' , dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 20 ) # Annotate Text for i , cty in enumerate ( df . cty ): ax . text ( i , cty + 0.5 , round ( cty , 1 ), horizontalalignment = 'center' ) # Title, Label, Ticks and Ylim ax . set_title ( 'Bar Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set ( ylabel = 'Miles Per Gallon' , ylim = ( 0 , 30 )) plt . xticks ( df . index , df . manufacturer . str . upper (), rotation = 60 , horizontalalignment = 'right' , fontsize = 12 ) # Add patches to color the X axis labels fig . patches . extend ([ plt . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure )]) fig . patches . extend ([ plt . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure )]) plt . show () Lollipop chart Lollipop chart serves a similar purpose as a ordered bar chart. In [3]: # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot fig , ax = plt . subplots ( figsize = ( 14 , 10 ), dpi = 80 ) ax . vlines ( x = df . index , ymin = 0 , ymax = df . cty , color = 'firebrick' , alpha = 0.7 , linewidth = 2 ) ax . scatter ( x = df . index , y = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Title, Label, Ticks and Ylim ax . set_title ( 'Lollipop Chart for Highway Mileage' , fontdict = { 'size' : 22 }) ax . set_ylabel ( 'Miles Per Gallon' ) ax . set_xticks ( df . index ) ax . set_xticklabels ( df . manufacturer . str . upper (), rotation = 60 , fontdict = { 'horizontalalignment' : 'right' , 'size' : 12 }) ax . set_ylim ( 0 , 30 ) # Annotate for row in df . itertuples (): ax . text ( row . Index , row . cty +. 5 , s = round ( row . cty , 2 ), horizontalalignment = 'center' , verticalalignment = 'bottom' , fontsize = 14 ) fig . patches . extend ([ plt . Rectangle (( . 57 , - 0.005 ), width =. 33 , height =. 13 , alpha =. 1 , facecolor = 'green' , transform = fig . transFigure )]) fig . patches . extend ([ plt . Rectangle (( . 124 , - 0.005 ), width =. 446 , height =. 13 , alpha =. 1 , facecolor = 'red' , transform = fig . transFigure )]) plt . show () Dot Plot The dot plot is another way to present rankings. An example is presented below. In [4]: # Import data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , inplace = True ) df . reset_index ( inplace = True ) # Plot fig , ax = plt . subplots ( figsize = ( 14 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 11 , xmax = 26 , color = 'gray' , alpha = 0.7 , linewidth = 1 , linestyle = 'dashdot' ) ax . scatter ( y = df . index , x = df . cty , s = 75 , color = 'firebrick' , alpha = 0.7 ) # Decorations ax . set_title ( 'Dot plot for Highway Mileage' , fontdict = { 'size' : 18 }) ax . set_xlabel ( 'Miles Per Gallon' ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }) ax . set_xlim ( 10 , 27 ) plt . show () Slope chart Slope chart is most suitable for comparing the ‘Before' and ‘After' positions of a given person/item. In [5]: import matplotlib.lines as mlines # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/gdppercap.csv\" ) left_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1952' ])] right_label = [ str ( c ) + ', ' + str ( round ( y )) for c , y in zip ( df . continent , df [ '1957' ])] klass = [ 'red' if ( y1 - y2 ) < 0 else 'green' for y1 , y2 in zip ( df [ '1952' ], df [ '1957' ])] # draw line # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-matplotlib/36479941 def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'red' if p1 [ 1 ] - p2 [ 1 ] > 0 else 'green' , marker = 'o' , markersize = 6 ) ax . add_line ( l ) return l fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), dpi = 80 ) # Vertical Lines ax . vlines ( x = 1 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x = 3 , ymin = 500 , ymax = 13000 , color = 'black' , alpha = 0.7 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ '1952' ], x = np . repeat ( 1 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) ax . scatter ( y = df [ '1957' ], x = np . repeat ( 3 , df . shape [ 0 ]), s = 10 , color = 'black' , alpha = 0.7 ) # Line Segmentsand Annotation for p1 , p2 , c in zip ( df [ '1952' ], df [ '1957' ], df [ 'continent' ]): newline ([ 1 , p1 ], [ 3 , p2 ]) ax . text ( 1 - 0.05 , p1 , c + ', ' + str ( round ( p1 )), horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) ax . text ( 3 + 0.05 , p2 , c + ', ' + str ( round ( p2 )), horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 14 }) # 'Before' and 'After' Annotations ax . text ( 1 - 0.05 , 13000 , 'BEFORE' , horizontalalignment = 'right' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) ax . text ( 3 + 0.05 , 13000 , 'AFTER' , horizontalalignment = 'left' , verticalalignment = 'center' , fontdict = { 'size' : 18 , 'weight' : 700 }) # Decoration ax . set_title ( \"Slopechart: Comparing GDP Per Capita between 1952 vs 1957\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , 4 ), ylim = ( 0 , 14000 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ 1 , 3 ]) ax . set_xticklabels ([ \"1952\" , \"1957\" ]) plt . yticks ( np . arange ( 500 , 13000 , 2000 ), fontsize = 12 ) # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 0 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 0 ) plt . show () Dumbell plot Dumbell plot conveys the ‘before' and ‘after' positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project / initiative on different objects. In [6]: import matplotlib.lines as mlines # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/health.csv\" ) df . sort_values ( 'pct_2014' , inplace = True ) df . reset_index ( inplace = True ) # Func to draw line segment def newline ( p1 , p2 , color = 'black' ): ax = plt . gca () l = mlines . Line2D ([ p1 [ 0 ], p2 [ 0 ]], [ p1 [ 1 ], p2 [ 1 ]], color = 'skyblue' ) ax . add_line ( l ) return l # Figure and Axes fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 14 ), facecolor = '#f7f7f7' , dpi = 80 ) # Vertical Lines ax . vlines ( x =. 05 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 10 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 15 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) ax . vlines ( x =. 20 , ymin = 0 , ymax = 26 , color = 'black' , alpha = 1 , linewidth = 1 , linestyles = 'dotted' ) # Points ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2013' ], s = 50 , color = '#0e668b' , alpha = 0.7 ) ax . scatter ( y = df [ 'index' ], x = df [ 'pct_2014' ], s = 50 , color = '#a3c4dc' , alpha = 0.7 ) # Line Segments for i , p1 , p2 in zip ( df [ 'index' ], df [ 'pct_2013' ], df [ 'pct_2014' ]): newline ([ p1 , i ], [ p2 , i ]) # Decoration ax . set_facecolor ( '#f7f7f7' ) ax . set_title ( \"Dumbell Chart: Pct Change - 2013 vs 2014\" , fontdict = { 'size' : 22 }) ax . set ( xlim = ( 0 , . 25 ), ylim = ( - 1 , 27 ), ylabel = 'Mean GDP Per Capita' ) ax . set_xticks ([ . 05 , . 1 , . 15 , . 20 ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) ax . set_xticklabels ([ '5%' , '15%' , '20%' , '25%' ]) plt . show () 4. Distribution Histogram for continuous variable Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem. In [7]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'displ' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , 30 , stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 25 ) plt . xticks ( bins [:: 3 ], [ round ( b , 1 ) for b in bins [:: 3 ]]) plt . show () Histogram for categorical variable The histogram of a categorical variable shows the frequency distribution of a that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors. In [8]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Prepare data x_var = 'manufacturer' groupby_var = 'class' df_agg = df . loc [:, [ x_var , groupby_var ]] . groupby ( groupby_var ) vals = [ df [ x_var ] . values . tolist () for i , df in df_agg ] # Draw plt . figure ( figsize = ( 16 , 9 ), dpi = 80 ) colors = [ plt . cm . Spectral ( i / float ( len ( vals ) - 1 )) for i in range ( len ( vals ))] n , bins , patches = plt . hist ( vals , df [ x_var ] . unique () . __len__ (), stacked = True , density = False , color = colors [: len ( vals )]) # Decoration plt . legend ({ group : col for group , col in zip ( np . unique ( df [ groupby_var ]) . tolist (), colors [: len ( vals )])}) plt . title ( f \"Stacked Histogram of $ { x_var } $ colored by $ { groupby_var } $\" , fontsize = 22 ) plt . xlabel ( x_var ) plt . ylabel ( \"Frequency\" ) plt . ylim ( 0 , 40 ) plt . xticks ( bins , np . unique ( df [ x_var ]) . tolist (), rotation = 90 , horizontalalignment = 'left' ) plt . show () Density plot Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the ‘response' variable, you can inspect the relationship between the X and the Y. The below case if for representational purpose to describe how the distribution of city mileage varies with respect the number of cylinders. In [9]: import warnings warnings . filterwarnings ( \"ignore\" ) # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 14 , 8 ), dpi = 80 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 4 , \"cty\" ], shade = True , color = \"g\" , label = \"Cyl=4\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 5 , \"cty\" ], shade = True , color = \"deeppink\" , label = \"Cyl=5\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 6 , \"cty\" ], shade = True , color = \"dodgerblue\" , label = \"Cyl=6\" , alpha =. 7 ) sns . kdeplot ( df . loc [ df [ 'cyl' ] == 8 , \"cty\" ], shade = True , color = \"orange\" , label = \"Cyl=8\" , alpha =. 7 ) # Decoration plt . title ( 'Density Plot of City Mileage by n_Cylinders' , fontsize = 18 ) plt . xlabel ( 'City Mileage' , fontsize = 18 ) plt . ylabel ( 'Density' , fontsize = 18 ) plt . legend () plt . show () Density curves with histogram Density curve with histogram brings together the collective information conveyed by the two plots so you can have and compare them both in a single figure instead of two. In [10]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 8 ), dpi = 80 ) sns . distplot ( df . loc [ df [ 'class' ] == 'compact' , \"cty\" ], color = \"dodgerblue\" , label = \"Compact\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'suv' , \"cty\" ], color = \"orange\" , label = \"SUV\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) sns . distplot ( df . loc [ df [ 'class' ] == 'minivan' , \"cty\" ], color = \"g\" , label = \"minivan\" , hist_kws = { 'alpha' : . 7 }, kde_kws = { 'linewidth' : 3 }) plt . ylim ( 0 , 0.35 ) # Decoration plt . title ( 'Density Plot of City Mileage by Vehicle Type' , fontsize = 18 ) plt . xlabel ( 'City Mileage' , fontsize = 18 ) plt . ylabel ( 'Density' , fontsize = 18 ) plt . legend () plt . show () Joy plot Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib In [11]: import joypy # Import Data mpg = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 14 , 9 ), dpi = 80 ) fig , axes = joypy . joyplot ( mpg , column = [ 'hwy' , 'cty' ], by = \"class\" , ylim = 'own' , fade = True , figsize = ( 14 , 10 )) # Decoration plt . title ( 'Joy Plot of City and Highway Mileage by Class' , fontsize = 18 ) plt . legend ([ 'hwy' , 'cty' ]) plt . show () <Figure size 1120x720 with 0 Axes> Distriubted dot plot Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly. In [12]: import matplotlib.patches as mpatches # Prepare Data df_raw = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) cyl_colors = { 4 : 'tab:red' , 5 : 'tab:green' , 6 : 'tab:blue' , 8 : 'tab:orange' } df_raw [ 'cyl_color' ] = df_raw . cyl . map ( cyl_colors ) # Mean and Median city mileage by make df = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . mean ()) df . sort_values ( 'cty' , ascending = False , inplace = True ) df . reset_index ( inplace = True ) df_median = df_raw [[ 'cty' , 'manufacturer' ]] . groupby ( 'manufacturer' ) . apply ( lambda x : x . median ()) # Draw horizontal lines fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) ax . hlines ( y = df . index , xmin = 0 , xmax = 40 , color = 'gray' , alpha = 0.5 , linewidth =. 5 , linestyles = 'dashdot' ) # Draw the Dots for i , make in enumerate ( df . manufacturer ): df_make = df_raw . loc [ df_raw . manufacturer == make , :] ax . scatter ( y = np . repeat ( i , df_make . shape [ 0 ]), x = 'cty' , data = df_make , s = 75 , edgecolors = 'gray' , c = 'w' , alpha = 0.5 ) ax . scatter ( y = i , x = 'cty' , data = df_median . loc [ df_median . index == make , :], s = 75 , c = 'firebrick' ) # Annotate ax . text ( 33 , 13 , \"$red \\; dots \\; are \\; the \\: median$\" , fontdict = { 'size' : 12 }, color = 'firebrick' ) # Decorations red_patch = plt . plot ([],[], marker = \"o\" , ms = 10 , ls = \"\" , mec = None , color = 'firebrick' , label = \"Median\" ) plt . legend ( handles = red_patch ) ax . set_title ( 'Distribution of City Mileage by Make' , fontdict = { 'size' : 22 }) ax . set_xlabel ( 'Miles Per Gallon (City)' , alpha = 0.7 ) ax . set_yticks ( df . index ) ax . set_yticklabels ( df . manufacturer . str . title (), fontdict = { 'horizontalalignment' : 'right' }, alpha = 0.7 ) ax . set_xlim ( 1 , 40 ) plt . xticks ( alpha = 0.7 ) plt . gca () . spines [ \"top\" ] . set_visible ( False ) plt . gca () . spines [ \"bottom\" ] . set_visible ( False ) plt . gca () . spines [ \"right\" ] . set_visible ( False ) plt . gca () . spines [ \"left\" ] . set_visible ( False ) plt . grid ( axis = 'both' , alpha =. 4 , linewidth =. 1 ) plt . show () Box plot Box plots are a great way to visualize the distribution, keeping the median, 25th, and 75th quartiles and the outliers in mind. However, you need to be careful about interpreting the size the boxes which can potentially distort the number of points contained within that group. So, manually providing the number of observations in each box can help overcome this drawback. We can use sns.boxplot to draw the box plot. For example, the first two boxes on the left have boxes of the same size even though they have 5 and 47 obs respectively. So writing the number of observations in that group becomes necessary. In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) sns . boxplot ( x = 'class' , y = 'hwy' , data = df , notch = False ) # Add N Obs inside boxplot (optional) def add_n_obs ( df , group_col , y ): medians_dict = { grp [ 0 ]: grp [ 1 ][ y ] . median () for grp in df . groupby ( group_col )} xticklabels = [ x . get_text () for x in plt . gca () . get_xticklabels ()] n_obs = df . groupby ( group_col )[ y ] . size () . values for ( x , xticklabel ), n_ob in zip ( enumerate ( xticklabels ), n_obs ): plt . text ( x , medians_dict [ xticklabel ] * 1.01 , \"#obs : \" + str ( n_ob ), horizontalalignment = 'center' , fontdict = { 'size' : 14 }, color = 'white' ) add_n_obs ( df , group_col = 'class' , y = 'hwy' ) # Decoration plt . title ( 'Box Plot of Highway Mileage by Vehicle Class' , fontsize = 22 ) plt . ylim ( 10 , 40 ) plt . show () Population pyramid Population pyramid can be used to show either the distribution of the groups ordered by the volumne. Or it can also be used to show the stage-by-stage filtering of the population as it is used below to show how many people pass through each stage of a marketing funnel. In [14]: # Read data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/email_campaign_funnel.csv\" ) # Draw Plot plt . figure ( figsize = ( 13 , 10 ), dpi = 80 ) group_col = 'Gender' order_of_bars = df . Stage . unique ()[:: - 1 ] colors = [ plt . cm . Spectral ( i / float ( len ( df [ group_col ] . unique ()) - 1 )) for i in range ( len ( df [ group_col ] . unique ()))] for c , group in zip ( colors , df [ group_col ] . unique ()): sns . barplot ( x = 'Users' , y = 'Stage' , data = df . loc [ df [ group_col ] == group , :], order = order_of_bars , color = c , label = group ) # Decorations plt . xlabel ( \"$Users$\" ) plt . ylabel ( \"Stage of Purchase\" ) plt . yticks ( fontsize = 10 ) plt . title ( \"Population Pyramid of the Marketing Funnel\" , fontsize = 22 ) plt . legend () plt . show () Categorical plots Categorical plots can be used to visualize the counts distribution of 2 or more categorical variables. It can be done using sns.catplot(). In [15]: # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot g = sns . catplot ( \"alive\" , col = \"deck\" , col_wrap = 4 , data = titanic [ titanic . deck . notnull ()], kind = \"count\" , height = 3.5 , aspect =. 8 , palette = 'tab20' ) fig . suptitle ( 'sf' ) plt . show () In [16]: # Load Dataset titanic = sns . load_dataset ( \"titanic\" ) # Plot sns . catplot ( x = \"age\" , y = \"embark_town\" , hue = \"sex\" , col = \"class\" , data = titanic [ titanic . embark_town . notnull ()], orient = \"h\" , height = 5 , aspect = 1 , palette = \"tab10\" , kind = \"violin\" , dodge = True , cut = 0 , bw =. 2 ) plt . show ()","tags":"Data Science","url":"https://haochen23.github.io/2018/11/50-visualization-part2.html","loc":"https://haochen23.github.io/2018/11/50-visualization-part2.html"},{"title":"Valuable Matplotlib & Seaborn Visualization Handbook, Part I","text":"Valuable Matplotlib & Seaborn Visualization Handbook, Part I 1. Introduction This post summarizes the top 50 most valuable Matplotlib & Seaborn data visualizations in data science. It can be taken as a data visualization handbook for you to look up for useful visulaization. The 50 visualizations are categorized into 7 different application scenarios, and this post would mainly focuses on the first two categories, shown as follows: Correlation Scatter plot Bubble plot with Encircling Scatter plot with line of the best fit Jittering with stripplot Counts plot Marginal histogram Marginal boxplot Correlogram Pairwise plot Deviation Diverging bars Diverging texts Diverging dot plot Diverging Lollipop chart with markers Area chart Ranking Ordered bar chart Lollipop chart Dot plot Slope chart Dumbbell plot Distribution Histogram for continuous variables Histogram for categorical variables Density plot Density curves with histogram Joy plot Distributed dot plot Box plot Dot+box plot Violin plot Population pyramid Categorical plots Composition Waffle chart Pie chart Treemap Bar chart Change Time series plot Time series with peaks and troughs annotated Autocorrelation plot Cross correlation plot Time series decomposition plot Multiple time series Plotting with different scales using secondary Y axis Time series with error bands Stacked area chart Unstacked area chart Calender heat map Seasonal plot Groups Dendrogram Cluster plot Andrews curve Parallel coordinates Before heading into the 50 visualizations, let's first import neccesary libraries and plot settings. In [1]: import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns large = 22 med = 16 small = 12 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . rcParams . update ( params ) plt . style . use ( 'seaborn-whitegrid' ) sns . set_style ( 'white' ) import warnings warnings . filterwarnings ( action = 'once' ) % matplotlib inline 2. Correlation The plot of correlation can used to visualize the correlation between two or more variables. Scatter plot Scatter plot is a classical plot to visualize the correlation between variables. It can be done using plt.scatterplot(). In [2]: #import dataset midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) #prepare data #create as many colors as there are unique midewest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] #draw plot for each category plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 20 , cmap = colors [ i ], label = str ( category )) #decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Bubble plot with Encircling This type of plot shows the significance within a circular range. Although we use the data same as we did for the scatter plot above, the data importing procedure is done once again to keep the visualization as a whole process. In [3]: from matplotlib import patches from scipy.spatial import ConvexHull #import data midwest = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/midwest_filter.csv\" ) # As many colors as there are unique midwest['category'] categories = np . unique ( midwest [ 'category' ]) colors = [ plt . cm . tab10 ( i / float ( len ( categories ) - 1 )) for i in range ( len ( categories ))] #Step 2: draw scatterplot with unique color for each category fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) for i , category in enumerate ( categories ): plt . scatter ( 'area' , 'poptotal' , data = midwest . loc [ midwest . category == category , :], s = 'dot_size' , cmap = colors [ i ], label = str ( category ), edgecolors = 'black' , linewidths = . 5 ) #Step 3: Encircling def encircle ( x , y , ax = None , ** kw ): if not ax : ax = plt . gca () p = np . c_ [ x , y ] hull = ConvexHull ( p ) poly = plt . Polygon ( p [ hull . vertices , :], ** kw ) ax . add_patch ( poly ) #Select the data to be encircled midwest_encircle_data = midwest . loc [ midwest . state == 'IN' , :] #Draw polygon surrounding vertices encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = 'k' , fc = 'gold' , alpha = 0.1 ) encircle ( midwest_encircle_data . area , midwest_encircle_data . poptotal , ec = 'firebrick' , fc = 'none' , linewidth = 1.5 ) #decorations plt . gca () . set ( xlim = ( 0.0 , 0.1 ), ylim = ( 0 , 90000 ), xlabel = 'Area' , ylabel = 'Population' ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . title ( \"Scatterplot of Midwest Area vs Population\" , fontsize = 22 ) plt . legend ( fontsize = 12 ) plt . show () Scatter plot with line of the best fit We use linear regression to find the best fit line. It can be used to visualize how two variables interact with each other. If you only want to fit one single line for the entire dataset, delete the parameter hue = 'cyl' in sns.lmplot(). In [4]: import warnings warnings . simplefilter ( 'ignore' ) #Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = 'displ' , y = 'hwy' , hue = 'cyl' , data = df_select , size = 7 , aspect = 1.6 , robust = True , palette = 'tab10' , scatter_kws = dict ( s = 60 , linewidths = . 7 , edgecolors = 'black' )) #Decorations gridobj . set ( xlim = [ 0.5 , 7.5 ], ylim = [ 0 , 50 ]) plt . title ( \"Scatterplot with line of best fit grouped by number of cylinders\" , fontsize = 20 ) plt . show () Or we can plot the best fit line for each column. This can be done by setting col = groupingcolumn in sns.lmplot(). In [5]: # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_select = df . loc [ df . cyl . isin ([ 4 , 8 ]), :] # Each line in its own column sns . set_style ( \"white\" ) gridobj = sns . lmplot ( x = \"displ\" , y = \"hwy\" , data = df_select , size = 7 , robust = True , palette = 'Set1' , col = \"cyl\" , scatter_kws = dict ( s = 60 , linewidths = . 7 , edgecolors = 'black' )) # Decorations gridobj . set ( xlim = ( 0.5 , 7.5 ), ylim = ( 0 , 50 )) plt . show () Jittering with stripplot In order to avoid hidden data points in the plot, usually due to the same or similar value, we use jittering plot to see all the overlapping data points. This can be done using stripplot() from seaborn. In [6]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df . cty , df . hwy , jitter = 0.25 , size = 8 , ax = ax , linewidth = 0.5 ) # Decorations plt . title ( \"Use jittered plots to avoid overlapping of points\" , fontsize = 20 ) plt . show () Counts plot This is another approach to visualize overlapping points. The bigger the ploted point is, the more data points are located nearby. In [7]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) df_counts = df . groupby ([ 'hwy' , 'cty' ]) . size () . reset_index ( name = 'counts' ) # Draw Stripplot fig , ax = plt . subplots ( figsize = ( 16 , 10 ), dpi = 80 ) sns . stripplot ( df_counts . cty , df_counts . hwy , size = df_counts . counts * 2 , ax = ax ) # Decorations plt . title ( \"Counts plot - Size of circle is bigger as more points overlap\" , fontsize = 20 ) plt . show () Marginal histogram A plot of marginal histogram has two histograms along X and Y axes. This plot can not only visualize the relationship between X and Y, but also their separate distribution. This plot is often used in exploratory data analysis (EDA). In [8]: # Import data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) #Create figure and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) #Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) #Scatterplot on the main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 4 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha = 0.9 , data = df , cmap = \"tab10\" , edgecolors = 'gray' , linewidths = 0.5 ) #histogram in the bottom ax_bottom . hist ( df . displ , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'b' ) ax_bottom . invert_yaxis () #histogram on the right ax_right . hist ( df . hwy , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'b' ) #Decorations ax_main . set ( title = \"Scatterplot with Histograms \\n displ vs hwy\" , xlabel = 'displ' , ylabel = 'hwy' ) ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) xlabels = ax_main . get_xticks () . tolist () ax_main . set_xticklabels ( xlabels ) plt . show () Mariginal boxplot Marginal boxplot allows to see the median and percentiles of the two variables, X and Y. In [9]: # Import Data df = pd . read_csv ( \"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\" ) # Create Fig and gridspec fig = plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) grid = plt . GridSpec ( 4 , 4 , hspace = 0.5 , wspace = 0.2 ) # Define the axes ax_main = fig . add_subplot ( grid [: - 1 , : - 1 ]) ax_right = fig . add_subplot ( grid [: - 1 , - 1 ], xticklabels = [], yticklabels = []) ax_bottom = fig . add_subplot ( grid [ - 1 , 0 : - 1 ], xticklabels = [], yticklabels = []) # Scatterplot on main ax ax_main . scatter ( 'displ' , 'hwy' , s = df . cty * 5 , c = df . manufacturer . astype ( 'category' ) . cat . codes , alpha =. 9 , data = df , cmap = \"Set1\" , edgecolors = 'black' , linewidths =. 5 ) # Add a boxplot in each part sns . boxplot ( df . hwy , ax = ax_right , orient = 'v' ) sns . boxplot ( df . displ , ax = ax_bottom , orient = 'h' ) # Decorations ------------------ # Remove x axis name for the boxplot ax_bottom . set ( xlabel = '' ) ax_right . set ( ylabel = '' ) # Main Title, Xlabel and YLabel ax_main . set ( title = 'Scatterplot with Histograms \\n displ vs hwy' , xlabel = 'displ' , ylabel = 'hwy' ) # Set font size of different components ax_main . title . set_fontsize ( 20 ) for item in ([ ax_main . xaxis . label , ax_main . yaxis . label ] + ax_main . get_xticklabels () + ax_main . get_yticklabels ()): item . set_fontsize ( 14 ) plt . show () Correlogram Correlogram is used to show the correlation matrix between all possible pairs of numeric variables in a given dataframe. We can use the sns.heatmap() to draw the plot. In [10]: # Import Dataset df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) # Plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) sns . heatmap ( df . corr (), xticklabels = df . corr () . columns , yticklabels = df . corr () . columns , cmap = 'RdYlGn' , center = 0 , annot = True ) # Decorations plt . title ( \"Correlogram of mtcars\" , fontsize = 20 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . show () Pairwise plot Pairwise plot is another way to visualize all possible relationships between each pair of numeric variables. We can use sns.pairplot() to generate the plot. It's easy and nice. First, let's use the scatter plot in the pairplot. In [11]: # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"scatter\" , hue = \"species\" , plot_kws = dict ( s = 80 , edgecolor = \"white\" , linewidth = 2.5 )) plt . show () <Figure size 800x640 with 0 Axes> Then, let's see the pairplot with regression lines. In [12]: # Load Dataset df = sns . load_dataset ( 'iris' ) # Plot plt . figure ( figsize = ( 10 , 8 ), dpi = 80 ) sns . pairplot ( df , kind = \"reg\" , hue = \"species\" ) plt . show () <Figure size 800x640 with 0 Axes> 2. Deviation Diverging bars The diverging bars plot is a useful tool if you want to visualize how your observations behave based on a single variable. In [13]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] # Calculate the z score of mpg df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 5 ) # Decorations plt . gca () . set ( ylabel = '$Model$' , xlabel = '$Mileage$' ) plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Diverging texts Diverging texts is similar to diverging bars and it is preferred if you want to show the exact value of eahc observationwithin the chart in a nice and presentable way. In [14]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'green' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 2 ), horizontalalignment = 'right' if x < 0 else 'left' , verticalalignment = 'center' , fontdict = { 'color' : 'red' if x < 0 else 'green' , 'size' : 14 }) # Decorations plt . yticks ( df . index , df . cars , fontsize = 12 ) plt . title ( 'Diverging Text Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging dot plot The dot plot changes the lines into dots. The absence of bars reduces the amount of contrast and disparity between observations. In my opinion, it is not as good as the above two plots, but it is still an option. In [15]: # Import Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = [ 'red' if x < 0 else 'darkgreen' for x in df [ 'mpg_z' ]] df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot plt . figure ( figsize = ( 12 , 10 ), dpi = 80 ) plt . scatter ( df . mpg_z , df . index , s = 450 , alpha =. 6 , color = df . colors ) for x , y , tex in zip ( df . mpg_z , df . index , df . mpg_z ): t = plt . text ( x , y , round ( tex , 1 ), horizontalalignment = 'center' , verticalalignment = 'center' , fontdict = { 'color' : 'white' }) # Decorations # Lighten borders plt . gca () . spines [ \"top\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"bottom\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"right\" ] . set_alpha ( . 3 ) plt . gca () . spines [ \"left\" ] . set_alpha ( . 3 ) plt . yticks ( df . index , df . cars , fontsize = 10 ) plt . title ( 'Diverging Dotplot of Car Mileage' , fontdict = { 'size' : 20 }) plt . xlabel ( '$Mileage$' ) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . xlim ( - 2.5 , 2.5 ) plt . show () Diverging Lollipop chart with markers Lollipop with markers provides a flexible way fo visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately. In [16]: # Prepare Data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/mtcars.csv\" ) x = df . loc [:, [ 'mpg' ]] df [ 'mpg_z' ] = ( x - x . mean ()) / x . std () df [ 'colors' ] = 'black' # Color Fiat differently than other cars df . loc [ df . cars == 'Fiat X1-9' , 'colors' ] = 'darkorange' df . sort_values ( 'mpg_z' , inplace = True ) df . reset_index ( inplace = True ) # Draw plot import matplotlib.patches as patches plt . figure ( figsize = ( 14 , 10 ), dpi = 80 ) plt . hlines ( y = df . index , xmin = 0 , xmax = df . mpg_z , color = df . colors , alpha = 0.4 , linewidth = 1 ) plt . scatter ( df . mpg_z , df . index , color = df . colors , s = [ 600 if x == 'Fiat X1-9' else 300 for x in df . cars ], alpha = 0.6 ) plt . yticks ( df . index , df . cars ) plt . xticks ( fontsize = 12 ) # Annotate plt . annotate ( 'Mercedes Models' , xy = ( 0.0 , 11.0 ), xytext = ( 1.0 , 11 ), xycoords = 'data' , fontsize = 15 , ha = 'center' , va = 'center' , bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( arrowstyle = '-[, widthB=2.0, lengthB=1.5' , lw = 2.0 , color = 'steelblue' ), color = 'white' ) # Add Patches p1 = patches . Rectangle (( - 2.0 , - 1 ), width =. 3 , height = 3 , alpha =. 2 , facecolor = 'red' ) p2 = patches . Rectangle (( 1.5 , 27 ), width =. 8 , height = 5 , alpha =. 2 , facecolor = 'green' ) plt . gca () . add_patch ( p1 ) plt . gca () . add_patch ( p2 ) # Decorate plt . title ( 'Diverging Bars of Car Mileage' , fontdict = { 'size' : 20 }) plt . grid ( linestyle = '--' , alpha = 0.5 ) plt . show () Area chart By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs, but also the duration of hte highs and lows. The longer the duration of the highs, the larger is the area under the line. In [17]: # Import data df = pd . read_csv ( \"https://github.com/selva86/datasets/raw/master/economics.csv\" , parse_dates = [ 'date' ]) . head ( 100 ) x = np . arange ( df . shape [ 0 ]) y_returns = ( df . psavert . diff () . fillna ( 0 ) / df . psavert . shift ( 1 )) . fillna ( 0 ) * 100 # Plot plt . figure ( figsize = ( 16 , 10 ), dpi = 80 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] >= 0 , facecolor = 'green' , interpolate = True , alpha = 0.7 ) plt . fill_between ( x [ 1 :], y_returns [ 1 :], 0 , where = y_returns [ 1 :] <= 0 , facecolor = 'red' , interpolate = True , alpha = 0.7 ) plt . annotate ( 'Peak \\n 1975' , xy = ( 94.0 , 21.0 ), xytext = ( 88.0 , 28 ), bbox = dict ( boxstyle = 'square' , fc = 'firebrick' ), arrowprops = dict ( facecolor = 'steelblue' , shrink = 0.05 ), fontsize = 15 , color = 'white' ) # Decorations xtickvals = [ str ( m )[: 3 ] . upper () + \"-\" + str ( y ) for y , m in zip ( df . date . dt . year , df . date . dt . month_name ())] plt . gca () . set_xticks ( x [:: 6 ]) plt . gca () . set_xticklabels ( xtickvals [:: 6 ], rotation = 45 , fontdict = { 'horizontalalignment' : 'center' , 'verticalalignment' : 'top' , 'fontsize' : 12 }) plt . ylim ( - 35 , 35 ) plt . xlim ( 1 , 100 ) plt . title ( \"Month Economics Return %\" , fontsize = 22 ) plt . ylabel ( 'Monthly returns %' ) plt . grid ( alpha = 0.5 ) plt . show () The rest of the top visualizations will be presented in the future posts. Peace~","tags":"Data Science","url":"https://haochen23.github.io/2018/10/50-visualization-part1.html","loc":"https://haochen23.github.io/2018/10/50-visualization-part1.html"},{"title":"New Airbnb User Booking Prediction","text":"Airbnb New User Booking Predictions Introduction Predict new users' first bookings for their stay in a specific country. Details can be found in a Kaggle competion here . Data Overview There are 6 files provided. Two of these files provide background information (countries.csv and age_gender_bkts.csv), while sample_submission_NDF.csv provides an example of how the submission file containing our final predictions should be formatted. The three remaining files are the key ones: 1. train_users_2.csv – This dataset contains data on Airbnb users, including the destination countries. 2. test_users.csv – This dataset also contains data on Airbnb users, in the same format as train_users_2.csv, except without the destination country. These are the users for which we will have to make our final predictions. 3. sessions.csv – This data is supplementary data that can be used to train the model and make the final predictions. It contains information about the actions (e.g. clicked on a listing, updated a wish list, ran a search etc.) taken by the users in both the testing and training datasets above. A glimpse on the data In [1]: import pandas as pd #import data tr_datapath = \"data/train_users_2.csv\" te_datapath = \"data/test_users.csv\" df_train = pd . read_csv ( tr_datapath , header = 0 , index_col = None ) df_test = pd . read_csv ( te_datapath , header = 0 , index_col = None ) In [2]: # size of training data print ( df_train . shape ) df_train . head () (213451, 16) Out[2]: id date_account_created timestamp_first_active date_first_booking gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser country_destination 0 gxn3p5htnn 2010-06-28 20090319043255 NaN -unknown- NaN facebook 0 en direct direct untracked Web Mac Desktop Chrome NDF 1 820tgsjxq7 2011-05-25 20090523174809 NaN MALE 38.0 facebook 0 en seo google untracked Web Mac Desktop Chrome NDF 2 4ft3gnwmtx 2010-09-28 20090609231247 2010-08-02 FEMALE 56.0 basic 3 en direct direct untracked Web Windows Desktop IE US 3 bjjt8pjhuk 2011-12-05 20091031060129 2012-09-08 FEMALE 42.0 facebook 0 en direct direct untracked Web Mac Desktop Firefox other 4 87mebub9p4 2010-09-14 20091208061105 2010-02-18 -unknown- 41.0 basic 0 en direct direct untracked Web Mac Desktop Chrome US In [3]: # size of test data, short of the country_destination column which need to be predicted by our model print ( df_test . shape ) df_test . head () (62096, 15) Out[3]: id date_account_created timestamp_first_active date_first_booking gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser 0 5uwns89zht 2014-07-01 20140701000006 NaN FEMALE 35.0 facebook 0 en direct direct untracked Moweb iPhone Mobile Safari 1 jtl0dijy2j 2014-07-01 20140701000051 NaN -unknown- NaN basic 0 en direct direct untracked Moweb iPhone Mobile Safari 2 xx0ulgorjt 2014-07-01 20140701000148 NaN -unknown- NaN basic 0 en direct direct linked Web Windows Desktop Chrome 3 6c6puo6ix0 2014-07-01 20140701000215 NaN -unknown- NaN basic 0 en direct direct linked Web Windows Desktop IE 4 czqhjk3yfe 2014-07-01 20140701000305 NaN -unknown- NaN basic 0 en direct direct untracked Web Mac Desktop Safari Data cleansing From the above snapshot of the data in training and test files, a few key pieces of information about the integrity of this dataset can be identified. Firstly, is that at least two columns have missing values – the age column and date_first_booking column. Secondly, most of the columns provided contain categorical data. In fact 11 of the 16 columns provided appear to be categorical. Thirdly, the timestamp_first_active column looks to be a full timestamp, but in the format of a number. For example 20090609231247 looks like it should be 2009-06-09 23:12:47. Fourthly, erroneous values. For some columns, there are values that can be identified as obviously incorrect. This may be a gender column where someone has entered a number, or an age column where someone has entered a value well over 100. These values either need to be corrected (if the correct value can be determined) or assumed to be missing. Lastly, some columns need to be standardized. For example, when collecting data on country of birth, if users are not provided with a standardized list of countries, the data will inevitably contain multiple spellings of the same country (e.g. USA, United States, U.S. and so on). One of the main cleaning tasks often involves standardizing these values to ensure that there is only one version of each value. First, let's combine the training data and test data into one DataFrame so that we can do data cleansing at the same time. In [4]: # combine df_train and df_test into one DataFrame df_all = pd . concat (( df_train , df_test ), axis = 0 , ignore_index = True , sort = False ) Fix the format of the dates Because we will use the date information, cleaning the date timestamps is necessary. If we want to do anything with those dates (e.g. subtract one date from another, extract the month of the year from each date etc.), it will be far easier if Python recognizes the values as dates. In [5]: # fixing the date_account_created column df_all [ 'date_account_created' ] = pd . to_datetime ( df_all [ 'date_account_created' ], format = '%Y-%m- %d ' ) # fixing the timestamp_first_active column df_all [ 'timestamp_first_active' ] = pd . to_datetime ( df_all [ 'timestamp_first_active' ], format = '%Y%m %d %H%M%S' ) # use the timestamp_first_active column to fill the missing values in data_account_created column df_all [ 'date_account_created' ] . fillna ( df_all . timestamp_first_active , inplace = True ) Drop inconsistant columns There are three date fields, but we have only covered two above. The remaining date field, date_first_booking, we are going to drop (remove) from the training data altogether. The reason is that this field is only populated for users who have made a booking. For the data in training_users_2.csv, all the users that have a first booking country have a value in the date_first_booking column and for those that have not made a booking (country_destination = NDF) the value is missing. However, for the data in test_users.csv, the date_first_booking column is empty for all the records. This means that this column is not going to be useful for predicting which country a booking will be made. What is more, if we leave it in the training dataset when building the model, it will likely increase the chances that the model predicts NDF as those are the records without dates in the training dataset. In [6]: # Drop the date_first_booking column df_all . drop ( 'date_first_booking' , axis = 1 , inplace = True ) Correct the age column As noticed earlier, there are several age values that are clearly incorrect (unreasonably high or too low). In this step, we replace these incorrect values with ‘NaN'. To do this, we create a simple function that intakes a dataframe (table), a column name, a maximum acceptable value (90) and a minimum acceptable value (15). This function will then replace the values in the specified column that are outside the acceptable range with NaN. Besides, the significant portion of users who did not provide a age value should also be noticed. After we have converted the incorrect age values to NaN, we then change all the NaN values to -1. After testing with other methods of filling the NaN values, including average, median, and most frequent value, using the value -1 yields the best prediction model. In [7]: import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) #avoid comparison with NaN values df_all [ 'age' ] . fillna ( - 1 , inplace = True ) # function to clean incorrect value def remove_outliers ( df , column , min_val , max_val ): col_values = df [ column ] . values df [ column ] = np . where ( np . logical_or ( col_values < min_val , col_values > max_val ), np . NaN , col_values ) return df # Fixing age column df_all = remove_outliers ( df = df_all , column = 'age' , min_val = 15 , max_val = 90 ) df_all [ 'age' ] . fillna ( - 1 , inplace = True ) Fill the missing values in column first_affiliate_tracked And then view the DataFrame. In [8]: # Fill missing values in first_affliate_tracked df_all [ 'first_affiliate_tracked' ] . fillna ( - 1 , inplace = True ) df_all . tail () Out[8]: id date_account_created timestamp_first_active gender age signup_method signup_flow language affiliate_channel affiliate_provider first_affiliate_tracked signup_app first_device_type first_browser country_destination 275542 cv0na2lf5a 2014-09-30 2014-09-30 23:52:32 -unknown- 31.0 basic 0 en direct direct untracked Web Windows Desktop IE NaN 275543 zp8xfonng8 2014-09-30 2014-09-30 23:53:06 -unknown- -1.0 basic 23 ko direct direct untracked Android Android Phone -unknown- NaN 275544 fa6260ziny 2014-09-30 2014-09-30 23:54:08 -unknown- -1.0 basic 0 de direct direct linked Web Windows Desktop Firefox NaN 275545 87k0fy4ugm 2014-09-30 2014-09-30 23:54:30 -unknown- -1.0 basic 0 en sem-brand google omg Web Mac Desktop Safari NaN 275546 9uqfg8txu3 2014-09-30 2014-09-30 23:59:01 FEMALE 49.0 basic 0 en other other tracked-other Web Windows Desktop Chrome NaN Data transformation and feature extraction We use data transformation is undertaken with the intention to enhance the ability of the classification algorithm to extract information from the data. We then use feature extraction to create new features which will help improve the prediction accuracy of our model. We will first focus on data transformation. Transforming categorical data - one hot encoding The first step we are going to undertake is some One Hot Encoding – replacing the categorical fields in the dataset with multiple columns representing one value from each column. In [9]: # one hot encoding function def convert_to_onehot ( df , column_to_convert ): categories = list ( df [ column_to_convert ] . drop_duplicates ()) for category in categories : cat_name = str ( category ) . replace ( \" \" , \"_\" ) . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"-\" , \"\" ) . lower () col_name = column_to_convert [: 5 ] + '_' + cat_name [: 10 ] df [ col_name ] = 0 df . loc [( df [ column_to_convert ] == category ), col_name ] = 1 return df #One hot encoding, and drop the original column from df_all columns_to_convert = [ 'gender' , 'signup_method' , 'signup_flow' , 'language' , 'affiliate_channel' , 'affiliate_provider' , 'first_affiliate_tracked' , 'signup_app' , 'first_device_type' , 'first_browser' ] for column in columns_to_convert : df_all = convert_to_onehot ( df_all , column ) df_all . drop ( column , axis = 1 , inplace = True ) df_all . head () Out[9]: id date_account_created timestamp_first_active age country_destination gende_unknown gende_male gende_female gende_other signu_facebook ... first_theworld_b first_slimbrowse first_epic first_stainless first_googlebot first_outlook_20 first_icedragon first_ibrowse first_nintendo_b first_uc_browser 0 gxn3p5htnn 2010-06-28 2009-03-19 04:32:55 -1.0 NDF 1 0 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 1 820tgsjxq7 2011-05-25 2009-05-23 17:48:09 38.0 NDF 0 1 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 2 4ft3gnwmtx 2010-09-28 2009-06-09 23:12:47 56.0 US 0 0 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 bjjt8pjhuk 2011-12-05 2009-10-31 06:01:29 42.0 other 0 0 1 0 1 ... 0 0 0 0 0 0 0 0 0 0 4 87mebub9p4 2010-09-14 2009-12-08 06:11:05 41.0 US 1 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows × 157 columns Creating new featrues Two fields that can be used to create some new features are the two date fields – date_account_created and timestamp_first_active. We want to extract all the information we can out of these two date fields that could potentially differentiate which country someone will make their first booking in. In [10]: # Add new datetime related fields df_all [ 'day_account_created' ] = df_all [ 'date_account_created' ] . dt . weekday df_all [ 'month_account_created' ] = df_all [ 'date_account_created' ] . dt . month df_all [ 'quarter_account_created' ] = df_all [ 'date_account_created' ] . dt . quarter df_all [ 'year_account_created' ] = df_all [ 'date_account_created' ] . dt . year df_all [ 'hour_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . hour df_all [ 'day_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . weekday df_all [ 'month_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . month df_all [ 'quarter_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . quarter df_all [ 'year_first_active' ] = df_all [ 'timestamp_first_active' ] . dt . year df_all [ 'created_less_active' ] = ( df_all [ 'date_account_created' ] - df_all [ 'timestamp_first_active' ]) . dt . days # Drop unnecessary columns columns_to_drop = [ 'date_account_created' , 'timestamp_first_active' , 'date_first_booking' , 'country_destination' ] for column in columns_to_drop : if column in df_all . columns : df_all . drop ( column , axis = 1 , inplace = True ) print ( df_all . shape ) (275547, 164) Adding new data We will see what new data we can add from the sessios.csv file. The dataset contains records of user actions, with each row representing one action a user took. Every time a user reviewed search results, updated a wish list or updated their account information, a new row was created in this dataset. Although this data is likely to be very useful for our goal of predicting which country a user will make their first booking in, it also complicates the process of combining this data with the data from training.csv, as it will have to be aggregated so that there is one row per user. Aside from details of the actions taken, there are a couple of interesting fields in this data. The first is device_type – this field contains the type of device used for the specified action. The second interesting field is the secs_elapsed field. This shows us how long (in seconds) was spent on a particular action. Import sessions data In [11]: # read sessions.csv session_path = 'data/sessions.csv' sessions = pd . read_csv ( session_path , header = 0 , index_col = False ) sessions . head () Out[11]: user_id action action_type action_detail device_type secs_elapsed 0 d1mm9tcy42 lookup NaN NaN Windows Desktop 319.0 1 d1mm9tcy42 search_results click view_search_results Windows Desktop 67753.0 2 d1mm9tcy42 lookup NaN NaN Windows Desktop 301.0 3 d1mm9tcy42 search_results click view_search_results Windows Desktop 22141.0 4 d1mm9tcy42 lookup NaN NaN Windows Desktop 435.0 Extract the primary and secondary devides for each user How do we determine what a user's primary and secondary devices are? We look at how much time they spent on each device. One thing to note as we make these transformations is that by aggregating the data this way, we are also implicitly removing the missing values. In [12]: # Determine primary device sessions_device = sessions . loc [:, [ 'user_id' , 'device_type' , 'secs_elapsed' ]] aggregated_lvl1 = sessions_device . groupby ([ 'user_id' , 'device_type' ], as_index = False , sort = False ) . aggregate ( np . sum ) index = aggregated_lvl1 . groupby ([ 'user_id' ], sort = False )[ 'secs_elapsed' ] . transform ( max ) == aggregated_lvl1 [ 'secs_elapsed' ] df_primary = pd . DataFrame ( aggregated_lvl1 . loc [ index , [ 'user_id' , 'device_type' , 'secs_elapsed' ]]) df_primary . rename ( columns = { 'device_type' : 'primary_device' , 'secs_elapsed' : 'primary_secs' }, inplace = True ) df_primary = convert_to_onehot ( df_primary , column_to_convert = 'primary_device' ) df_primary . drop ( 'primary_device' , axis = 1 , inplace = True ) # Determine secondary device remaining = aggregated_lvl1 . drop ( aggregated_lvl1 . index [ index ]) index = remaining . groupby ( [ 'user_id' ], sort = False )[ 'secs_elapsed' ] . transform ( max ) == remaining [ 'secs_elapsed' ] df_secondary = pd . DataFrame ( remaining . loc [ index , [ 'user_id' , 'device_type' , 'secs_elapsed' ]]) df_secondary . rename ( columns = { 'device_type' : 'secondary_device' , 'secs_elapsed' : 'secondary secs' }, inplace = True ) df_secondary = convert_to_onehot ( df_secondary , 'secondary_device' ) df_secondary . drop ( 'secondary_device' , axis = 1 , inplace = True ) Determine action counts Determine action counts for the three columns action, action_type, action_detail , to generate 3 sepparate tables. Then we join the three tables together on the basis of the user_id. In [13]: # function to count occurrences of value in a column def convert_to_counts ( df , id_col , column_to_convert ): id_list = df [ id_col ] . drop_duplicates () df_counts = df . loc [:,[ id_col , column_to_convert ]] df_counts [ 'count' ] = 1 df_counts = df_counts . groupby ( by = [ id_col , column_to_convert ], as_index = False , sort = False ) . sum () new_df = df_counts . pivot ( index = id_col , columns = column_to_convert , values = 'count' ) new_df = new_df . fillna ( 0 ) #rename columns categories = list ( df [ column_to_convert ] . drop_duplicates ()) for category in categories : cat_name = str ( category ) . replace ( \" \" , \"_\" ) . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . replace ( \"/\" , \"_\" ) . replace ( \"-\" , \"\" ) . lower () col_name = column_to_convert + '_' + cat_name new_df . rename ( columns = { category : col_name }, inplace = True ) return new_df # Aggregate and combine actions taken columns session_actions = sessions . loc [:, [ 'user_id' , 'action' , 'action_type' , 'action_detail' ]] columns_to_convert = [ 'action' , 'action_type' , 'action_detail' ] session_actions = session_actions . fillna ( 'not provided' ) # flag indicating the first loop first = True for column in columns_to_convert : print ( \"Converting \" + column + \" column...\" ) current_data = convert_to_counts ( df = session_actions , id_col = 'user_id' , column_to_convert = column ) if first : first = False actions_data = current_data else : actions_data = pd . concat ([ actions_data , current_data ], axis = 1 , join = 'inner' ) Converting action column... Converting action_type column... Converting action_detail column... Combine data sets The last steps are to combine the various datasets into one large dataset. First we combine the two device dataframes (df_primary and df_secondary) to create a device dataframe. Then we combine the device dataframe with the actions dataframe to create a sessions dataframe with all the features we extracted from sessions.csv. Finally, we combine the sessions dataframe with the user data dataframe. The first two joins need outer join because not all users have a secondary deivce. The second merge could use an outer join or an inner join, as both the device and actions datasets should contain all users. In this case we use an outer join just to ensure that if a user is missing from one of the datasets (for whatever reason), we will still capture them. For the third step we use an inner join for a key reason – we want our final training dataset to only include users that also have sessions data. Using an inner join here is an easy way to join the datasets and filter for the users with sessions data in one step. In [14]: # Combine device datasets df_primary . set_index ( 'user_id' , inplace = True ) df_secondary . set_index ( 'user_id' , inplace = True ) device_data = pd . concat ([ df_primary , df_secondary ], axis = 1 , join = 'outer' , sort = False ) #Combine device and actions datasets combined_results = pd . concat ([ device_data , actions_data ], axis = 1 , join = 'outer' , sort = False ) df_sessions = combined_results . fillna ( 0 ) #Combine user and sessions datasets df_all . set_index ( 'id' , inplace = True ) df_all = pd . concat ([ df_all , df_sessions ], axis = 1 , join = 'inner' , sort = False ) df_all . head () Out[14]: age gende_unknown gende_male gende_female gende_other signu_facebook signu_basic signu_google signu_weibo signu_0 ... action_detail_view_resolutions action_detail_view_search_results action_detail_view_security_checks action_detail_view_user_real_names action_detail_wishlist action_detail_wishlist_content_update action_detail_wishlist_note action_detail_your_listings action_detail_your_reservations action_detail_your_trips d1mm9tcy42 62.0 0 1 0 0 0 1 0 0 1 ... 0.0 23.0 0.0 0.0 0.0 25.0 0.0 0.0 0.0 0.0 yo8nz8bqcq -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 4grx6yxeby -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 ncf87guaf0 -1.0 1 0 0 0 0 1 0 0 1 ... 0.0 32.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 0.0 4rvqpxoh3h -1.0 1 0 0 0 0 1 0 0 0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows × 720 columns Create a Model So far, the df_all dataset is ready to be used to train and test a model to predict the first booking destination country for each new user. The traing algorithm we will use is the popular XGBoost. From my perspective, this method is superior to random forest. It builds a first tree, typically a shallower tree than if you use one single decision tree, and makes predictions using that tree. Then the algorithm finds the records that are misclassified by that tree, and assigns a higher weight of importance to those records than the records that were correctly classified. The algorithm then builds a new tree with these new weightings. This whole process is repeated as many times as specified by the user. Once the specified number of trees have been built, all the trees built during this process are used to classify the records, with a majority rules approach used to determine the final prediction. Cross validation To avoid overfitting in our model, I will use 10-fold cross validation. K-fold cross validation involves splitting the training data into k subsets (where k is greater than or equal to 2), training the model using k – 1 of those subsets, then running the model on the subset that was not used in the training process. Because all of the data used in the cross validation process is training data, the correct classification for each record is known and so the predicted category can be compared to the actual category. Once all folds have been completed, the average score across all folds is taken as an estimate of how the model will perform on other data. In [15]: # Import libraries import xgboost as xgb from sklearn import decomposition from sklearn.model_selection import GridSearchCV , cross_validate from sklearn.preprocessing import LabelEncoder Prepare training data We previously combined the training and test data to simplify the cleaning and transforming process. To feed these into the model, we also need to split the training data into the three main components – the user IDs (we don't want to use these for training as they are randomly generated), the features to use for training (X), and the categories we are trying to predict (y). In [16]: # Prepare training data for model training df_train . set_index ( 'id' , inplace = True ) df_train = pd . concat ([ df_train [ 'country_destination' ], df_all ], axis = 1 , join = 'inner' , sort = False ) index_train = df_train . index . values labels = df_train [ 'country_destination' ] le = LabelEncoder () y = le . fit_transform ( labels ) # training labels x = df_train . drop ( 'country_destination' , axis = 1 , inplace = False ) # training data Now that we have our training data ready, we can use GridSearchCV to run the algorithm with a range of parameters, then select the model that has the highest cross validated score based on the chosen measure of a performance (in this case accuracy, but there are a range of metrics we could use based on our needs). In [17]: # Grid Search - used to find the best combination of parameters XGB_model = xgb . XGBClassifier ( objective = 'multi:softprob' , subsample = 0.8 , colsample_bytree = 0.8 , seed = 0 ) param_grid = { 'max_depth' : [ 3 , 4 , 5 ], 'learning_rate' : [ 0.1 , 0.3 ], 'n_estimators' : [ 25 , 50 ]} model = GridSearchCV ( estimator = XGB_model , param_grid = param_grid , scoring = 'accuracy' , verbose = 10 , n_jobs = 1 , iid = True , refit = True , cv = 3 ) # Model training model . fit ( x , y ) print ( \"Best score: %0.3f \" % model . best_score_ ) print ( \"Best parameters set:\" ) best_parameters = model . best_estimator_ . get_params () for param_name in sorted ( param_grid . keys ()): print ( \" \\t %s : %r \" % ( param_name , best_parameters [ param_name ])) Fitting 3 folds for each of 12 candidates, totalling 36 fits [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 2.7min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.7030189752549673, total= 2.7min C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 5.1min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.6923795976427556, total= 2.4min [CV] learning_rate=0.1, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 3 out of 3 | elapsed: 7.5min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=25, score=0.6961665108337737, total= 2.3min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 4 out of 4 | elapsed: 12.1min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7052943805615375, total= 4.5min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 16.7min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.6867709815078236, total= 4.6min [CV] learning_rate=0.1, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 6 out of 6 | elapsed: 21.4min remaining: 0.0s [CV] learning_rate=0.1, max_depth=3, n_estimators=50, score=0.7000691085003455, total= 4.6min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 7 out of 7 | elapsed: 24.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.7052943805615375, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 8 out of 8 | elapsed: 27.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.6942491363543996, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 30.3min remaining: 0.0s [CV] learning_rate=0.1, max_depth=4, n_estimators=25, score=0.7007195414447742, total= 2.9min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.7052943805615375, total= 5.8min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.6862426336110546, total= 5.6min [CV] learning_rate=0.1, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=4, n_estimators=50, score=0.7025082320419529, total= 5.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.7049286904229816, total= 3.6min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.6935175777281041, total= 3.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=25, score=0.703849749989837, total= 3.7min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.704847425947747, total= 7.3min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.6824629140418614, total= 7.3min [CV] learning_rate=0.1, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.1, max_depth=5, n_estimators=50, score=0.7048660514655067, total= 7.2min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.7021250660273861, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.6848201585043691, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=25, score=0.7008008455628277, total= 2.3min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.701353053512657, total= 4.5min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.6821784190205242, total= 4.6min [CV] learning_rate=0.3, max_depth=3, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=3, n_estimators=50, score=0.7010447579169885, total= 4.5min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.7028970785421154, total= 3.0min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.6659215606584028, total= 2.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=25, score=0.7034838814585959, total= 2.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.7018000081264475, total= 5.8min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.6597845966267019, total= 5.9min [CV] learning_rate=0.3, max_depth=4, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=4, n_estimators=50, score=0.7045001829342656, total= 5.8min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.7033034009182886, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.6759195285511075, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=25 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=25, score=0.7001097605593724, total= 3.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.700743569948397, total= 7.5min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.6653932127616338, total= 7.6min [CV] learning_rate=0.3, max_depth=5, n_estimators=50 ................. C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: C:\\Users\\hche958\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty. if diff: [Parallel(n_jobs=1)]: Done 36 out of 36 | elapsed: 161.6min finished [CV] learning_rate=0.3, max_depth=5, n_estimators=50, score=0.6992560673198097, total= 7.5min Best score: 0.701 Best parameters set: learning_rate: 0.1 max_depth: 5 n_estimators: 25 Make Predictions on test data Now we can use our trained model (with best training parameters) to make predictions on our test data. In [18]: # Prepare test data for prediction df_test . set_index ( 'id' , inplace = True ) df_test = pd . merge ( df_test . loc [:, [ 'date_first_booking' ]], df_all , how = 'left' , left_index = True , right_index = True , sort = False ) x_test = df_test . drop ( 'date_first_booking' , axis = 1 , inplace = False ) x_test = x_test . fillna ( - 1 ) id_test = df_test . index . values # Make predictions y_pred = model . predict_proba ( x_test )","tags":"Data Science","url":"https://haochen23.github.io/2018/10/user-booking-prediction.html","loc":"https://haochen23.github.io/2018/10/user-booking-prediction.html"},{"title":"Indian diabetes database modeling using Naive Bayes","text":"Indian diabetes database modeling using Naive Bayes Coding from scratch. To better understand the process of Naive Bayes model training. And in the end, we make another Naive Bayes Classifier through normal workflow using sklearn. Pros and cons of Naive Bayes Classifiers Pros: Computationally fast Simple to implement Works well with small datasets Works well with high dimensions Perform well even if the Naive Assumption is not perfectly met. In many cases, the approximation is enough to build a good classifier Cons: Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance. If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as \"Zero Frequency\". To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier. In [1]: #import libraries and basic settings import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np import pandas as pd import random import math from IPython.display import display pi = math . pi Exploratory analysis Check out the data to get a brief understanding of the dataset, including data integrity, size of the dataset, any missing values. In [2]: full_catalog = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) print ( full_catalog . columns ) print ( 'Size of the catalogue: {} ' . format ( len ( full_catalog ))) print ( 'Is there any missing value?: {} ' . format ( full_catalog . isnull () . any () . any ())) full_catalog . head () Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'], dtype='object') Size of the catalogue: 768 Is there any missing value?: False Out[2]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 No missing values are found in our dataset, which is good. Because the dataset is pretty clean we don't need to do too much data cleansing. The Outcome column is our target column. Value 1 stands positive diagnosis of diabetes and value 0 represents negative. Let's see how many people are positive and/or negative in our database. In [3]: # diabetes positive positive = full_catalog [ full_catalog [ 'Outcome' ] == 1 ] print ( 'The number of patients with diabetes: ' , len ( positive )) # diabetes negative negative = full_catalog [ full_catalog [ 'Outcome' ] == 0 ] print ( 'The number of healthy patients: ' , len ( negative )) The number of patients with diabetes: 268 The number of healthy patients: 500 Deeper into the data : let's see which features (attributes) can better explain our data according to the Outcome . Scatter matrix of all data First let's look at the scatter matrix of attributes one against another. Blue represents negative patients (healthy), and red represent positive patients (with diabetes). In [4]: # According to the color map, blue = 0, red = 1 df = pd . DataFrame ( full_catalog , columns = full_catalog . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = full_catalog [ 'Outcome' ] . values , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'green' }, s = 10 , alpha = 0.2 , cmap = plt . get_cmap ( 'bwr' )) plt . show () Scatter matrix of diabetes patients (positive) In [5]: # Scatter of patients with diabetes df = pd . DataFrame ( positive , columns = positive . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = 'red' , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'red' }, s = 10 , alpha = 0.2 ) plt . show () Scatter matrix of heathy patients (negative outcomes) In [6]: # Scatter of patients with diabetes df = pd . DataFrame ( negative , columns = negative . columns . drop ( 'Outcome' )) pd . plotting . scatter_matrix ( df , c = 'blue' , figsize = ( 15 , 15 ), marker = 'o' , hist_kwds = { 'bins' : 10 , 'color' : 'blue' }, s = 10 , alpha = 0.2 ) plt . show () Training set and test set preparation Of course we can use train_test_split from sklearn. But here, we use a scratch function to realize it. In [7]: ''' Function create_training_test divide the whole dataset into training set and test set. Parameters: dataset: the original dataset to be split fraction_training: fraction of training set, number in [0, 1] msg: debug flag. If True, display message of current process Output: training_set test_set ''' def create_training_test ( dataset , fraction_training , msg ): # define size of training and test sets size_dataset = len ( dataset ) size_training = round ( size_dataset * fraction_training ) size_test = size_dataset - size_training # initializing both the training and test set using the whole dataset training_set = dataset . copy () test_set = dataset . copy () #index of the dataset dataframe total_idx_list = list ( dataset . index . values ) #index of the test set. Randomly selected from total_idx_list test_idx_list = random . sample ( list ( dataset . index . values ), size_test ) test_idx_list . sort () #index of the training set training_idx_list = list ( set ( total_idx_list ) - set ( test_idx_list )) #drop the corresponding rows from the training and test dataframe training_set . drop ( training_set . index [ test_idx_list ], inplace = True ) test_set . drop ( test_set . index [ training_idx_list ], inplace = True ) if msg == True : training_positive = training_set [ training_set [ 'Outcome' ] == 1 ] training_negative = training_set [ training_set [ 'Outcome' ] == 0 ] print ( \"Size of the dataset : {} \" . format ( size_dataset )) print ( \"Size of the training set : {} samples ( {} of the whole dataset)\" . format ( len ( training_set ), fraction_training )) print ( \" \\t Positive cases in the training set: {} \" . format ( len ( training_positive ))) print ( \" \\t Negative cases in the training set: {} \" . format ( len ( training_negative ))) print ( \"Size of the test set : {} \" . format ( len ( test_set ))) return training_set , test_set In [8]: ''' Function get_parameters create a dictionary that contain the mean and standard deviation of each column in dataset Input: dataset: input dataset frame msg: debug flag Output: dict_parameters: a dictionary that contain the mean and standard deviation of each attribute in dataset ''' def get_parameters ( dataset , msg ): features = dataset . columns . values nbins = 10 dict_parameters = {} # exclude the 'Outcome' column from the loop for i in range ( 0 , len ( features ) - 1 ): #we single out the column 'feature[i]' from dataset aux_df = pd . DataFrame ( dataset [ features [ i ]]) #Here we make partition into nbins. #aux_df has an extra column indicating to which bin each instance belongs to aux_df [ 'bin' ] = pd . cut ( aux_df [ features [ i ]], nbins ) # 'counts' is a searies whose index is the bin interval and the values are the #number of counts in each bin counts = pd . value_counts ( aux_df [ 'bin' ]) points_X = np . zeros ( nbins ) points_Y = np . zeros ( nbins ) for j in range ( 0 , nbins ): points_X [ j ] = counts . index [ j ] . mid # the mid point of each bin points_Y [ j ] = counts . iloc [ j ] # the number of counts total_Y = np . sum ( points_Y ) # we compute the mean and std and store them in dict_parameters # whose keys are column names and values are (mu, sigma) tuple mu = np . sum ( points_X * points_Y ) / total_Y sigma2 = np . sum (( points_X - mu ) ** 2 * points_Y ) / ( total_Y - 1 ) sigma = math . sqrt ( sigma2 ) dict_parameters [ features [ i ]] = ( mu , sigma ) if msg == True : print ( ' \\t\\t feature: {} , mean: {} , standard deviation: {} ' . format ( features [ i ], mu , sigma )) return dict_parameters In [9]: ''' Function likelihood calculates the probability density of each column Input: instance: series, can be considered as each row in our dataset, the index of the series is the columns of our dataset dictionary Output: dict_likelihood: dictionary contains probability density ''' def likelihood ( instance , dictionary ): instance = instance [ instance . index != 'Outcome' ] dict_likelihood = {} for feature in instance . index : mu = dictionary [ feature ][ 0 ] sigma = dictionary [ feature ][ 1 ] measurement = instance [ feature ] if feature in [ 'Pregnancies' , 'Insulin' , 'DiabetesPedigreeFunction' , 'Age' ]: # We use exponential distribution for # columns ['Pregnancies', 'Insulin', 'DiabetesPedigreeFunction', 'Age'] dict_likelihood [ feature ] = 1. / mu * math . exp ( - measurement / mu ) elif feature in [ 'Glucose' , 'BloodPressure' , 'SkinThickness' , 'BMI' ]: # We use Gaussian distribution for columns['Glucose', 'BloodPressure', 'SkinThickness', 'BMI'] dict_likelihood [ feature ] = 1. / ( math . sqrt ( 2 * pi ) * sigma ) * math . exp ( - ( measurement - mu ) ** 2 / ( 2. * sigma ** 2 )) return dict_likelihood In [10]: ''' Function bayes classify the input instances according to Bayes Theory Input: lkh_positive: for each feature, P(features|outcome == 1) lkh_negative: for each feature, P(features|outcome == 0) prob_positive: the probability of patients with diabetes Output: predictions, 1-positive, 0-negative ''' def bayes ( lkh_positive , lkh_negative , prob_positive ): logPositive = 0 logNegative = 0 for feature in lkh_positive : logPositive += math . log ( lkh_positive [ feature ]) logNegative += math . log ( lkh_negative [ feature ]) logPositive = logPositive + math . log ( prob_positive ) logNegative = logNegative + math . log ( 1. - prob_positive ) if logPositive > logNegative : return 1 else : return 0 In [11]: # funciton to run the test def pima_diabetes_NBClassifier ( training_fraction , msg ): dataset = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) training , test = create_training_test ( dataset , training_fraction , msg ) #split the training set into training_positive and training_negative according to Outcome training_positive = training [ training [ 'Outcome' ] == 1 ] training_negative = training [ training [ 'Outcome' ] == 0 ] prob_positive = len ( training_positive ) / ( len ( training )) if msg == True : print ( 'Getting the parameters for the training set...' ) print ( ' \\t Positive cases subsample' ) param_positive = get_parameters ( training_positive , msg ) if msg == True : print ( ' \\t Negative cases subsample' ) param_negative = get_parameters ( training_negative , msg ) if msg == True : print ( ' \\t Probability of finding a positive case: {} ' . format ( prob_positive )) print ( 'Analyzing the test set...' ) # Here we compute the accuracy of the classifier by looping over the instances of the test set error_count = 0 for idx in test . index . values : instance = test . loc [ idx ] likelihood_positive = likelihood ( instance , param_positive ) likelihood_negative = likelihood ( instance , param_negative ) prediction = bayes ( likelihood_positive , likelihood_negative , prob_positive ) answer = int ( instance [ 'Outcome' ]) if prediction != answer : error_count += 1 error_rate = float ( error_count ) / len ( test ) if msg == True : print ( 'Results for this implementation:' ) print ( ' \\t Error rate: : {} ' . format ( error_rate )) print ( ' \\t Successful classification rate : {} ' . format ( 1. - error_rate )) return error_rate Single implementation In [12]: training_fraction = 0.75 msg = True pima_diabetes_NBClassifier ( training_fraction , msg ) Size of the dataset : 768 Size of the training set : 576 samples (0.75 of the whole dataset) Positive cases in the training set: 203 Negative cases in the training set: 373 Size of the test set : 192 Getting the parameters for the training set... Positive cases subsample feature: Pregnancies, mean: 5.06026354679803, standard deviation: 3.659147466140387 feature: Glucose, mean: 139.54409359605913, standard deviation: 31.20971649275315 feature: BloodPressure, mean: 70.25017241379311, standard deviation: 20.63956588198645 feature: SkinThickness, mean: 23.708549261083746, standard deviation: 16.27891796774379 feature: Insulin, mean: 109.93596059113301, standard deviation: 116.28083680196309 feature: BMI, mean: 35.053634975369455, standard deviation: 7.323798557556501 feature: DiabetesPedigreeFunction, mean: 0.5482635467980296, standard deviation: 0.35793174891647633 feature: Age, mean: 36.80145320197044, standard deviation: 10.80830792627758 Negative cases subsample feature: Pregnancies, mean: 3.3208900804289545, standard deviation: 2.885346134494473 feature: Glucose, mean: 110.5148873994638, standard deviation: 25.689628327169185 feature: BloodPressure, mean: 68.60210455764074, standard deviation: 18.27917311567462 feature: SkinThickness, mean: 20.48474798927614, standard deviation: 14.190959528879588 feature: Insulin, mean: 86.43265415549598, standard deviation: 88.69429208023446 feature: BMI, mean: 30.5620226541555, standard deviation: 7.792696402992349 feature: DiabetesPedigreeFunction, mean: 0.43494249329758705, standard deviation: 0.3122588634553088 feature: Age, mean: 31.4157908847185, standard deviation: 11.052104394041816 Probability of finding a positive case: 0.3524305555555556 Analyzing the test set... Results for this implementation: Error rate: : 0.19791666666666666 Successful classification rate : 0.8020833333333334 Out[12]: 0.19791666666666666 Multiple implementations Let's run the NBClassifier multiple times to get an mean and std values of the error rate and successful rate. In [13]: training_fraction = 0.75 nrealizations = 500 msg = False error_rate = np . zeros ( nrealizations ) sucess_rate = np . zeros ( nrealizations ) for i in range ( 0 , nrealizations ): aux = pima_diabetes_NBClassifier ( training_fraction , msg ) error_rate [ i ] = aux success_rate = 1. - aux print ( 'Results after {} realizations and training the classifier wiht {} of the wholesamples...' . format ( nrealizations , training_fraction )) print ( 'error rate mean: {} , std: {} ' . format ( np . mean ( error_rate ), np . std ( error_rate ))) print ( 'success rate mean: {} , std: {} ' . format ( np . mean ( success_rate ), np . std ( success_rate ))) Results after 500 realizations and training the classifier wiht 0.75 of the wholesamples... error rate mean: 0.232875, std: 0.02677764991036783 success rate mean: 0.734375, std: 0.0 Naive Bayes using sklearn: a comparison Now let's look at the normal workflow using the library of sklearn. In [14]: from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB #importing dataset data = pd . read_csv ( 'data\\indian-diabetes-database.csv' ) data . dropna ( axis = 0 , how = 'any' , inplace = True ) X = data [[ 'Pregnancies' , 'Glucose' , 'BloodPressure' , 'SkinThickness' , 'Insulin' , 'BMI' , 'DiabetesPedigreeFunction' , 'Age' ]] y = data [ 'Outcome' ] #split dataset into training and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) In [15]: #initailizing the classifier, we use the GaussianNB gnb = GaussianNB () # train the classifier using training set gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) # print results print ( \"Number of misclassified samples out of a total {} samples: {} , performance {:05.2f} %\" . format ( X_test . shape [ 0 ], ( y_test != y_pred ) . sum (), 100 * ( 1 - ( y_test != y_pred ) . sum () / X_test . shape [ 0 ]))) Number of misclassified samples out of a total 192 samples: 51, performance 73.44%","tags":"Data Science","url":"https://haochen23.github.io/2018/09/Naive-Bayes.html","loc":"https://haochen23.github.io/2018/09/Naive-Bayes.html"},{"title":"Churn Rate Prediction Using Neural Networks","text":"Churn Rate Prediction for a bank The basic aim of this notebook is to predict customer churn for a certain bank i.e. which customer is going to leave this bank service. Neural network will be used as the modelling method for this notebook. The dataset used in this notebook is introduced by Pushkar Mandot on his blog post. The dataset can be down load here . The dataset contains 10000 rows with 14 columns. I am not explaining data in detail as dataset is self explanatory. Importing data In [1]: # Importing the libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd # Importing the dataset dataset = pd . read_csv ( 'data\\Churn_Modelling.csv' ) print ( dataset . shape ) dataset . head () (10000, 14) Out[1]: RowNumber CustomerId Surname CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 0 1 15634602 Hargrave 619 France Female 42 2 0.00 1 1 1 101348.88 1 1 2 15647311 Hill 608 Spain Female 41 1 83807.86 1 0 1 112542.58 0 2 3 15619304 Onio 502 France Female 42 8 159660.80 3 1 0 113931.57 1 3 4 15701354 Boni 699 France Female 39 1 0.00 2 0 0 93826.63 0 4 5 15737888 Mitchell 850 Spain Female 43 2 125510.82 1 1 1 79084.10 0 Creating training and test set Create matrix of features and matrix of target variable. In this case we are excluding column 1 & 2 as those are ‘row_number' and ‘customerid' which are not useful in our analysis. Column 14, ‘Exited' is our Target Variable In [2]: X = dataset . iloc [:, 3 : 13 ] . values y = dataset . iloc [:, 13 ] . values Let us take a glimpse on the predictors in X. As can be seen below, the dataset is pretty clean. Only two columns of string variables need to be transferred to categorical variables or one hot values in order to be fed into a classifier. In [3]: print ( X ) [[619 'France' 'Female' ... 1 1 101348.88] [608 'Spain' 'Female' ... 0 1 112542.58] [502 'France' 'Female' ... 1 0 113931.57] ... [709 'France' 'Female' ... 0 1 42085.58] [772 'Germany' 'Male' ... 1 0 92888.52] [792 'France' 'Female' ... 1 0 38190.78]] The target y only contains 0s and 1s as 0 stands for customers still with us, and 1 represents customers left us. In [4]: print ( y ) [1 0 1 ... 1 1 0] Encoding categorical variables : we need to use the LabelEncoder and OneHotEncoder from the sklearn to transform string variables in X. Use LabelEncoder first to encode different labels in a certain column to numbers between 0 to n_class-1. Then, use OneHotEncoder to tranform the numbers into one hot manner. In [5]: from sklearn.preprocessing import LabelEncoder , OneHotEncoder labelencoder_X_1 = LabelEncoder () X [:, 1 ] = labelencoder_X_1 . fit_transform ( X [:, 1 ]) labelencoder_X_2 = LabelEncoder () X [:, 2 ] = labelencoder_X_2 . fit_transform ( X [:, 2 ]) X Out[5]: array([[619, 0, 0, ..., 1, 1, 101348.88], [608, 2, 0, ..., 0, 1, 112542.58], [502, 0, 0, ..., 1, 0, 113931.57], ..., [709, 0, 0, ..., 0, 1, 42085.58], [772, 1, 1, ..., 1, 0, 92888.52], [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object) Now you can see that Country names are replaced by 0,1 and 2 while male and female are replaced by 0 and 1. Label encoding has introduced new problem in our data. LabelEncoder has replaced France with 0, Germany 1 and Spain 2 but Germany is not higher than France and France is not smaller than Spain so we need to create a dummy variable for Country. We don't need to do same for Gender Variable as it is binary. Here, we use the OneHotEncoder to do the job. In [6]: onehotencoder = OneHotEncoder ( categorical_features = [ 1 ]) X = onehotencoder . fit_transform ( X ) . toarray () X = X [:, 1 :] X Out[6]: array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, ..., 1.0000000e+00, 1.0000000e+00, 1.0134888e+05], [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, ..., 0.0000000e+00, 1.0000000e+00, 1.1254258e+05], [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 1.1393157e+05], ..., [0.0000000e+00, 0.0000000e+00, 7.0900000e+02, ..., 0.0000000e+00, 1.0000000e+00, 4.2085580e+04], [1.0000000e+00, 0.0000000e+00, 7.7200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 9.2888520e+04], [0.0000000e+00, 0.0000000e+00, 7.9200000e+02, ..., 1.0000000e+00, 0.0000000e+00, 3.8190780e+04]]) Assigning training set and test set In [7]: # Splitting the dataset into the Training set and Test set from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) Preprocessing We are going to fitting and transforming StandardScaler method on both the training. In order to make our model work on test data, we have to standardize our scaling so we will use the same fitted method to transform/scale test data. Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False. In [8]: # Feature Scaling from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train = sc . fit_transform ( X_train ) X_test = sc . transform ( X_test ) Now, the preprocessing on our data is done. We will start building our neural network model. The library we use to build our NN model is Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. We need Sequential module for initializing NN and Dense module to add Hidden Layers. In [9]: # Importing the Keras libraries and packages import keras from keras.models import Sequential from keras.layers import Dense D:\\ANOCONDA\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. In [10]: #Initializing Neural Network classifier = Sequential () Adding layers to the neural network. Which activation function should be used is critical task. Here we are using rectifier(relu) function in our hidden layer and Sigmoid function in our output layer as we want binary result from output layer but if the number of categories in output layer is more than 2 then use SoftMax function. In [11]: # Adding the input layer and the first hidden layer classifier . add ( Dense ( activation = \"relu\" , input_dim = 11 , units = 6 , kernel_initializer = \"uniform\" )) # Adding the second hidden layer classifier . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the output layer classifier . add ( Dense ( activation = \"sigmoid\" , units = 1 , kernel_initializer = \"uniform\" )) Till now we have added multiple layers to out classifier now let's compile them which can be done using compile method. Arguments added in final compilation will control whole neural network so be careful on this step. In [12]: # Compiling Neural Network classifier . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) We will now train our model on training data but still one thing is remaining. We use fit method to the fit our model In previous some steps I said that we will be optimizing our weights to improve model efficiency so when are we updating out weights? Batch size is used to specify the number of observation after which you want to update weight. Epoch is nothing but the total number of iterations. In [13]: # Fitting our model classifier . fit ( X_train , y_train , batch_size = 10 , epochs = 100 ) Epoch 1/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.4914 - acc: 0.8006 Epoch 2/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.4090 - acc: 0.8232 Epoch 3/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3953 - acc: 0.8291 Epoch 4/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3854 - acc: 0.8310 Epoch 5/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3782 - acc: 0.8322 Epoch 6/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3724 - acc: 0.8431 Epoch 7/100 8000/8000 [==============================] - 1s 142us/step - loss: 0.3688 - acc: 0.8449 Epoch 8/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3655 - acc: 0.8459 Epoch 9/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3628 - acc: 0.8506 Epoch 10/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3599 - acc: 0.8527 Epoch 11/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3571 - acc: 0.8526 Epoch 12/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3556 - acc: 0.8530 Epoch 13/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3544 - acc: 0.8531 Epoch 14/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3532 - acc: 0.8550 Epoch 15/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3509 - acc: 0.8560 Epoch 16/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3500 - acc: 0.8589 Epoch 17/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3500 - acc: 0.8576 Epoch 18/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3496 - acc: 0.8596 Epoch 19/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3485 - acc: 0.8572 Epoch 20/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3482 - acc: 0.8571 Epoch 21/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3476 - acc: 0.8587 Epoch 22/100 8000/8000 [==============================] - 1s 143us/step - loss: 0.3466 - acc: 0.8547 Epoch 23/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3470 - acc: 0.8585 Epoch 24/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3458 - acc: 0.8584 Epoch 25/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3458 - acc: 0.8590 Epoch 26/100 8000/8000 [==============================] - 1s 164us/step - loss: 0.3466 - acc: 0.8602 Epoch 27/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3447 - acc: 0.8591 Epoch 28/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3446 - acc: 0.8581 Epoch 29/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3439 - acc: 0.8590 Epoch 30/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3450 - acc: 0.8581 Epoch 31/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3445 - acc: 0.8592 Epoch 32/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3435 - acc: 0.8592 Epoch 33/100 8000/8000 [==============================] - 1s 158us/step - loss: 0.3424 - acc: 0.8607 Epoch 34/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3432 - acc: 0.8580 Epoch 35/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3433 - acc: 0.8597 Epoch 36/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3431 - acc: 0.8592 Epoch 37/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3430 - acc: 0.8609 Epoch 38/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3428 - acc: 0.8579 Epoch 39/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3424 - acc: 0.8595 Epoch 40/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3421 - acc: 0.8585 Epoch 41/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3419 - acc: 0.8592 Epoch 42/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3421 - acc: 0.8597 Epoch 43/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3419 - acc: 0.8582 Epoch 44/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3403 - acc: 0.8609 Epoch 45/100 8000/8000 [==============================] - 1s 184us/step - loss: 0.3414 - acc: 0.8569 Epoch 46/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3413 - acc: 0.8585 Epoch 47/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3414 - acc: 0.8606 Epoch 48/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3419 - acc: 0.8589 Epoch 49/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3411 - acc: 0.8589 Epoch 50/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3419 - acc: 0.8575 Epoch 51/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3403 - acc: 0.8617 Epoch 52/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3400 - acc: 0.8597 Epoch 53/100 8000/8000 [==============================] - 1s 155us/step - loss: 0.3409 - acc: 0.8596 Epoch 54/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3402 - acc: 0.8611 Epoch 55/100 8000/8000 [==============================] - 1s 158us/step - loss: 0.3401 - acc: 0.8591 Epoch 56/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3403 - acc: 0.8582 Epoch 57/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3404 - acc: 0.8622 Epoch 58/100 8000/8000 [==============================] - 1s 163us/step - loss: 0.3406 - acc: 0.8577 Epoch 59/100 8000/8000 [==============================] - 1s 160us/step - loss: 0.3394 - acc: 0.8592 Epoch 60/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3393 - acc: 0.8621 Epoch 61/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3411 - acc: 0.8601 Epoch 62/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3397 - acc: 0.8589 Epoch 63/100 8000/8000 [==============================] - 1s 160us/step - loss: 0.3400 - acc: 0.8590 Epoch 64/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3405 - acc: 0.8590 Epoch 65/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3395 - acc: 0.8606 Epoch 66/100 8000/8000 [==============================] - 1s 149us/step - loss: 0.3392 - acc: 0.8579 Epoch 67/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3397 - acc: 0.8590 Epoch 68/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3396 - acc: 0.8602 Epoch 69/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3390 - acc: 0.8610 Epoch 70/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3395 - acc: 0.8595 Epoch 71/100 8000/8000 [==============================] - 1s 165us/step - loss: 0.3389 - acc: 0.8597 Epoch 72/100 8000/8000 [==============================] - 1s 162us/step - loss: 0.3391 - acc: 0.8601 Epoch 73/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3389 - acc: 0.8605 Epoch 74/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3395 - acc: 0.8600 Epoch 75/100 8000/8000 [==============================] - 1s 151us/step - loss: 0.3391 - acc: 0.8600 Epoch 76/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3397 - acc: 0.8621 Epoch 77/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3385 - acc: 0.8614 Epoch 78/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3393 - acc: 0.8585 Epoch 79/100 8000/8000 [==============================] - 1s 152us/step - loss: 0.3390 - acc: 0.8597 Epoch 80/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3388 - acc: 0.8615 Epoch 81/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3395 - acc: 0.8622 Epoch 82/100 8000/8000 [==============================] - 1s 144us/step - loss: 0.3387 - acc: 0.8597 Epoch 83/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3385 - acc: 0.8602 Epoch 84/100 8000/8000 [==============================] - 1s 173us/step - loss: 0.3388 - acc: 0.8589 Epoch 85/100 8000/8000 [==============================] - 1s 166us/step - loss: 0.3389 - acc: 0.8617 Epoch 86/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3385 - acc: 0.8625 Epoch 87/100 8000/8000 [==============================] - 1s 154us/step - loss: 0.3379 - acc: 0.8579 Epoch 88/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3391 - acc: 0.8601 Epoch 89/100 8000/8000 [==============================] - 1s 147us/step - loss: 0.3378 - acc: 0.8601 Epoch 90/100 8000/8000 [==============================] - 1s 148us/step - loss: 0.3388 - acc: 0.8611 Epoch 91/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3384 - acc: 0.8596 Epoch 92/100 8000/8000 [==============================] - 1s 153us/step - loss: 0.3381 - acc: 0.8596 Epoch 93/100 8000/8000 [==============================] - 1s 150us/step - loss: 0.3380 - acc: 0.8576 Epoch 94/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3379 - acc: 0.8582 Epoch 95/100 8000/8000 [==============================] - 1s 146us/step - loss: 0.3376 - acc: 0.8591 Epoch 96/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3374 - acc: 0.8619 Epoch 97/100 8000/8000 [==============================] - 1s 157us/step - loss: 0.3370 - acc: 0.8626 Epoch 98/100 8000/8000 [==============================] - 1s 156us/step - loss: 0.3355 - acc: 0.8580 Epoch 99/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3361 - acc: 0.8625 Epoch 100/100 8000/8000 [==============================] - 1s 145us/step - loss: 0.3354 - acc: 0.8619 Out[13]: <keras.callbacks.History at 0x232a0752320> Predicting on the test data Predicting the test set result. The prediction result will give you probability of the customer leaving the company. We will convert that probability into binary 0 and 1. In [14]: # Predicting the Test set results y_pred = classifier . predict ( X_test ) y_pred = ( y_pred > 0.5 ) Confusion matrix of test set and prediction accuracy This is the final step where we are evaluating our model performance. We already have original results and thus we can build confusion matrix to check the accuracy of model. In [15]: # Creating the Confusion Matrix from sklearn.metrics import confusion_matrix cm = confusion_matrix ( y_test , y_pred ) cm Out[15]: array([[1477, 111], [ 186, 226]], dtype=int64) In [16]: test_accuracy = np . sum ([ cm [ 0 , 0 ], cm [ 1 , 1 ]]) / np . sum ( cm ) test_accuracy Out[16]: 0.8515 We achieved 85.15% accuracy on the test set which is quite good. Adjusting the neural network Let's changing the structure of the network to see what happens. Let's add another hidden layer. In [18]: #Initializing Neural Network classifier2 = Sequential () # Adding the input layer and the first hidden layer classifier2 . add ( Dense ( activation = \"relu\" , input_dim = 11 , units = 8 , kernel_initializer = \"uniform\" )) # Adding the second hidden layer classifier2 . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the third hidden layer classifier2 . add ( Dense ( activation = \"relu\" , units = 6 , kernel_initializer = \"uniform\" )) # Adding the output layer classifier2 . add ( Dense ( activation = \"sigmoid\" , units = 1 , kernel_initializer = \"uniform\" )) # Compiling Neural Network classifier2 . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) # Fitting our model classifier2 . fit ( X_train , y_train , batch_size = 10 , epochs = 100 ) # Predicting the Test set results y_pred2 = classifier2 . predict ( X_test ) y_pred2 = ( y_pred2 > 0.5 ) cm2 = confusion_matrix ( y_test , y_pred2 ) test_accuracy2 = np . sum ([ cm2 [ 0 , 0 ], cm2 [ 1 , 1 ]]) / np . sum ( cm2 ) test_accuracy2 Epoch 1/100 8000/8000 [==============================] - 2s 280us/step - loss: 0.4712 - acc: 0.7961 Epoch 2/100 8000/8000 [==============================] - 1s 186us/step - loss: 0.4183 - acc: 0.7969 Epoch 3/100 8000/8000 [==============================] - 2s 226us/step - loss: 0.4090 - acc: 0.8270 Epoch 4/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3973 - acc: 0.8306 Epoch 5/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3861 - acc: 0.8331 Epoch 6/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3772 - acc: 0.8431 Epoch 7/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3696 - acc: 0.8480 Epoch 8/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3659 - acc: 0.8495 Epoch 9/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3629 - acc: 0.8541 Epoch 10/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3613 - acc: 0.8555 Epoch 11/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3595 - acc: 0.8529 Epoch 12/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3587 - acc: 0.8561 Epoch 13/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3570 - acc: 0.8552 Epoch 14/100 8000/8000 [==============================] - 2s 217us/step - loss: 0.3554 - acc: 0.8560 Epoch 15/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3546 - acc: 0.8561 Epoch 16/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3544 - acc: 0.8564 Epoch 17/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3532 - acc: 0.8571 Epoch 18/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3517 - acc: 0.8595 Epoch 19/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3513 - acc: 0.8582 Epoch 20/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3516 - acc: 0.8562 Epoch 21/100 8000/8000 [==============================] - 2s 211us/step - loss: 0.3506 - acc: 0.8579 Epoch 22/100 8000/8000 [==============================] - 2s 204us/step - loss: 0.3485 - acc: 0.8589 Epoch 23/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3500 - acc: 0.8586 Epoch 24/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3505 - acc: 0.8579 Epoch 25/100 8000/8000 [==============================] - 2s 215us/step - loss: 0.3487 - acc: 0.8581 Epoch 26/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3484 - acc: 0.8594 Epoch 27/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3488 - acc: 0.8602 Epoch 28/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3486 - acc: 0.8574 Epoch 29/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3477 - acc: 0.8592 Epoch 30/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3480 - acc: 0.8590 Epoch 31/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3473 - acc: 0.8589 Epoch 32/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3458 - acc: 0.8587 Epoch 33/100 8000/8000 [==============================] - 2s 205us/step - loss: 0.3461 - acc: 0.8591 Epoch 34/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3452 - acc: 0.8616 Epoch 35/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3453 - acc: 0.8560 Epoch 36/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3454 - acc: 0.8581 Epoch 37/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3454 - acc: 0.8599 Epoch 38/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3454 - acc: 0.8585 Epoch 39/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3441 - acc: 0.8604 Epoch 40/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3443 - acc: 0.8596 Epoch 41/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3433 - acc: 0.8582 Epoch 42/100 8000/8000 [==============================] - 1s 187us/step - loss: 0.3435 - acc: 0.8586 Epoch 43/100 8000/8000 [==============================] - 2s 215us/step - loss: 0.3438 - acc: 0.8594 Epoch 44/100 8000/8000 [==============================] - 2s 206us/step - loss: 0.3436 - acc: 0.8590 Epoch 45/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3431 - acc: 0.8612 Epoch 46/100 8000/8000 [==============================] - 1s 187us/step - loss: 0.3431 - acc: 0.8599 Epoch 47/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3440 - acc: 0.8596 Epoch 48/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3427 - acc: 0.8602 Epoch 49/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3433 - acc: 0.8606 Epoch 50/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3415 - acc: 0.8621 Epoch 51/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3430 - acc: 0.8587 Epoch 52/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3418 - acc: 0.8600 Epoch 53/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3427 - acc: 0.8611 Epoch 54/100 8000/8000 [==============================] - 2s 217us/step - loss: 0.3415 - acc: 0.8612 Epoch 55/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3423 - acc: 0.8599 Epoch 56/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3421 - acc: 0.8604 Epoch 57/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3417 - acc: 0.8609 0s - loss: 0.3412 - a Epoch 58/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3426 - acc: 0.8599 Epoch 59/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3414 - acc: 0.8605 Epoch 60/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3424 - acc: 0.8607 Epoch 61/100 8000/8000 [==============================] - 2s 193us/step - loss: 0.3413 - acc: 0.8616 Epoch 62/100 8000/8000 [==============================] - 2s 199us/step - loss: 0.3418 - acc: 0.8611 Epoch 63/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3408 - acc: 0.8604 Epoch 64/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3411 - acc: 0.8626 Epoch 65/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3418 - acc: 0.8625 Epoch 66/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3411 - acc: 0.8621 Epoch 67/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3412 - acc: 0.8591 Epoch 68/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3404 - acc: 0.8607 Epoch 69/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3406 - acc: 0.8619 Epoch 70/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3407 - acc: 0.8590 Epoch 71/100 8000/8000 [==============================] - 2s 188us/step - loss: 0.3417 - acc: 0.8601 Epoch 72/100 8000/8000 [==============================] - 2s 189us/step - loss: 0.3414 - acc: 0.8615 Epoch 73/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3407 - acc: 0.8607 Epoch 74/100 8000/8000 [==============================] - 2s 222us/step - loss: 0.3413 - acc: 0.8615 Epoch 75/100 8000/8000 [==============================] - 2s 210us/step - loss: 0.3411 - acc: 0.8620 Epoch 76/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3394 - acc: 0.8610 Epoch 77/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3405 - acc: 0.8626 Epoch 78/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3403 - acc: 0.8586 Epoch 79/100 8000/8000 [==============================] - 2s 201us/step - loss: 0.3399 - acc: 0.8624 Epoch 80/100 8000/8000 [==============================] - 2s 204us/step - loss: 0.3404 - acc: 0.8600 0s - loss: 0.3438 - a Epoch 81/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3392 - acc: 0.8632 Epoch 82/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3399 - acc: 0.8617 Epoch 83/100 8000/8000 [==============================] - 2s 195us/step - loss: 0.3395 - acc: 0.8622 Epoch 84/100 8000/8000 [==============================] - 2s 222us/step - loss: 0.3389 - acc: 0.8611 Epoch 85/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3401 - acc: 0.8606 Epoch 86/100 8000/8000 [==============================] - 2s 191us/step - loss: 0.3397 - acc: 0.8631 Epoch 87/100 8000/8000 [==============================] - 2s 198us/step - loss: 0.3388 - acc: 0.8626 Epoch 88/100 8000/8000 [==============================] - 2s 203us/step - loss: 0.3404 - acc: 0.8616 Epoch 89/100 8000/8000 [==============================] - 2s 193us/step - loss: 0.3398 - acc: 0.8629 Epoch 90/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3394 - acc: 0.8609 Epoch 91/100 8000/8000 [==============================] - 2s 202us/step - loss: 0.3397 - acc: 0.8607 Epoch 92/100 8000/8000 [==============================] - 2s 197us/step - loss: 0.3396 - acc: 0.8611 Epoch 93/100 8000/8000 [==============================] - 2s 194us/step - loss: 0.3391 - acc: 0.8632 Epoch 94/100 8000/8000 [==============================] - 2s 208us/step - loss: 0.3395 - acc: 0.8619 Epoch 95/100 8000/8000 [==============================] - 2s 219us/step - loss: 0.3390 - acc: 0.8614 Epoch 96/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3389 - acc: 0.8632 Epoch 97/100 8000/8000 [==============================] - 2s 192us/step - loss: 0.3394 - acc: 0.8634 Epoch 98/100 8000/8000 [==============================] - 2s 196us/step - loss: 0.3396 - acc: 0.8620 Epoch 99/100 8000/8000 [==============================] - 2s 200us/step - loss: 0.3376 - acc: 0.8625 Epoch 100/100 8000/8000 [==============================] - 2s 190us/step - loss: 0.3392 - acc: 0.8605 Out[18]: 0.8555 Now the prediction accuracy on test set has increased a little bit from 85.15% to 85.55%. Adjusting the architect of the neural network may achieve better results.","tags":"Data Science","url":"https://haochen23.github.io/2018/08/Churn-rate-prediction.html","loc":"https://haochen23.github.io/2018/08/Churn-rate-prediction.html"},{"title":"SAT Results Analysis NYC","text":"Explore the SAT test in NYC The SAT, or Scholastic Aptitude Test, is a test that high schoolers take in the US before applying to college. Colleges take the test scores into account when making admissions decisions, so it's fairly important to do well on. The test is divided into 3 sections, each of which is scored out of 800 points. The total score is out of 2400 (although this has changed back and forth a few times, the scores in this dataset are out of 2400). High schools are often ranked by their average SAT scores, and high SAT scores are considered a sign of how good a school district is. Combine 7 related datasets together. Datasets descriptions can be found in Readme. Briefly, the datasets we use are listed as follows: SAT results - contains average SAT scores on the three subjects by each school and the number of SAT takers. High School Directory - contains information of each high school. Math test results contains math test results for each school. Class size - class size of each high school. AP test results - Advanced Placement exam results for each high school. Passing AP exams can get you college credit in the US. Graduation outcomes - percentage of graduated students and other outcome information. Demographics and accountability - demographic information for each school. School district maps - contains information on the layout of the school districts, so that we can map them out. School survey - surveys of parents, teachers, and students at each school (scores). Showcase: Data cleaning Data wrangling Data preprocessing Mapping and visulization Insights Understanding the data We first do a quick look at the data, i.e. what contains in each file. We read in all files and see what attributes are in each dataset. In [1]: import pandas as pd import numpy as np files = [ 'ap_college_board.csv' , 'class_size.csv' , 'demographics_and_accountability.csv' , 'graduation_outcomes.csv' , 'high_school_directory.csv' , 'math_test_results.csv' , 'sat_results.csv' ] data = {} for f in files : d = pd . read_csv ( \"data/ {} \" . format ( f )) data [ f . replace ( \".csv\" , \"\" )] = d In [2]: for k , v in data . items (): print ( \" \\n \" + k + \" \\n \" ) print ( v . head ( 5 )) ap_college_board DBN SchoolName AP Test Takers \\ 0 01M448 UNIVERSITY NEIGHBORHOOD H.S. 39 1 01M450 EAST SIDE COMMUNITY HS 19 2 01M515 LOWER EASTSIDE PREP 24 3 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 4 02M296 High School of Hospitality Management s Total Exams Taken Number of Exams with scores 3 4 or 5 0 49 10 1 21 s 2 26 24 3 377 191 4 s s class_size CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE \\ 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) \\ 0 - - 1 - - 2 - - 3 - - 4 - - SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED \\ 0 - 19.0 1 - 21.0 2 - 17.0 3 - 17.0 4 - 15.0 NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS \\ 0 1.0 19.0 19.0 1 1.0 21.0 21.0 2 1.0 17.0 17.0 3 1.0 17.0 17.0 4 1.0 15.0 15.0 SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO 0 19.0 ATS NaN 1 21.0 ATS NaN 2 17.0 ATS NaN 3 17.0 ATS NaN 4 15.0 ATS NaN demographics_and_accountability DBN Name schoolyear fl_percent frl_percent \\ 0 01M015 P.S. 015 ROBERTO CLEMENTE 20052006 89.4 NaN 1 01M015 P.S. 015 ROBERTO CLEMENTE 20062007 89.4 NaN 2 01M015 P.S. 015 ROBERTO CLEMENTE 20072008 89.4 NaN 3 01M015 P.S. 015 ROBERTO CLEMENTE 20082009 89.4 NaN 4 01M015 P.S. 015 ROBERTO CLEMENTE 20092010 96.5 total_enrollment prek k grade1 grade2 ... black_num black_per \\ 0 281 15 36 40 33 ... 74 26.3 1 243 15 29 39 38 ... 68 28.0 2 261 18 43 39 36 ... 77 29.5 3 252 17 37 44 32 ... 75 29.8 4 208 16 40 28 32 ... 67 32.2 hispanic_num hispanic_per white_num white_per male_num male_per female_num \\ 0 189 67.3 5 1.8 158.0 56.2 123.0 1 153 63.0 4 1.6 140.0 57.6 103.0 2 157 60.2 7 2.7 143.0 54.8 118.0 3 149 59.1 7 2.8 149.0 59.1 103.0 4 118 56.7 6 2.9 124.0 59.6 84.0 female_per 0 43.8 1 42.4 2 45.2 3 40.9 4 40.4 [5 rows x 38 columns] graduation_outcomes Demographic DBN School Name Cohort \\ 0 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2003 1 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2004 2 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2005 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 4 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 Aug Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n \\ 0 5 s s s 1 55 37 67.3% 17 2 64 43 67.2% 27 3 78 43 55.1% 36 4 78 44 56.4% 37 Total Regents - % of cohort Total Regents - % of grads \\ 0 s s 1 30.9% 45.9% 2 42.2% 62.8% 3 46.2% 83.7% 4 47.4% 84.1% ... Regents w/o Advanced - n \\ 0 ... s 1 ... 17 2 ... 27 3 ... 36 4 ... 37 Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads \\ 0 s s 1 30.9% 45.9% 2 42.2% 62.8% 3 46.2% 83.7% 4 47.4% 84.1% Local - n Local - % of cohort Local - % of grads Still Enrolled - n \\ 0 s s s s 1 20 36.4% 54.1% 15 2 16 25% 37.200000000000003% 9 3 7 9% 16.3% 16 4 7 9% 15.9% 15 Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 0 s s s 1 27.3% 3 5.5% 2 14.1% 9 14.1% 3 20.5% 11 14.1% 4 19.2% 11 14.1% [5 rows x 23 columns] high_school_directory dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max \\ 0 NaN NaN 1 NaN NaN 2 NaN NaN 3 9.0 14.0 4 NaN NaN ... priority08 priority09 \\ 0 ... NaN NaN 1 ... NaN NaN 2 ... NaN NaN 3 ... NaN NaN 4 ... NaN NaN priority10 Location 1 \\ 0 NaN 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 1 NaN 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 2 NaN 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 3 NaN 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 4 NaN 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... Community Board Council District Census Tract BIN BBL \\ 0 9.0 35.0 213.0 3029686.0 3.011870e+09 1 3.0 16.0 135.0 2004526.0 2.026340e+09 2 4.0 14.0 209.0 2008336.0 2.028590e+09 3 1.0 1.0 29.0 1001388.0 1.001130e+09 4 8.0 24.0 1267.0 4539721.0 4.068580e+09 NTA 0 Crown Heights South ... 1 Morrisania-Melrose ... 2 West Concourse ... 3 Chinatown ... 4 Pomonok-Flushing Heights-Hillcrest ... [5 rows x 64 columns] math_test_results DBN Grade Year Category Number Tested Mean Scale Score Level 1 # \\ 0 01M015 3 2006 All Students 39 667 2 1 01M015 3 2007 All Students 31 672 2 2 01M015 3 2008 All Students 37 668 0 3 01M015 3 2009 All Students 33 668 0 4 01M015 3 2010 All Students 26 677 6 Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # Level 4 % \\ 0 5.1% 11 28.2% 20 51.3% 6 15.4% 1 6.5% 3 9.7% 22 71% 4 12.9% 2 0% 6 16.2% 29 78.4% 2 5.4% 3 0% 4 12.1% 28 84.8% 1 3% 4 23.1% 12 46.2% 6 23.1% 2 7.7% Level 3+4 # Level 3+4 % 0 26 66.7% 1 26 83.9% 2 31 83.8% 3 29 87.9% 4 8 30.8% sat_results DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score \\ 0 29 355 404 1 91 383 423 2 70 377 402 3 7 414 401 4 44 390 433 SAT Writing Avg. Score 0 363 1 366 2 370 3 359 4 384 Combine Files into 1 Single Dataset DBN column appears in all the datasets, it is a unique code for each school in NYC. The problem now is that two of the datasets, class_size, and hs_directory, don't have a DBN field. In the high_school_directory data, it's just named dbn, so we can just rename the column, or copy it over into a new column called DBN. In the class_size data, we'll need to try a different approach. Most DBN in the dataset looks like \"01M448\". For example: In [3]: data [ \"ap_college_board\" ][ \"DBN\" ] . head ( 3 ) Out[3]: 0 01M448 1 01M450 2 01M515 Name: DBN, dtype: object Although the class_size dataset does not contain the DBN columnn, it has certain columns that can be used to build a DBN column. Let's look at the columns in class_size. The first 3 columns are just what we need. In [4]: data [ \"class_size\" ] . head () Out[4]: CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED - - - 19.0 1.0 19.0 19.0 19.0 ATS NaN 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT - - - 21.0 1.0 21.0 21.0 21.0 ATS NaN 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED - - - 17.0 1.0 17.0 17.0 17.0 ATS NaN 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT - - - 17.0 1.0 17.0 17.0 17.0 ATS NaN 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED - - - 15.0 1.0 15.0 15.0 15.0 ATS NaN Now we have enough information to build the DBN columns for class_size and high_school_directory. We add a new column DBN to these two datasets. In [5]: data [ \"class_size\" ][ \"DBN\" ] = ( data [ \"class_size\" ] . apply ( lambda x : \" {0:02d}{1} \" . format ( x [ \"CSD\" ], x [ \"SCHOOL CODE\" ]), axis = 1 )) data [ \"high_school_directory\" ][ \"DBN\" ] = data [ \"high_school_directory\" ][ \"dbn\" ] Adding in the survey data. Student, parent, and teacher suverys about the quality of schools. One of the most potentially interesting datasets to look at is the dataset on student, parent, and teacher surveys about the quality of schools. These surveys include information about the perceived safety of each school, academic standards, and more. Before we combine our datasets, let's add in the survey data. In this case, we'll add the survey data into our data dictionary, and then combine all the datasets afterwards. The survey data consists of 2 files, one for all schools, and one for school district 75. In [6]: survey1 = pd . read_excel ( \"data/survey_all.xlsx\" , sheet_name = 'Sheet1' ) survey2 = pd . read_excel ( \"data/survey_d75.xlsx\" , sheet_name = 'Sheet1' ) print ( survey1 . head ()) print ( survey2 . head ()) dbn sch_type location \\ 0 01M015 Elementary School P.S. 015 Roberto Clemente 1 01M019 Elementary School P.S. 019 Asher Levy 2 01M020 Elementary School P.S. 020 Anna Silver 3 01M034 Elementary / Middle School P.S. 034 Franklin D. Roosevelt 4 01M063 Elementary School P.S. 063 William McKinley enrollment borough principal studentsurvey rr_s rr_t \\ 0 198 MANHATTAN Thomas Staebell No NaN 93 1 286 MANHATTAN JACQUELINE FLANAGAN No NaN 69 2 539 MANHATTAN James Lee No NaN 59 3 396 MANHATTAN Joyce Stallings Harte Yes 91.0 48 4 172 MANHATTAN DARLENE DESPEIGNES No NaN 55 rr_p ... s_N_q14e_3 s_N_q14e_4 s_N_q14f_1 s_N_q14f_2 \\ 0 63 ... NaN NaN NaN NaN 1 33 ... NaN NaN NaN NaN 2 44 ... NaN NaN NaN NaN 3 39 ... 26.0 14.0 28.0 58.0 4 73 ... NaN NaN NaN NaN s_N_q14f_3 s_N_q14f_4 s_N_q14g_1 s_N_q14g_2 s_N_q14g_3 s_N_q14g_4 0 NaN NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN NaN 3 39.0 25.0 46.0 41.0 17.0 12.0 4 NaN NaN NaN NaN NaN NaN [5 rows x 1976 columns] dbn sch_type location enrollment borough principal \\ 0 75K004 ES P.S. K004 320 BROOKLYN Deborah Evans 1 75K036 ES/MS/HS P.S. K036 292 BROOKLYN Johanna Schneider 2 75K053 ES/MS/HS P.S. K053 375 BROOKLYN AMY BLUTSTEIN 3 75K077 ES/MS/HS P.S. K077 294 BROOKLYN Merryl Redner-Cohen 4 75K140 ES/MS P.S. K140 282 BROOKLYN Michelle Carpenter studentsurvey rr_s rr_t rr_p ... s_q14_2 s_q14_3 s_q14_4 \\ 0 NO NaN 72 33 ... NaN NaN NaN 1 Yes 8.0 20 9 ... 36.0 18.0 45.0 2 Yes 83.0 79 56 ... 11.0 15.0 10.0 3 Yes 76.0 73 54 ... 8.0 4.0 10.0 4 Yes 88.0 87 42 ... 33.0 41.0 9.0 s_q14_5 s_q14_6 s_q14_7 s_q14_8 s_q14_9 s_q14_10 s_q14_11 0 NaN NaN NaN NaN NaN NaN NaN 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 15.0 11.0 6.0 18.0 4.0 1.0 1.0 3 13.0 17.0 13.0 10.0 8.0 12.0 2.0 4 0.0 0.0 0.0 0.0 0.0 0.0 3.0 [5 rows x 1800 columns] In [7]: survey1 [ \"d75\" ] = False survey2 [ \"d75\" ] = True survey = pd . concat ([ survey1 , survey2 ], axis = 0 , sort = False ) print ( survey1 . shape ) print ( survey2 . shape ) print ( survey . shape ) survey . head () (1597, 1977) (56, 1801) (1653, 2852) Out[7]: dbn sch_type location enrollment borough principal studentsurvey rr_s rr_t rr_p ... s_q14_2 s_q14_3 s_q14_4 s_q14_5 s_q14_6 s_q14_7 s_q14_8 s_q14_9 s_q14_10 s_q14_11 0 01M015 Elementary School P.S. 015 Roberto Clemente 198 MANHATTAN Thomas Staebell No NaN 93 63 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 01M019 Elementary School P.S. 019 Asher Levy 286 MANHATTAN JACQUELINE FLANAGAN No NaN 69 33 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 01M020 Elementary School P.S. 020 Anna Silver 539 MANHATTAN James Lee No NaN 59 44 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 01M034 Elementary / Middle School P.S. 034 Franklin D. Roosevelt 396 MANHATTAN Joyce Stallings Harte Yes 91.0 48 39 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 01M063 Elementary School P.S. 063 William McKinley 172 MANHATTAN DARLENE DESPEIGNES No NaN 55 73 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 2852 columns Too many columns are in the survey data, and most of them are extraneous. By looking at the column dictionary of the survey data, let's just keep the important fields and remove any extraneous columns. In [8]: import warnings warnings . filterwarnings ( 'ignore' ) survey [ \"DBN\" ] = survey [ \"dbn\" ] survey_fields = [ \"DBN\" , \"rr_s\" , \"rr_t\" , \"rr_p\" , \"N_s\" , \"N_t\" , \"N_p\" , \"saf_p_11\" , \"com_p_11\" , \"eng_p_11\" , \"aca_p_11\" , \"saf_t_11\" , \"com_t_11\" , \"eng_t_10\" , \"aca_t_11\" , \"saf_s_11\" , \"com_s_11\" , \"eng_s_11\" , \"aca_s_11\" , \"saf_tot_11\" , \"com_tot_11\" , \"eng_tot_11\" , \"aca_tot_11\" ,] survey = survey . loc [:, survey_fields ] data [ \"survey\" ] = survey survey . shape Out[8]: (1653, 23) Condensing data, more preprocessing The datasets such as class_size , demographics_and_accountabilities, have multiple rows for each school, whereas the datasets such as sat_results and high_school_directory only has one row per school. We need to find a way to combine these datasets together. For example: In [9]: print ( data [ \"demographics_and_accountability\" ] . head ()) print ( data [ \"class_size\" ] . head ()) DBN Name schoolyear fl_percent frl_percent \\ 0 01M015 P.S. 015 ROBERTO CLEMENTE 20052006 89.4 NaN 1 01M015 P.S. 015 ROBERTO CLEMENTE 20062007 89.4 NaN 2 01M015 P.S. 015 ROBERTO CLEMENTE 20072008 89.4 NaN 3 01M015 P.S. 015 ROBERTO CLEMENTE 20082009 89.4 NaN 4 01M015 P.S. 015 ROBERTO CLEMENTE 20092010 96.5 total_enrollment prek k grade1 grade2 ... black_num black_per \\ 0 281 15 36 40 33 ... 74 26.3 1 243 15 29 39 38 ... 68 28.0 2 261 18 43 39 36 ... 77 29.5 3 252 17 37 44 32 ... 75 29.8 4 208 16 40 28 32 ... 67 32.2 hispanic_num hispanic_per white_num white_per male_num male_per female_num \\ 0 189 67.3 5 1.8 158.0 56.2 123.0 1 153 63.0 4 1.6 140.0 57.6 103.0 2 157 60.2 7 2.7 143.0 54.8 118.0 3 149 59.1 7 2.8 149.0 59.1 103.0 4 118 56.7 6 2.9 124.0 59.6 84.0 female_per 0 43.8 1 42.4 2 45.2 3 40.9 4 40.4 [5 rows x 38 columns] CSD BOROUGH SCHOOL CODE SCHOOL NAME GRADE PROGRAM TYPE \\ 0 1 M M015 P.S. 015 Roberto Clemente 0K GEN ED 1 1 M M015 P.S. 015 Roberto Clemente 0K CTT 2 1 M M015 P.S. 015 Roberto Clemente 01 GEN ED 3 1 M M015 P.S. 015 Roberto Clemente 01 CTT 4 1 M M015 P.S. 015 Roberto Clemente 02 GEN ED CORE SUBJECT (MS CORE and 9-12 ONLY) CORE COURSE (MS CORE and 9-12 ONLY) \\ 0 - - 1 - - 2 - - 3 - - 4 - - SERVICE CATEGORY(K-9* ONLY) NUMBER OF STUDENTS / SEATS FILLED \\ 0 - 19.0 1 - 21.0 2 - 17.0 3 - 17.0 4 - 15.0 NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS \\ 0 1.0 19.0 19.0 1 1.0 21.0 21.0 2 1.0 17.0 17.0 3 1.0 17.0 17.0 4 1.0 15.0 15.0 SIZE OF LARGEST CLASS DATA SOURCE SCHOOLWIDE PUPIL-TEACHER RATIO DBN 0 19.0 ATS NaN 01M015 1 21.0 ATS NaN 01M015 2 17.0 ATS NaN 01M015 3 17.0 ATS NaN 01M015 4 15.0 ATS NaN 01M015 In [10]: print ( data [ \"sat_results\" ] . head ()) print ( data [ \"high_school_directory\" ] . head ()) DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score SAT Math Avg. Score \\ 0 29 355 404 1 91 383 423 2 70 377 402 3 7 414 401 4 44 390 433 SAT Writing Avg. Score 0 363 1 366 2 370 3 359 4 384 dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max ... priority09 priority10 \\ 0 NaN NaN ... NaN NaN 1 NaN NaN ... NaN NaN 2 NaN NaN ... NaN NaN 3 9.0 14.0 ... NaN NaN 4 NaN NaN ... NaN NaN Location 1 Community Board \\ 0 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 9.0 1 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 3.0 2 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 4.0 3 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 1.0 4 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... 8.0 Council District Census Tract BIN BBL \\ 0 35.0 213.0 3029686.0 3.011870e+09 1 16.0 135.0 2004526.0 2.026340e+09 2 14.0 209.0 2008336.0 2.028590e+09 3 1.0 29.0 1001388.0 1.001130e+09 4 24.0 1267.0 4539721.0 4.068580e+09 NTA DBN 0 Crown Heights South ... 17K548 1 Morrisania-Melrose ... 09X543 2 West Concourse ... 09X327 3 Chinatown ... 02M280 4 Pomonok-Flushing Heights-Hillcrest ... 28Q680 [5 rows x 65 columns] In order to concatenate these datasets into 1 dataset, we need to make every data set contains only one row per school. If not, we cannnot compare SAT results to the class size. We can accomplish this by first understanding the data better, then by doing some aggregation. With the class_size dataset, it looks like GRADE and PROGRAM TYPE have multiple values for each school. By restricting each field to a single value, we can filter most of the duplicate rows. In the below code, we: Only select values from class_size where the GRADE field is 09-12. Only select values from class_size where the PROGRAM TYPE field is GEN ED. Group the class_size dataset by DBN, and take the average of each column. -Essentially, we'll find the average class_size values for each school. Reset the index, so DBN is added back in as a column. In [11]: class_size = data [ \"class_size\" ] # we only keep the high school data, which is grade 9 to 12 with general education class_size = class_size [ class_size [ \"GRADE \" ] == \"09-12\" ] class_size = class_size [ class_size [ \"PROGRAM TYPE\" ] == \"GEN ED\" ] #the aggregation by mean will drop non-numerical columns class_size = class_size . groupby ( \"DBN\" ) . agg ( np . mean ) class_size . reset_index ( inplace = True ) data [ \"class_size\" ] = class_size In [12]: # condensing the demographics_and_accountability dataset # we choose the school year of 20112012 demographics_and_accountability = data [ \"demographics_and_accountability\" ] demographics_and_accountability = demographics_and_accountability [ demographics_and_accountability [ \"schoolyear\" ] == 20112012 ] data [ \"demographics_and_accountability\" ] = demographics_and_accountability In [13]: #condensing the math_test_results dataset # we choose the year of 2011, and Grade 8 data [ \"math_test_results\" ] = data [ \"math_test_results\" ][ data [ \"math_test_results\" ][ \"Year\" ] == 2011 ] data [ \"math_test_results\" ] = data [ \"math_test_results\" ][ data [ \"math_test_results\" ][ \"Grade\" ] == '8' ] data [ \"math_test_results\" ] . head () Out[13]: DBN Grade Year Category Number Tested Mean Scale Score Level 1 # Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # Level 4 % Level 3+4 # Level 3+4 % 111 01M034 8 2011 All Students 48 646 15 31.3% 22 45.8% 11 22.9% 0 0% 11 22.9% 280 01M140 8 2011 All Students 61 665 1 1.6% 43 70.5% 17 27.9% 0 0% 17 27.9% 346 01M184 8 2011 All Students 49 727 0 0% 0 0% 5 10.2% 44 89.8% 49 100% 388 01M188 8 2011 All Students 49 658 10 20.4% 26 53.1% 10 20.4% 3 6.1% 13 26.5% 411 01M292 8 2011 All Students 49 650 15 30.6% 25 51% 7 14.3% 2 4.1% 9 18.4% In [14]: # condesing the graduation_outcomes dataset data [ \"graduation_outcomes\" ] = data [ \"graduation_outcomes\" ][ data [ \"graduation_outcomes\" ][ \"Cohort\" ] == \"2006\" ] data [ \"graduation_outcomes\" ] = data [ \"graduation_outcomes\" ][ data [ \"graduation_outcomes\" ][ \"Demographic\" ] == \"Total Cohort\" ] data [ \"graduation_outcomes\" ] . head () Out[14]: Demographic DBN School Name Cohort Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n Total Regents - % of cohort Total Regents - % of grads ... Regents w/o Advanced - n Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads Local - n Local - % of cohort Local - % of grads Still Enrolled - n Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 78 43 55.1% 36 46.2% 83.7% ... 36 46.2% 83.7% 7 9% 16.3% 16 20.5% 11 14.1% 10 Total Cohort 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2006 124 53 42.7% 42 33.9% 79.2% ... 34 27.4% 64.2% 11 8.9% 20.8% 46 37.1% 20 16.100000000000001% 17 Total Cohort 01M450 EAST SIDE COMMUNITY SCHOOL 2006 90 70 77.8% 67 74.400000000000006% 95.7% ... 67 74.400000000000006% 95.7% 3 3.3% 4.3% 15 16.7% 5 5.6% 24 Total Cohort 01M509 MARTA VALLE HIGH SCHOOL 2006 84 47 56% 40 47.6% 85.1% ... 23 27.4% 48.9% 7 8.300000000000001% 14.9% 25 29.8% 5 6% 31 Total Cohort 01M515 LOWER EAST SIDE PREPARATORY HIGH SCHO 2006 193 105 54.4% 91 47.2% 86.7% ... 22 11.4% 21% 14 7.3% 13.3% 53 27.5% 35 18.100000000000001% 5 rows × 23 columns Feature engineering For the sat_results dataset, we add a new column to compute sat_score to store the total score of 'SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score' In [15]: cols = [ 'SAT Math Avg. Score' , 'SAT Critical Reading Avg. Score' , 'SAT Writing Avg. Score' ] # convert string to numerical data for c in cols : data [ \"sat_results\" ][ c ] = data [ \"sat_results\" ][ c ] . convert_objects ( convert_numeric = True ) data [ \"sat_results\" ][ \"sat_score\" ] = ( data [ \"sat_results\" ][ cols [ 0 ]] + data [ \"sat_results\" ][ cols [ 1 ]] + data [ \"sat_results\" ][ cols [ 2 ]]) data [ \"sat_results\" ][ \"sat_score\" ] . head () Out[15]: 0 1122.0 1 1172.0 2 1149.0 3 1174.0 4 1207.0 Name: sat_score, dtype: float64 Now, we are finding the coordinates of each school so that we can make maps. The coordinates can be parsed out from the \"Location 1\" column in the high_school_directory dataset. In [16]: data [ \"high_school_directory\" ][ 'latitude' ] = data [ \"high_school_directory\" ][ 'Location 1' ] . apply ( lambda x : x . split ( \" \\n \" )[ - 1 ] . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . split ( \", \" )[ 0 ]) data [ \"high_school_directory\" ][ 'longitude' ] = data [ \"high_school_directory\" ][ 'Location 1' ] . apply ( lambda x : x . split ( \" \\n \" )[ - 1 ] . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" ) . split ( \", \" )[ 1 ]) # convert string to numerical data for c in [ \"latitude\" , \"longitude\" ]: data [ \"high_school_directory\" ][ c ] = data [ \"high_school_directory\" ][ c ] . convert_objects ( convert_numeric = True ) print ( data [ \"high_school_directory\" ][ 'latitude' ] . head ()) print ( data [ \"high_school_directory\" ][ 'longitude' ] . head ()) 0 40.670299 1 40.827603 2 40.842414 3 40.710679 4 40.718810 Name: latitude, dtype: float64 0 -73.961648 1 -73.904475 2 -73.916162 3 -74.000807 4 -73.806500 Name: longitude, dtype: float64 We have done the preliminaries now. Let's see what are in each dataset. In [17]: for k , v in data . items (): print ( \" \\n \" + k + \" \\n \" ) print ( v . head ()) ap_college_board DBN SchoolName AP Test Takers \\ 0 01M448 UNIVERSITY NEIGHBORHOOD H.S. 39 1 01M450 EAST SIDE COMMUNITY HS 19 2 01M515 LOWER EASTSIDE PREP 24 3 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 4 02M296 High School of Hospitality Management s Total Exams Taken Number of Exams with scores 3 4 or 5 0 49 10 1 21 s 2 26 24 3 377 191 4 s s class_size DBN CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS \\ 0 01M292 1 88.0000 4.000000 1 01M332 1 46.0000 2.000000 2 01M378 1 33.0000 1.000000 3 01M448 1 105.6875 4.750000 4 01M450 1 57.6000 2.733333 AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS SIZE OF LARGEST CLASS \\ 0 22.564286 18.50 26.571429 1 22.000000 21.00 23.500000 2 33.000000 33.00 33.000000 3 22.231250 18.25 27.062500 4 21.200000 19.40 22.866667 SCHOOLWIDE PUPIL-TEACHER RATIO 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN demographics_and_accountability DBN Name schoolyear \\ 6 01M015 P.S. 015 ROBERTO CLEMENTE 20112012 13 01M019 P.S. 019 ASHER LEVY 20112012 20 01M020 PS 020 ANNA SILVER 20112012 27 01M034 PS 034 FRANKLIN D ROOSEVELT 20112012 35 01M063 PS 063 WILLIAM MCKINLEY 20112012 fl_percent frl_percent total_enrollment prek k grade1 grade2 \\ 6 NaN 89.4 189 13 31 35 28 13 NaN 61.5 328 32 46 52 54 20 NaN 92.5 626 52 102 121 87 27 NaN 99.7 401 14 34 38 36 35 NaN 78.9 176 18 20 30 21 ... black_num black_per hispanic_num hispanic_per white_num \\ 6 ... 63 33.3 109 57.7 4 13 ... 81 24.7 158 48.2 28 20 ... 55 8.8 357 57.0 16 27 ... 90 22.4 275 68.6 8 35 ... 41 23.3 110 62.5 15 white_per male_num male_per female_num female_per 6 2.1 97.0 51.3 92.0 48.7 13 8.5 147.0 44.8 181.0 55.2 20 2.6 330.0 52.7 296.0 47.3 27 2.0 204.0 50.9 197.0 49.1 35 8.5 97.0 55.1 79.0 44.9 [5 rows x 38 columns] graduation_outcomes Demographic DBN School Name Cohort \\ 3 Total Cohort 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL 2006 10 Total Cohort 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2006 17 Total Cohort 01M450 EAST SIDE COMMUNITY SCHOOL 2006 24 Total Cohort 01M509 MARTA VALLE HIGH SCHOOL 2006 31 Total Cohort 01M515 LOWER EAST SIDE PREPARATORY HIGH SCHO 2006 Total Cohort Total Grads - n Total Grads - % of cohort Total Regents - n \\ 3 78 43 55.1% 36 10 124 53 42.7% 42 17 90 70 77.8% 67 24 84 47 56% 40 31 193 105 54.4% 91 Total Regents - % of cohort Total Regents - % of grads \\ 3 46.2% 83.7% 10 33.9% 79.2% 17 74.400000000000006% 95.7% 24 47.6% 85.1% 31 47.2% 86.7% ... Regents w/o Advanced - n \\ 3 ... 36 10 ... 34 17 ... 67 24 ... 23 31 ... 22 Regents w/o Advanced - % of cohort Regents w/o Advanced - % of grads \\ 3 46.2% 83.7% 10 27.4% 64.2% 17 74.400000000000006% 95.7% 24 27.4% 48.9% 31 11.4% 21% Local - n Local - % of cohort Local - % of grads Still Enrolled - n \\ 3 7 9% 16.3% 16 10 11 8.9% 20.8% 46 17 3 3.3% 4.3% 15 24 7 8.300000000000001% 14.9% 25 31 14 7.3% 13.3% 53 Still Enrolled - % of cohort Dropped Out - n Dropped Out - % of cohort 3 20.5% 11 14.1% 10 37.1% 20 16.100000000000001% 17 16.7% 5 5.6% 24 29.8% 5 6% 31 27.5% 35 18.100000000000001% [5 rows x 23 columns] high_school_directory dbn school_name borough \\ 0 17K548 Brooklyn School for Music & Theatre Brooklyn 1 09X543 High School for Violin and Dance Bronx 2 09X327 Comprehensive Model School Project M.S. 327 Bronx 3 02M280 Manhattan Early College School for Advertising Manhattan 4 28Q680 Queens Gateway to Health Sciences Secondary Sc... Queens building_code phone_number fax_number grade_span_min grade_span_max \\ 0 K440 718-230-6250 718-230-6262 9.0 12 1 X400 718-842-0687 718-589-9849 9.0 12 2 X240 718-294-8111 718-294-8109 6.0 12 3 M520 718-935-3477 NaN 9.0 10 4 Q695 718-969-3155 718-969-3552 6.0 12 expgrade_span_min expgrade_span_max ... \\ 0 NaN NaN ... 1 NaN NaN ... 2 NaN NaN ... 3 9.0 14.0 ... 4 NaN NaN ... Location 1 Community Board \\ 0 883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67... 9.0 1 1110 Boston Road\\nBronx, NY 10456\\n(40.8276026... 3.0 2 1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241... 4.0 3 411 Pearl Street\\nNew York, NY 10038\\n(40.7106... 1.0 4 160 20 Goethals Avenue\\nJamaica, NY 11432\\n(40... 8.0 Council District Census Tract BIN BBL \\ 0 35.0 213.0 3029686.0 3.011870e+09 1 16.0 135.0 2004526.0 2.026340e+09 2 14.0 209.0 2008336.0 2.028590e+09 3 1.0 29.0 1001388.0 1.001130e+09 4 24.0 1267.0 4539721.0 4.068580e+09 NTA DBN latitude \\ 0 Crown Heights South ... 17K548 40.670299 1 Morrisania-Melrose ... 09X543 40.827603 2 West Concourse ... 09X327 40.842414 3 Chinatown ... 02M280 40.710679 4 Pomonok-Flushing Heights-Hillcrest ... 28Q680 40.718810 longitude 0 -73.961648 1 -73.904475 2 -73.916162 3 -74.000807 4 -73.806500 [5 rows x 67 columns] math_test_results DBN Grade Year Category Number Tested Mean Scale Score \\ 111 01M034 8 2011 All Students 48 646 280 01M140 8 2011 All Students 61 665 346 01M184 8 2011 All Students 49 727 388 01M188 8 2011 All Students 49 658 411 01M292 8 2011 All Students 49 650 Level 1 # Level 1 % Level 2 # Level 2 % Level 3 # Level 3 % Level 4 # \\ 111 15 31.3% 22 45.8% 11 22.9% 0 280 1 1.6% 43 70.5% 17 27.9% 0 346 0 0% 0 0% 5 10.2% 44 388 10 20.4% 26 53.1% 10 20.4% 3 411 15 30.6% 25 51% 7 14.3% 2 Level 4 % Level 3+4 # Level 3+4 % 111 0% 11 22.9% 280 0% 17 27.9% 346 89.8% 49 100% 388 6.1% 13 26.5% 411 4.1% 9 18.4% sat_results DBN SCHOOL NAME \\ 0 01M292 HENRY STREET SCHOOL FOR INTERNATIONAL STUDIES 1 01M448 UNIVERSITY NEIGHBORHOOD HIGH SCHOOL 2 01M450 EAST SIDE COMMUNITY SCHOOL 3 01M458 FORSYTH SATELLITE ACADEMY 4 01M509 MARTA VALLE HIGH SCHOOL Num of SAT Test Takers SAT Critical Reading Avg. Score \\ 0 29 355.0 1 91 383.0 2 70 377.0 3 7 414.0 4 44 390.0 SAT Math Avg. Score SAT Writing Avg. Score sat_score 0 404.0 363.0 1122.0 1 423.0 366.0 1172.0 2 402.0 370.0 1149.0 3 401.0 359.0 1174.0 4 433.0 384.0 1207.0 survey DBN rr_s rr_t rr_p N_s N_t N_p saf_p_11 com_p_11 eng_p_11 \\ 0 01M015 NaN 93 63 NaN 27.0 104.0 NaN NaN NaN 1 01M019 NaN 69 33 NaN 24.0 89.0 NaN NaN NaN 2 01M020 NaN 59 44 NaN 32.0 207.0 NaN NaN NaN 3 01M034 91.0 48 39 159.0 22.0 119.0 NaN NaN NaN 4 01M063 NaN 55 73 NaN 17.0 117.0 NaN NaN NaN ... eng_t_10 aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 \\ 0 ... 7.9 NaN NaN NaN NaN NaN 1 ... 7.8 NaN NaN NaN NaN NaN 2 ... 6.9 NaN NaN NaN NaN NaN 3 ... 6.7 NaN NaN NaN NaN NaN 4 ... 7.5 NaN NaN NaN NaN NaN saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 0 NaN NaN NaN NaN 1 NaN NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN [5 rows x 23 columns] Combining all datasets together From the above display, we see that each dataset has a column of \"DBN\". We can use this column to combine all datasets into one. Handle missing values during combining When we join them, it's important to note that some of the datasets are missing high schools that exist in the sat_results dataset. To resolve this, we'll need to merge the datasets that have missing rows using the outer join strategy, so we don't lose data. It's a common thing to have missing values. In [18]: flat_data_names = [ k for k , v in data . items ()] flat_data = [ data [ k ] for k in flat_data_names ] # flat_data[0] is our first dataset which is ap_college_board full = flat_data [ 0 ] for i , f in enumerate ( flat_data [ 1 :]): name = flat_data_names [ i + 1 ] print ( name ) print ( len ( f [ \"DBN\" ]) - len ( f [ \"DBN\" ] . unique ())) join_type = \"inner\" if name in [ \"sat_results\" , \"ap_college_board\" , \"graduation_outcomes\" ]: join_type = \"outer\" if name not in [ \"ap_college_board\" ]: full = full . merge ( f , on = \"DBN\" , how = join_type ) full . shape class_size 0 demographics_and_accountability 0 graduation_outcomes 0 high_school_directory 0 math_test_results 0 sat_results 0 survey 0 Out[18]: (455, 180) In [19]: full . head () Out[19]: DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS ... eng_t_10 aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 0 01M450 EAST SIDE COMMUNITY HS 19 21 s 1.0 57.600000 2.733333 21.200000 19.400000 ... 8.1 NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 377 191 1.0 156.368421 6.157895 25.510526 19.473684 ... 6.6 NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 02M408 PROFESSIONAL PERFORMING ARTS 20 20 15 2.0 104.882353 3.529412 28.952941 25.117647 ... 8.1 NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 02M655 LIFE SCIENCES SECONDARY SCHL 50 90 10 2.0 138.052632 5.789474 23.921053 16.684211 ... 6.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 03M415 WADLEIGH ARTS HIGH SCHOOL 65 73 s 3.0 142.846154 6.000000 23.600000 18.307692 ... 5.7 NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 180 columns Adding missing values The full dataset contains almost all the information we need for analysis. But, there are a few missing pieces. We may want to correlate the AP (Advanced Placement) exam results with SAT results, but first we need to convert those columns to numbers, then fill in the missing values. In [20]: cols = [ 'AP Test Takers ' , 'Total Exams Taken' , 'Number of Exams with scores 3 4 or 5' ] for col in cols : full [ col ] = full [ col ] . infer_objects () full [ cols ] = full [ cols ] . fillna ( value = 0 ) Add a new column \"school_dist\" that indicates the school district of each school. It can be extracted from the \"DBN\" column. In [21]: full [ \"school_dist\" ] = full [ \"DBN\" ] . apply ( lambda x : x [: 2 ]) Finally, fill any missing values with the mean of the column. In [22]: full = full . fillna ( full . mean ()) full . head () Out[22]: DBN SchoolName AP Test Takers Total Exams Taken Number of Exams with scores 3 4 or 5 CSD NUMBER OF STUDENTS / SEATS FILLED NUMBER OF SECTIONS AVERAGE CLASS SIZE SIZE OF SMALLEST CLASS ... aca_t_11 saf_s_11 com_s_11 eng_s_11 aca_s_11 saf_tot_11 com_tot_11 eng_tot_11 aca_tot_11 school_dist 0 01M450 EAST SIDE COMMUNITY HS 19 21 s 1.0 57.600000 2.733333 21.200000 19.400000 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 01 1 01M539 NEW EXPLORATIONS SCI,TECH,MATH 255 377 191 1.0 156.368421 6.157895 25.510526 19.473684 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 01 2 02M408 PROFESSIONAL PERFORMING ARTS 20 20 15 2.0 104.882353 3.529412 28.952941 25.117647 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 02 3 02M655 LIFE SCIENCES SECONDARY SCHL 50 90 10 2.0 138.052632 5.789474 23.921053 16.684211 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 02 4 03M415 WADLEIGH ARTS HIGH SCHOOL 65 73 s 3.0 142.846154 6.000000 23.600000 18.307692 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 03 5 rows × 181 columns Computing variable correlations A good way to explore a dataset and see what columns are related to the one you care about is to compute correlations. This will tell you which columns are closely related to the column you're interested in. The sat_score column is what we care about. So let's see the its correlations with other columns. In [23]: full . corr ()[ 'sat_score' ] Out[23]: CSD 0.053884 NUMBER OF STUDENTS / SEATS FILLED 0.082401 NUMBER OF SECTIONS 0.069660 AVERAGE CLASS SIZE 0.085135 SIZE OF SMALLEST CLASS 0.048874 SIZE OF LARGEST CLASS 0.091046 SCHOOLWIDE PUPIL-TEACHER RATIO NaN schoolyear NaN frl_percent -0.188389 total_enrollment 0.138870 ell_num -0.123795 ell_percent -0.135156 sped_num -0.064685 sped_percent -0.157326 asian_num 0.193204 asian_per 0.191027 black_num 0.027135 black_per -0.064892 hispanic_num -0.054458 hispanic_per -0.102325 white_num 0.166775 white_per 0.168373 male_num 0.084583 male_per -0.071738 female_num 0.177495 female_per 0.071738 Total Cohort 0.061225 grade_span_min -0.048926 grade_span_max NaN expgrade_span_min NaN ... latitude -0.061575 longitude -0.006395 Year NaN Number Tested 0.089128 SAT Critical Reading Avg. Score 0.976192 SAT Math Avg. Score 0.953759 SAT Writing Avg. Score 0.981303 sat_score 1.000000 rr_s 0.283358 rr_t -0.056380 rr_p 0.057937 N_s 0.393010 N_t 0.242898 N_p 0.371894 saf_p_11 NaN com_p_11 NaN eng_p_11 NaN aca_p_11 NaN saf_t_11 NaN com_t_11 NaN eng_t_10 0.018455 aca_t_11 NaN saf_s_11 NaN com_s_11 NaN eng_s_11 NaN aca_s_11 NaN saf_tot_11 NaN com_tot_11 NaN eng_tot_11 NaN aca_tot_11 NaN Name: sat_score, Length: 69, dtype: float64 There are so many variables, it's hard to find interesting patterns. Let's see the top 10 biggest absolute correlations. In [24]: full . corr ()[ 'sat_score' ] . abs () . sort_values ( ascending = False ) . head ( 10 ) Out[24]: sat_score 1.000000 SAT Writing Avg. Score 0.981303 SAT Critical Reading Avg. Score 0.976192 SAT Math Avg. Score 0.953759 N_s 0.393010 N_p 0.371894 rr_s 0.283358 N_t 0.242898 asian_num 0.193204 asian_per 0.191027 Name: sat_score, dtype: float64 It is easy to recognize that the most correlated columns to the sat_score column are the test scores of each subjects. Of course, this is true. Because, the sat_score is the sum of the three subjects. We could use a pair plot to visualize the correlations. In [25]: import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline plt . style . use ( 'ggplot' ) sns . set ( style = \"ticks\" , color_codes = True ) cols = [ 'sat_score' , 'SAT Writing Avg. Score' , 'SAT Critical Reading Avg. Score' , 'SAT Math Avg. Score' ] vis_corr = full [ cols ] sns . pairplot ( vis_corr ) plt . show () Gender influence Now let's look at how gender correlates to the sat_score by investigating the female percentage and male percentage. We can find something interesting that the female percentage is positively correlated to sat_score , whereas male percentage is negatively correlated to sat_score . Females are somehow better at studying than male students. LOL. In [26]: print ( full . corr ()[ 'sat_score' ][[ 'female_per' , 'male_per' ]]) full . corr ()[ 'sat_score' ][[ 'female_per' , 'male_per' ]] . plot . bar () female_per 0.071738 male_per -0.071738 Name: sat_score, dtype: float64 Out[26]: <matplotlib.axes._subplots.AxesSubplot at 0xa27dda0> Race influence We have the data of percentage from four races (white, black, asian, hispanic). Let's find out how they are affecting the sat_score . I don't judge too much on races, just providing the facts. Findings: Asian and White percentages have positve correlation with the sat_score , whereas Black and Hispanic percentages have negative correlations with the sat_score . Asian students has the highest positve correlation among the four groups. Hispanic students has the highest negative correlations among the four groups. In [27]: full . corr ()[ 'sat_score' ][[ 'white_per' , 'black_per' , 'asian_per' , 'hispanic_per' ]] Out[27]: white_per 0.168373 black_per -0.064892 asian_per 0.191027 hispanic_per -0.102325 Name: sat_score, dtype: float64 In [28]: full . corr ()[ 'sat_score' ][[ 'white_per' , 'black_per' , 'asian_per' , 'hispanic_per' ]] . plot . bar ( color = 'b' ) Out[28]: <matplotlib.axes._subplots.AxesSubplot at 0x9e20f60> Atittude of respondents' influence Now let's see how students', parents', and teachers' ratings influence the SAT scores of the school. Findings: Students' ratings have a strong positive correlation to the SAT results. It might be due to students have a better understanding of their schools' performance. Parents' ratings also have a positive correlation to the SAT results. Teachers' ratings have a negative correlation which is interesting. In [29]: # rr_s students' ratings # rr_p parents' ratings # rr_t teachers' ratings full . corr ()[ 'sat_score' ][[ 'rr_s' , 'rr_p' , 'rr_t' ]] Out[29]: rr_s 0.283358 rr_p 0.057937 rr_t -0.056380 Name: sat_score, dtype: float64 In [30]: # visulize the correlations of respondents' ratings to the SAT scores full . corr ()[ 'sat_score' ][[ 'rr_s' , 'rr_p' , 'rr_t' ]] . plot ( kind = 'bar' , color = 'b' ) Out[30]: <matplotlib.axes._subplots.AxesSubplot at 0xba73748> Foreign language speaker differences Foreign language speaker means that English is not their first language. In other words, they are English language learners. In [31]: print ( full . corr ()[ 'sat_score' ][ 'ell_percent' ]) full . plot . scatter ( x = 'ell_percent' , y = 'sat_score' ) -0.13515582997474967 Out[31]: <matplotlib.axes._subplots.AxesSubplot at 0x9e5e898> Setting the context Map We'll map out the positions of the schools, which will give us a better visualization. This is done by: Setup a map centered on NYC. Add a marker for each school on the map. Display the map. In [32]: import folium from folium import plugins schools_map = folium . Map ( location = [ 40.7425 , - 73.9250 ], zoom_start = 10 ) marker_cluster = plugins . MarkerCluster () . add_to ( schools_map ) full [ \"school_name\" ] . fillna ( \"Missing\" , inplace = True ) full [ \"DBN\" ] . fillna ( \"Missing\" , inplace = True ) for row in full . iterrows (): folium . Marker ( location = [ row [ 1 ][ 'latitude' ], row [ 1 ][ 'longitude' ]], popup = \" {} \" . format ( row [ 1 ][ 'DBN' ])) . add_to ( marker_cluster ) # schools_map.createmap('schools.html') schools_map Out[32]: Let's use a heatmap to better visualize the assembly of schools in this area. In [33]: schools_heatmap = folium . Map ( location = [ full [ 'latitude' ] . mean (), full [ 'longitude' ] . mean ()], zoom_start = 10 ) schools_heatmap . add_children ( plugins . HeatMap ([[ row [ \"latitude\" ], row [ \"longitude\" ]] for name , row in full . iterrows ()])) schools_heatmap . save ( \"generated_maps/heatmap.html\" ) schools_heatmap Out[33]: District level mapping We can investigate the SAT results at district level, by: Grouping full dataset by district Computing the average of each column for each school district Converting the 'school_dist' field to remvoe leading 0s, so we can match our geographic district data. In [34]: district_data = full . groupby ( \"school_dist\" ) . agg ( np . mean ) district_data . reset_index ( inplace = True ) district_data [ \"school_dist\" ] = district_data [ \"school_dist\" ] . apply ( lambda x : str ( int ( x ))) In [35]: def show_district_map ( col , df ): geo_path = 'data/school_districts.geojson' districts = folium . Map ( location = [ full [ 'latitude' ] . mean (), full [ 'longitude' ] . mean ()], zoom_start = 10 ) districts . choropleth ( geo_data = geo_path , data = df , columns = [ 'school_dist' , col ], key_on = 'feature.properties.school_dist' , fill_color = 'YlGn' , fill_opacity = 0.7 , line_opacity = 0.2 , ) # folium.GeoJson(geo_path, name = 'geojson').add_to(districts) districts . save ( \"generated_maps/ {} .html\" . format ( col )) return districts sat_districs = show_district_map ( \"sat_score\" , district_data ) from IPython.display import IFrame IFrame ( 'generated_maps/sat_score.html' , width = 1000 , height = 400 ) Out[35]: Now we can clearly seen on the distribution of SAT scores in the NYC. Dark green areas on the map indicate high average scores. The color close to yellow indicates low average SAT score areas. In [36]: # English language learners distribution ell_districs = show_district_map ( \"ell_percent\" , district_data ) IFrame ( 'generated_maps/ell_percent.html' , width = 1000 , height = 400 ) Out[36]:","tags":"Data Science","url":"https://haochen23.github.io/2018/08/SAT-analysis.html","loc":"https://haochen23.github.io/2018/08/SAT-analysis.html"}]};